{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "This notebook will allow for the detection of modal filters and analysis of neighbourhood accessiablity within a single notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General set up\n",
    "Import libraries, set location etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = \"Newcastle Upon Tyne, United Kingdom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up python\n",
    "## Library imports\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import momepy\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import pandas as pd\n",
    "import overpy\n",
    "from shapely.geometry import LineString\n",
    "from shapely.geometry import Point\n",
    "import requests\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import Polygon\n",
    "import statistics\n",
    "from shapely.ops import unary_union\n",
    "import random\n",
    "\n",
    "## Update settings\n",
    "# update osmnx settings\n",
    "useful_tags_ways = ox.settings.useful_tags_way + ['cycleway'] + ['bicycle'] + ['motor_vehicle'] + ['railway'] + ['tunnel'] + ['barrier'] + ['bus'] + ['access'] + ['oneway'] + ['oneway:bicycle'] + ['covered'] + ['waterway']\n",
    "ox.config(use_cache=True, \n",
    "          log_console=True,\n",
    "          useful_tags_way=useful_tags_ways\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up location boundary\n",
    "# set location and get boundary\n",
    "boundary = ox.geocode_to_gdf(place)\n",
    "boundary = boundary.to_crs('EPSG:27700')\n",
    "\n",
    "# buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "boundary_buffered = boundary.buffer(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get our streets from OpenStreetMap and OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get street nodes and edges for walking and driving from OpenStreetMap\n",
    "\n",
    "## reset boundary_buffered crs for passing to OSM\n",
    "boundary_buffered_4326 = boundary_buffered.to_crs('4326')\n",
    "\n",
    "## get street networks\n",
    "all_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='all', simplify=False)\n",
    "walk_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='walk', simplify=True)\n",
    "drive_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='drive', simplify=False)\n",
    "\n",
    "all_edges = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "all_nodes = ox.graph_to_gdfs(all_streets, nodes=True, edges=False)\n",
    "\n",
    "walk_edges = ox.graph_to_gdfs(walk_streets, nodes=False, edges=True)\n",
    "walk_nodes = ox.graph_to_gdfs(walk_streets, nodes=True, edges=False)\n",
    "\n",
    "drive_edges = ox.graph_to_gdfs(drive_streets, nodes=False, edges=True)\n",
    "drive_nodes = ox.graph_to_gdfs(drive_streets, nodes=True, edges=False)\n",
    "\n",
    "\n",
    "\n",
    "## find the common nodes between networks\n",
    "# this ensures that shortest paths between points should always be able to be calculated\n",
    "common_nodes = drive_nodes.merge(walk_nodes, on='osmid', suffixes=('_drive', '_walk'))\n",
    "common_nodes_gdf = gpd.GeoDataFrame(common_nodes, geometry='geometry_drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in roads\n",
    "os_open_roads = gpd.read_file(r\"C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\oproad_gpkg_gb\\Data\\oproad_roads_only.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create neighbourhood boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up neighbourhoods\n",
    "# set location and get boundary\n",
    "boundary = ox.geocode_to_gdf(place)\n",
    "boundary = boundary.to_crs('EPSG:27700')\n",
    "\n",
    "# buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "boundary_buffered = boundary.buffer(50)\n",
    "\n",
    "## get railways\n",
    "# for unknown reasons, using rail = ox.graph_from_place(place, custom_filter='[\"railway\"]')\n",
    "# doesn't ALWAYS retrive the full rail network, hence why multiple lines are used to achive the same result\n",
    "\n",
    "# Get the major rail network\n",
    "railway_types = [\"\",\"rail\", \"light_rail\", \"narrow_gauge\", \"subway\", \"tram\"]\n",
    "\n",
    "# set an empty\n",
    "combined_railways = nx.MultiDiGraph()\n",
    "\n",
    "for railway_type in railway_types:\n",
    "    try:\n",
    "        network = ox.graph_from_place(place, simplify=False \n",
    "                                      , custom_filter=f'[\"railway\"~\"{railway_type}\"]'\n",
    "                                      )\n",
    "    \n",
    "    # handle locations where not all rail types are found\n",
    "    except Exception as e:\n",
    "        print(f\"No railway data found for '{railway_type}'.\")\n",
    "        network = nx.MultiGraph()\n",
    "\n",
    "    combined_railways = nx.compose(combined_railways, network)\n",
    "    \n",
    "\n",
    "\n",
    "# convert to gdf\n",
    "railways = ox.graph_to_gdfs(combined_railways, nodes=False, edges=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drop any other railway types that aren't needed\n",
    "railways = railways.loc[(~railways[\"railway\"].isin([\"tunnel\", \"abandoned\", \"razed\", \"disused\", \"funicular\", \"monorail\", \"miniature\"]))]\n",
    "\n",
    "# set railways crs\n",
    "railways = railways.to_crs('EPSG:27700')\n",
    "\n",
    "\n",
    "\n",
    "## get rivers\n",
    "# reset boundary crs to allow for features to be found\n",
    "boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "\n",
    "tags = {\"waterway\": [\"river\", \"rapids\"]}\n",
    "\n",
    "rivers = ox.features_from_polygon(polygon = boundary_buffered.iloc[0], tags = tags)\n",
    "\n",
    "# Dropping rows where 'tunnel' is equal to 'culvert'\n",
    "if 'tunnel' in rivers.columns:\n",
    "    # Dropping rows where 'tunnel' is equal to 'culvert'\n",
    "    rivers = rivers[rivers['tunnel'] != 'culvert']\n",
    "\n",
    "# set/reset crs\n",
    "rivers = rivers.to_crs('27700')\n",
    "boundary_buffered = boundary_buffered.to_crs('27700')\n",
    "\n",
    "\n",
    "## get unsuitable landcover types\n",
    "# reset boundary crs to allow for features to be found\n",
    "boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "# Define tags\n",
    "tags = {\"landuse\": [\"industrial\", \"railway\", \"brownfield\", \"commercial\", \"farmland\", \"meadow\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "landuse = ox.features_from_polygon(polygon = boundary_buffered.iloc[0], tags = tags)\n",
    "\n",
    "# set/reset crs\n",
    "landuse = landuse.to_crs('27700')\n",
    "\n",
    "## get unsuitable \"nature\" types\n",
    "\n",
    "# Define tags\n",
    "tags = {\"natural\": [\"wood\", \"water\", \"scrub\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "nature = ox.features_from_polygon(polygon = boundary_buffered.iloc[0], tags = tags)\n",
    "\n",
    "# set/reset crs\n",
    "nature = nature.to_crs('27700')\n",
    "\n",
    "## get unsuitable \"lesiure\" types. This is mainly for golfcourses\n",
    "\n",
    "# Define tags\n",
    "tags = {\"leisure\": [\"golf_course\", \"track\", \"park\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "leisure = ox.features_from_polygon(polygon = boundary_buffered.iloc[0], tags = tags)\n",
    "\n",
    "# set/reset crs\n",
    "leisure = leisure.to_crs('27700')\n",
    "\n",
    "\n",
    "## get unsuitable \"areoway\" types. This is mainly for airports\n",
    "\n",
    "# Define tags\n",
    "tags = {\"aeroway\": [\"aerodrome\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "aeroway = ox.features_from_polygon(polygon = boundary_buffered.iloc[0], tags = tags)\n",
    "\n",
    "# set/reset crs\n",
    "aeroway = aeroway.to_crs('27700')\n",
    "\n",
    "boundary_buffered = boundary_buffered.to_crs('27700')\n",
    "\n",
    "# concat\n",
    "landuse = pd.concat([landuse, nature, leisure, aeroway])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## get bus routes from OSM/NAPTAN\n",
    "\n",
    "# reset boundary crs to allow for features to be found\n",
    "boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "# Calculate the bounding box for XML query\n",
    "bounding_box = boundary_buffered.bounds\n",
    "\n",
    "# Extract the minimum and maximum coordinates\n",
    "minx = bounding_box['minx'].min()\n",
    "miny = bounding_box['miny'].min()\n",
    "maxx = bounding_box['maxx'].max()\n",
    "maxy = bounding_box['maxy'].max()\n",
    "\n",
    "# Create a list of four elements representing the bounding box\n",
    "bbox = [minx, miny, maxx, maxy]\n",
    "\n",
    "# reset boundary_buffer crs\n",
    "boundary_buffered = boundary_buffered.to_crs('27700')\n",
    "\n",
    "# Define the Overpass API endpoint\n",
    "overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# Define the XML query\n",
    "xml_query = f\"\"\"\n",
    "<osm-script output=\"json\" output-config=\"\" timeout=\"160\">\n",
    "  <union into=\"_\">\n",
    "    <query into=\"_\" type=\"node\">\n",
    "      <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "      <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "    </query>\n",
    "    <query into=\"_\" type=\"way\">\n",
    "      <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "      <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "    </query>\n",
    "    <query into=\"_\" type=\"relation\">\n",
    "      <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "      <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "    </query>\n",
    "  </union>\n",
    "  <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"body\" n=\"\" order=\"id\" s=\"\" w=\"\"/>\n",
    "  <recurse from=\"_\" into=\"_\" type=\"down\"/>\n",
    "  <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"skeleton\" n=\"\" order=\"quadtile\" s=\"\" w=\"\"/>\n",
    "</osm-script>\n",
    "\n",
    "\"\"\"\n",
    "# Initialize lists to store data\n",
    "geometries = []\n",
    "element_data = []\n",
    "\n",
    "# Make the Overpass API request\n",
    "response = requests.post(overpass_url, data=xml_query)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Access the data from the response\n",
    "    for element in data.get(\"elements\", []):\n",
    "        if element.get('type') == 'way' and 'geometry' in element:\n",
    "            # Extract geometry coordinates from 'geometry' field\n",
    "            coordinates = [(node['lon'], node['lat']) for node in element['geometry']]\n",
    "            # Create a LineString geometry\n",
    "            line = LineString(coordinates)\n",
    "            geometries.append(line)\n",
    "            element_data.append(element)\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    bus_routes = gpd.GeoDataFrame(element_data, geometry=geometries)\n",
    "\n",
    "    # Set CRS\n",
    "    bus_routes = bus_routes.set_crs('4326')\n",
    "    bus_routes = bus_routes.to_crs('27700')\n",
    "\n",
    "else:\n",
    "    print(f\"Error fetching data: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "\n",
    "## clip roads, rivers and railways to boundary\n",
    "\n",
    "# clip\n",
    "os_open_roads_clip = gpd.clip(os_open_roads, boundary_buffered)\n",
    "rivers_clip = gpd.clip(rivers, boundary_buffered)\n",
    "railways_clip = gpd.clip(railways, boundary_buffered)\n",
    "landuse_clip = gpd.clip(landuse, boundary_buffered)\n",
    "bus_routes_clip = gpd.clip(bus_routes, boundary_buffered)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## count bus routes per road and remove roads with greater than 1 bus route on them\n",
    "\n",
    "# set a buffer distance to convert roads to polygons\n",
    "buffer_distance = 0.2  # Adjust this value as needed. Set in meters\n",
    "\n",
    "# Create a new GeoDataFrame with the buffered geometries\n",
    "bus_routes_buffered = bus_routes_clip.copy()  # Copy the original GeoDataFrame\n",
    "bus_routes_buffered['geometry'] = bus_routes_buffered['geometry'].buffer(buffer_distance)\n",
    "\n",
    "# count the number of overlapping bus routes\n",
    "def count_overlapping_features(gdf):\n",
    "    # Create an empty column to store the count of overlapping features\n",
    "    gdf['Bus_routes_count'] = 0\n",
    "\n",
    "    # Iterate through each row in the GeoDataFrame\n",
    "    for idx, row in gdf.iterrows():\n",
    "        # Get the geometry of the current row\n",
    "        geometry = row['geometry']\n",
    "        \n",
    "        # Use a spatial filter to find overlapping features\n",
    "        overlaps = gdf[gdf['geometry'].intersects(geometry)]\n",
    "        \n",
    "        # Update the Bus_routes_count column with the count of overlapping features\n",
    "        gdf.at[idx, 'Bus_routes_count'] = len(overlaps)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# call function\n",
    "bus_routes_buffered_with_count = count_overlapping_features(bus_routes_buffered)\n",
    "\n",
    "# drop any roads which have less than two bus routes on them\n",
    "\n",
    "bus_routes_clip = bus_routes_buffered_with_count[bus_routes_buffered_with_count['Bus_routes_count'] >= 2]\n",
    "\n",
    "\n",
    "\n",
    "# Find \"boundary\" roads\n",
    "boundary_roads = os_open_roads_clip.loc[((os_open_roads_clip['primary_route'] == 'True') |\n",
    "                        (os_open_roads_clip['trunk_road'] == 'True') |\n",
    "                        (os_open_roads_clip['fictitious'] == 'True') |\n",
    "                        (os_open_roads_clip['road_classification'] == 'A Road') | \n",
    "                        (os_open_roads_clip['road_classification'] == 'B Road') | \n",
    "                        #(os_open_roads_clip['road_function'] == 'Restricted Local Access Road') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Minor Road') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Motorway') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Minor Road')  \n",
    "                        )]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## buffering and dissolving functions\n",
    " \n",
    "def buffer_and_dissolve(input_gdf):\n",
    "    # Buffer around boundaries\n",
    "    buffered_gdf = input_gdf.copy()  # Create a copy to avoid modifying the original\n",
    "    buffered_gdf['geometry'] = buffered_gdf['geometry'].buffer(5) # set a 5 meter buffer\n",
    "\n",
    "    # Dissolve the geometries\n",
    "    dissolved_geo = buffered_gdf.unary_union\n",
    "\n",
    "    # Create a new GeoDataFrame with the dissolved geometry\n",
    "    dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "\n",
    "    # Set the CRS (Coordinate Reference System)\n",
    "    dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "    return dissolved_gdf\n",
    "\n",
    "def dissolve_gdf(input_gdf):\n",
    "    # dissolve geometries\n",
    "    dissolved_geo = input_gdf.unary_union\n",
    "    dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "    dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "    return dissolved_gdf\n",
    "\n",
    "\n",
    "## buffer and dissolve \n",
    "boundary_roads_bd = buffer_and_dissolve(boundary_roads)\n",
    "boundary_rivers_bd = buffer_and_dissolve(rivers_clip)\n",
    "boundary_rail_bd = buffer_and_dissolve(railways_clip)\n",
    "boundary_landuse_bd = buffer_and_dissolve(landuse_clip)\n",
    "boundary_bus_routes_bd = buffer_and_dissolve(bus_routes_clip)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## join all boundary features\n",
    "boundaries = pd.concat([boundary_rivers_bd \n",
    "                        ,boundary_roads_bd \n",
    "                        ,boundary_rail_bd \n",
    "                        ,boundary_landuse_bd\n",
    "                        ,boundary_bus_routes_bd\n",
    "                        ], ignore_index=True)\n",
    "boundary_features = dissolve_gdf(boundaries)\n",
    "# Use the `difference` method to perform the \"Erase\" operation\n",
    "erased_boundary = boundary.difference(boundary_features.unary_union)\n",
    "\n",
    "# Convert the GeoSeries to a single geometry using unary_union\n",
    "erased_boundary = erased_boundary.unary_union\n",
    "\n",
    "# Create a new GeoDataFrame with the result of \"Erase\" operation\n",
    "erased_boundary_gdf = gpd.GeoDataFrame(geometry=[erased_boundary], crs=boundary.crs)\n",
    "\n",
    "# explode multipolygon to polygons\n",
    "erased_boundary_gdf = erased_boundary_gdf.explode()\n",
    "\n",
    "neighbourhoods = erased_boundary_gdf\n",
    "\n",
    "\n",
    "## drop very small areas (such as the centre of roundabouts etc)\n",
    "# calculate area\n",
    "neighbourhoods[\"area\"] = neighbourhoods.geometry.area\n",
    "# Drop rows where area is less than 5000. This value is arbitary\n",
    "neighbourhoods = neighbourhoods.loc[neighbourhoods[\"area\"] >= 10000]\n",
    "neighbourhoods.explore()\n",
    "\n",
    "\n",
    "## drop areas with no roads\n",
    "def count_roads_within_polygons(polygons_gdf, roads_gdf, polygon_column_name):\n",
    "    \"\"\"\n",
    "    Count the number of roads within each polygon in a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        polygons_gdf (GeoDataFrame): GeoDataFrame containing polygons.\n",
    "        roads_gdf (GeoDataFrame): GeoDataFrame containing roads.\n",
    "        polygon_column_name (str): Name of the column in polygons_gdf to use for grouping.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Original polygons GeoDataFrame with a \"road_count\" column added.\n",
    "    \"\"\"\n",
    "    \n",
    "    # spatial join\n",
    "    joined = gpd.sjoin(polygons_gdf, roads_gdf, how='left', op='intersects')\n",
    "    \n",
    "    # Group by the polygon column and count the number of roads in each\n",
    "    road_counts = joined.groupby(polygon_column_name).size().reset_index(name='road_count')\n",
    "    \n",
    "    # Merge the road counts back into the polygons GeoDataFrame\n",
    "    polygons_gdf = polygons_gdf.merge(road_counts, on=polygon_column_name, how='left')\n",
    "\n",
    "     # Calculate road density (area divided by road_count). It is mulitiplied by 10000 for ease of understanding the numbers involved with this\n",
    "    polygons_gdf['road_density'] = (polygons_gdf['road_count'] / polygons_gdf['area'] ) * 10000\n",
    "    \n",
    "    return polygons_gdf\n",
    "\n",
    "neighbourhoods = count_roads_within_polygons(neighbourhoods, os_open_roads_clip, 'geometry')\n",
    "\n",
    "# Drop rows with road_density below 0.2 or less than 4 roads\n",
    "neighbourhoods = neighbourhoods[(neighbourhoods['road_count'] > 2)]\n",
    "neighbourhoods = neighbourhoods[(neighbourhoods['road_density'] > 0.2)]\n",
    "## create unique IDs\n",
    "\n",
    "# simple number based ID\n",
    "neighbourhoods['ID'] = range(1, len(neighbourhoods) + 1)\n",
    "\n",
    "## remove holes from neighbourhoods (for visual reasons mostly)\n",
    "# Function to remove holes from neighbourhoods\n",
    "def remove_holes(polygon):\n",
    "    if polygon.geom_type == 'Polygon':\n",
    "        return Polygon(polygon.exterior)\n",
    "    else:\n",
    "        return polygon\n",
    "\n",
    "# Apply the function to the 'geometry' column of the GeoDataFrame\n",
    "neighbourhoods['geometry'] = neighbourhoods['geometry'].apply(remove_holes)\n",
    "\n",
    "## filter neighbourhoods to only locations with more than 1 intersection (1 or fewer intersections indicates that all travel modes will be the same)\n",
    "# reset neighbourhoods crs\n",
    "neighbourhoods = neighbourhoods.to_crs('4326')\n",
    "\n",
    "# Spatial join to count points within each neighborhood\n",
    "spatial_join = gpd.sjoin(neighbourhoods, common_nodes_gdf, how='left', op='contains')\n",
    "\n",
    "# Group by 'ID' and count the points within each neighborhood\n",
    "point_counts = spatial_join.groupby('ID').size().reset_index(name='point_count')\n",
    "\n",
    "# Filter out neighborhoods with 1 or 0 points\n",
    "filtered_neighbourhood_ids = point_counts[point_counts['point_count'] > 1]['ID']\n",
    "\n",
    "neighbourhoods= neighbourhoods[neighbourhoods['ID'].isin(filtered_neighbourhood_ids)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check neighbourhoods look good\n",
    "neighbourhoods.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate access metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find accessiablity\n",
    "\n",
    "## all to all\n",
    "def calculate_distance_stats_from_points(points_gdf, network):\n",
    "    all_pairs_shortest_paths = {}\n",
    "    points_osmids = points_gdf.index.tolist()  # Assuming the 'osmid' is the index in the GeoDataFrame\n",
    "\n",
    "    for start_node in points_osmids:\n",
    "        shortest_paths = {}\n",
    "        try:\n",
    "            for end_node in points_osmids:\n",
    "                if start_node != end_node:\n",
    "                    distance = nx.shortest_path_length(network, start_node, end_node, weight='length')\n",
    "                    shortest_paths[end_node] = distance\n",
    "            all_pairs_shortest_paths[start_node] = shortest_paths\n",
    "        except nx.NetworkXNoPath:\n",
    "            # If no path is found, skip adding to all_pairs_shortest_paths\n",
    "            continue\n",
    "\n",
    "    distances = [length for paths in all_pairs_shortest_paths.values() for length in paths.values()]\n",
    "\n",
    "    mean_distance = statistics.mean(distances)\n",
    "    median_distance = statistics.median(distances)\n",
    "    min_distance = min(distances)\n",
    "    max_distance = max(distances)\n",
    "    distance_range = max_distance - min_distance\n",
    "    total_distance = sum(distances)\n",
    "\n",
    "    return {\n",
    "        \"mean_distance\": mean_distance,\n",
    "        \"median_distance\": median_distance,\n",
    "        \"min_distance\": min_distance,\n",
    "        \"max_distance\": max_distance,\n",
    "        \"distance_range\": distance_range,\n",
    "        \"total_distance\": total_distance\n",
    "    }\n",
    "\n",
    "## processing for all to all \n",
    "results = []\n",
    "\n",
    "for index, row in neighbourhoods.iterrows():\n",
    "    neighbourhood = neighbourhoods.loc[[index]]\n",
    "    print(\"Starting index number\", index)\n",
    "\n",
    "    ## get neighbourhood boundary and neighbourhood boundary buffer\n",
    "    # set crs\n",
    "    neighbourhood = neighbourhood.to_crs('27700')\n",
    "    # create a buffer neighbourhood\n",
    "    neighbourhood_buffer = neighbourhood['geometry'].buffer(15)\n",
    "    # convert back to a geodataframe (for later on)\n",
    "    neighbourhood_buffer = gpd.GeoDataFrame(geometry=neighbourhood_buffer)\n",
    "    # reset crs\n",
    "    neighbourhood, neighbourhood_buffer = neighbourhood.to_crs('4326'), neighbourhood_buffer.to_crs('4326')\n",
    "\n",
    "\n",
    "    ## get nodes which can be driven to and walked to within area\n",
    "    neighbourhood_nodes = gpd.clip(common_nodes_gdf, neighbourhood_buffer)\n",
    "\n",
    "    ## get length of total edges within the neighbourhood\n",
    "    edges_within_neighbourhood = gpd.sjoin(all_edges, neighbourhood, how=\"inner\", op=\"intersects\")\n",
    "    total_length = edges_within_neighbourhood['length'].sum()\n",
    "\n",
    "\n",
    "    ## calculate neighbourhood distance stats for walking and driving\n",
    "    walk_stats = calculate_distance_stats_from_points(neighbourhood_nodes, walk_streets)\n",
    "    drive_stats = calculate_distance_stats_from_points(neighbourhood_nodes, drive_streets)\n",
    "\n",
    "\n",
    "    ## Add the statistics to the GeoDataFrame\n",
    "    neighbourhood['walk_mean_distance'] = walk_stats['mean_distance']\n",
    "    neighbourhood['walk_median_distance'] = walk_stats['median_distance']\n",
    "    neighbourhood['walk_min_distance'] = walk_stats['min_distance']\n",
    "    neighbourhood['walk_max_distance'] = walk_stats['max_distance']\n",
    "    neighbourhood['walk_distance_range'] = walk_stats['distance_range']\n",
    "    neighbourhood['walk_total_distance'] = walk_stats['total_distance']\n",
    "\n",
    "    neighbourhood['drive_mean_distance'] = drive_stats['mean_distance']\n",
    "    neighbourhood['drive_median_distance'] = drive_stats['median_distance']\n",
    "    neighbourhood['drive_min_distance'] = drive_stats['min_distance']\n",
    "    neighbourhood['drive_max_distance'] = drive_stats['max_distance']\n",
    "    neighbourhood['drive_distance_range'] = drive_stats['distance_range']\n",
    "    neighbourhood['drive_total_distance'] = drive_stats['total_distance']\n",
    "\n",
    "    ## Store statistics along with neighborhood ID or other identifying information\n",
    "    result = {\n",
    "        'neighbourhood_id': neighbourhood['ID'].iloc[0],  # Assuming you have an ID column\n",
    "        'walk_mean_distance': walk_stats['mean_distance'],\n",
    "        'walk_median_distance': walk_stats['median_distance'],\n",
    "        'walk_total_distance': walk_stats['total_distance'],\n",
    "        \n",
    "\n",
    "        'drive_mean_distance': drive_stats['mean_distance'],\n",
    "        'drive_median_distance': drive_stats['median_distance'],\n",
    "        'drive_total_distance': drive_stats['total_distance'],\n",
    "\n",
    "        'total_edge_length': total_length\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "## Convert the results to a new dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "## calculate differances\n",
    "\n",
    "results_df['mean_distance_diff'] = results_df['walk_mean_distance'] - results_df['drive_mean_distance']\n",
    "results_df['median_distance_diff'] = results_df['walk_median_distance'] - results_df['drive_median_distance']\n",
    "results_df['total_distance_diff'] = results_df['walk_total_distance'] - results_df['drive_total_distance']\n",
    "\n",
    "merged_df = pd.merge(neighbourhoods, results_df, left_on = \"ID\", right_on = \"neighbourhood_id\")\n",
    "access_results_gdf = gpd.GeoDataFrame(merged_df, geometry='geometry')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find modal filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find modal filters\n",
    "\n",
    "\n",
    "## get barrier filters\n",
    "# get the boundary in the correct CRS for OSMnx\n",
    "boundary_4326 = boundary.to_crs('EPSG:4326')\n",
    "\n",
    "# get the most \"basic\" filters mapped, the barriers/bollards etc\n",
    "# get barrier filters\n",
    "# Define tags\n",
    "tags = {\"barrier\": [\"bollard\", \"bus_trap\", \"entrance\", \"planter\", \"sump_buster\", \"wedge\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "barriers = ox.features_from_polygon(polygon = boundary_4326.geometry.iloc[0], tags = tags)\n",
    "\n",
    "\n",
    "## process any linestrings into point geometries\n",
    "\n",
    "# Filter the GeoDataFrame to select only rows with \"linestring\" geometry\n",
    "barriers_linestrings = barriers[barriers['geometry'].geom_type == 'LineString']\n",
    "\n",
    "# Create an empty GeoDataFrame to store the individual points\n",
    "points_gdf = gpd.GeoDataFrame(columns=list(barriers_linestrings.columns), crs=barriers_linestrings.crs)\n",
    "\n",
    "# Iterate through each row in the GeoDataFrame with linestrings\n",
    "for idx, row in barriers_linestrings.iterrows():\n",
    "    if isinstance(row['geometry'], LineString):\n",
    "        # Extract the individual points from the linestring\n",
    "        points = [Point(coord) for coord in list(row['geometry'].coords)]\n",
    "        \n",
    "        # Create a GeoDataFrame from the individual points and copy the attributes\n",
    "        points_df = gpd.GeoDataFrame(geometry=points, crs=barriers_linestrings.crs)\n",
    "        for col in barriers_linestrings.columns:\n",
    "            if col != 'geometry':\n",
    "                points_df[col] = row[col]\n",
    "        \n",
    "        # Rename the \"geometry\" column to \"merged_geometry\"\n",
    "        points_df = points_df.rename(columns={'geometry': 'merged_geometry'})\n",
    "        \n",
    "        # Append the points to the points_gdf\n",
    "        points_gdf = pd.concat([points_gdf, points_df], ignore_index=True)\n",
    "\n",
    "# Now, points_gdf contains all the individual points from the linestrings with inherited attributes\n",
    "\n",
    "# Remove the \"geometry\" column from the points GeoDataFrame\n",
    "points_gdf = points_gdf.drop(columns=['geometry'])\n",
    "\n",
    "# Remove the linestring rows from the original GeoDataFrame\n",
    "barriers = barriers[barriers['geometry'].geom_type != 'LineString']\n",
    "\n",
    "# Rename the \"merged_geometry\" column to \"geometry\" in the points GeoDataFrame\n",
    "points_gdf = points_gdf.rename(columns={'merged_geometry': 'geometry'})\n",
    "\n",
    "# Concatenate the individual points GeoDataFrame to the original GeoDataFrame\n",
    "barriers = pd.concat([barriers, points_gdf], ignore_index=True)\n",
    "\n",
    "# Reset the index to ensure it is continuous\n",
    "barriers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a new column \"previously_linestring\" and set it to False initially\n",
    "barriers['previously_linestring'] = False\n",
    "\n",
    "# Iterate through each row in the GeoDataFrame with linestrings\n",
    "for idx, row in barriers_linestrings.iterrows():\n",
    "    if isinstance(row['geometry'], LineString):\n",
    "        # Extract the individual points from the linestring\n",
    "        points = [Point(coord) for coord in list(row['geometry'].coords)]\n",
    "        \n",
    "        # Iterate through the points in the linestring\n",
    "        for point in points:\n",
    "            # Check if the point's geometry intersects with any of the original linestrings\n",
    "            mask = barriers['geometry'].intersects(point)\n",
    "            if mask.any():\n",
    "                # If the point intersects with any linestring, set \"previously_linestring\" to True\n",
    "                barriers.loc[mask, 'previously_linestring'] = True\n",
    "\n",
    "# add a unique ID\n",
    "barriers['barrier_id'] = range(1, len(barriers) + 1)\n",
    "\n",
    "# Convert the OSMnx graph to a GeoDataFrame of streets\n",
    "streets_gdf = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "\n",
    "# join the barriers to the streets \n",
    "streets_gdf = gpd.sjoin(streets_gdf, barriers, how = \"left\", op=\"intersects\")\n",
    "\n",
    "# clean geodataframe and drop streets without a barrier\n",
    "streets_gdf.columns = streets_gdf.columns.str.replace(\"_right\", \"_barrier\").str.replace(\"_left\",\"_street\")\n",
    "# we need to double check the name of \"barrier\"\n",
    "streets_gdf['barrier_barrier'] = streets_gdf['barrier'] if 'barrier' in streets_gdf.columns else streets_gdf['barrier_barrier']\n",
    "\n",
    "if 'name_street' in streets_gdf.columns:\n",
    "    streets_gdf = streets_gdf.rename(columns={'name_street': 'name'})\n",
    "barrier_streets = streets_gdf.dropna(subset=['barrier_barrier'])\n",
    "\n",
    "# add barrier tag\n",
    "barrier_streets['filter_type'] = 'barrier or bollard'\n",
    "\n",
    "\n",
    "## extract points which are on/within 1m of streets only\n",
    "streets_gdf['has_barrier'] = 'yes'\n",
    "\n",
    "# reset crs before spatail join\n",
    "barriers, streets_gdf = barriers.to_crs(3857), streets_gdf.to_crs(3857)\n",
    "\n",
    "barriers = gpd.sjoin_nearest(barriers, streets_gdf, how = \"left\", max_distance = 1)\n",
    "barriers = barriers.dropna(subset=['has_barrier'])\n",
    "barriers = barriers.reset_index(drop=True)  # Reset the index\n",
    "# Dissolve based on the 'geometry' column\n",
    "\n",
    "# re-reset crs \n",
    "barriers, streets_gdf = barriers.to_crs(4326), streets_gdf.to_crs(4326)\n",
    "\n",
    "# we need to double check the name of \"barrier_id\"\n",
    "import numpy as np\n",
    "streets_gdf['barrier_id_right'] = streets_gdf['barrier_id'] if 'barrier_id' in streets_gdf.columns else streets_gdf['barrier_id_right']\n",
    "# dissolve\n",
    "barriers = barriers.dissolve(by='barrier_id_right')\n",
    "# add barrier tag\n",
    "barriers['filter_type'] = 'barrier or bollard'\n",
    "# Reset the index to remove multi-index\n",
    "barriers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## bus gates also act as modal filters. lets now find all the bus gates within our streets\n",
    "\n",
    "# we need to double check the name of \"access\"\n",
    "streets_gdf['access_street'] = streets_gdf['access'] if 'access' in streets_gdf.columns else streets_gdf['access_street']\n",
    "streets_gdf['bicycle_street'] = streets_gdf['bicycle'] if 'bicycle' in streets_gdf.columns else streets_gdf['bicycle_street']\n",
    "streets_gdf['bus'] = streets_gdf['bus_street'] if 'bus_street' in streets_gdf.columns else streets_gdf['bus']\n",
    "\n",
    "\n",
    "busgates = streets_gdf[((streets_gdf[\"bus\"] == \"yes\") & (streets_gdf[\"access_street\"] == \"no\") & (streets_gdf[\"bicycle_street\"] == \"yes\")) |\n",
    "                    (streets_gdf[\"bus\"] == \"yes\") & (streets_gdf[\"motor_vehicle_street\"] == \"no\") & (streets_gdf[\"bicycle_street\"] == \"yes\")\n",
    "                    ]\n",
    "\n",
    "# add bus gate tag\n",
    "busgates['filter_type'] = 'bus gate'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## one-way streets also can act as modal filters. lets find where cycling is unrestricted but cars are\n",
    "oneways = streets_gdf[(streets_gdf[\"oneway\"] == True) & (streets_gdf[\"oneway:bicycle\"] == \"no\")]\n",
    "\n",
    "# we dissolve the roads with the same name as to not miscount the total number of oneways\n",
    "# Convert values in the \"name\" column to strings if they are not already\n",
    "oneways['name'] = oneways['name'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "# Perform dissolve \n",
    "oneways = oneways.dissolve(by='name')\n",
    "\n",
    "# Reset the index \n",
    "oneways = oneways.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# add one way tag\n",
    "oneways['filter_type'] = 'one-way bike'\n",
    "\n",
    "\n",
    "\n",
    "def filter_streets_continuations(input_gdf):\n",
    "    ## clean dataframe\n",
    "    # Check if 'highway_street' column exists and rename it to 'highway'\n",
    "    if 'highway_street' in input_gdf.columns:\n",
    "        input_gdf.rename(columns={'highway_street': 'highway'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # filter dataframe \n",
    "    ## remove indoor roads, these are likey pedestrian only however often don't have any \"cycling\" related tag\n",
    "    if 'covered' in input_gdf.columns:\n",
    "        input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'covered' in str(x))]\n",
    "        input_gdf = input_gdf[input_gdf['covered'] != 'yes']\n",
    "    ## also remove footways and steps, as these are almost pedestrain only, never cyclable\n",
    "    input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'footway' in str(x))]\n",
    "    input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'steps' in str(x))]\n",
    "\n",
    "\n",
    "\n",
    "    ## clean dataframe\n",
    "    input_gdf['name'] = input_gdf['name'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "    input_gdf['highway'] = input_gdf['highway'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## perform street continunation filtering\n",
    "    # Grouping by 'name' and checking for groups with 'pedestrian' and another highway type\n",
    "    grouped = input_gdf.groupby('name').filter(lambda x: any('pedestrian' in val for val in x['highway']) and len(x['highway'].unique()) > 1)\n",
    "    street_continuations_gdf = grouped[grouped['highway'].str.contains('pedestrian', case=False, na=False)] # Extracting the rows containing 'pedestrian' in the highway column\n",
    "\n",
    "    ## deal with nan names\n",
    "\n",
    "\n",
    "    ## dissolve lines that are very very close to each other\n",
    "    if not street_continuations_gdf.empty:\n",
    "        street_continuations_gdf = street_continuations_gdf.to_crs('27700')\n",
    "        street_continuations_gdf['buffer'] = street_continuations_gdf.geometry.buffer(1)\n",
    "        dissolved = street_continuations_gdf.dissolve(by='name')\n",
    "        \n",
    "        # If a MultiPolygon is formed, convert it to individual polygons\n",
    "        if isinstance(dissolved.geometry.iloc[0], MultiPolygon):\n",
    "            dissolved = dissolved.explode()\n",
    "        \n",
    "        # Remove the buffer column\n",
    "        dissolved = dissolved.drop(columns='buffer')\n",
    "        street_continuations_gdf = dissolved.to_crs('4326')\n",
    "\n",
    "    return street_continuations_gdf\n",
    "\n",
    "\n",
    "# get street continunations\n",
    "streets_continuations_gdf = filter_streets_continuations(streets_gdf)\n",
    "\n",
    "# add street conitinuation tag\n",
    "streets_continuations_gdf['filter_type'] = 'street continuation'\n",
    "\n",
    "\n",
    "filters = gpd.GeoDataFrame(pd.concat([barriers, busgates, oneways, streets_continuations_gdf], ignore_index=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## alter neighbourhoods before joining\n",
    "# reset neighbourhood crs\n",
    "filters_results_gdf  = neighbourhoods.to_crs('EPSG:27700')\n",
    "\n",
    "\n",
    "\n",
    "# buffer to ensure all filters are captured\n",
    "filters_results_gdf['geometry'] = filters_results_gdf['geometry'].buffer(5)\n",
    "\n",
    "# reset neighbourhood crs\n",
    "filters_results_gdf  = filters_results_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "## spatial join\n",
    "# Perform a spatial join between neighbourhoods and filters\n",
    "joined_data = gpd.sjoin(filters_results_gdf, filters, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# Count the number of each filter within each neighbourhood\n",
    "filter_type_counts = joined_data.groupby(['ID', 'filter_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Reset the index to make it more readable\n",
    "filter_type_counts = filter_type_counts.reset_index()\n",
    "\n",
    "# Merge the filter_type_counts DataFrame with the neighbourhoods GeoDataFrame on the ID column\n",
    "filters_results_gdf = filters_results_gdf.merge(filter_type_counts, on='ID', how='left')\n",
    "\n",
    "# find the total counts of each filter type per neighborhood\n",
    "filters_results_gdf['total_filter_types'] = filters_results_gdf.iloc[:, 5:].sum(axis=1)\n",
    "\n",
    "# Fill NaN values with 0 if necessary\n",
    "filters_results_gdf = filters_results_gdf.fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we should have filters_results_gdf and access_results_gdf\n",
    "\n",
    "filters_results_gdf, access_results_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing from this point onwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf = gpd.GeoDataFrame(filters_results_gdf.merge(access_results_gdf, on=\"ID\", suffixes=('_filters', \"_access\")))\n",
    "results_gdf = results_gdf.set_geometry('geometry_access')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(results_gdf[\"total_filter_types\"], results_gdf[\"mean_distance_diff\"].abs(), c='blue', alpha=0.7)\n",
    "plt.xlabel('Total Filter Types')\n",
    "plt.ylabel('Mean Distance Difference')\n",
    "plt.title('Scatter Plot of Total Filter Types vs Mean Distance Difference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_gdf[\"walk_mean_distance\"], label='Walk Mean Distance')\n",
    "plt.plot(results_gdf[\"drive_mean_distance\"], label='Drive Mean Distance')\n",
    "plt.xlabel('Index or ID')  # You can change this label as per your data\n",
    "plt.ylabel('Mean Distance')\n",
    "plt.title('Comparison of Walk Mean Distance and Drive Mean Difference')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a GeoDataFrame named results_gdf\n",
    "# with columns \"walk_mean_distance\" and \"drive_mean_distance\"\n",
    "\n",
    "# Creating a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(['Walk']*len(results_gdf), results_gdf[\"walk_mean_distance\"], label='Walk Mean Distance', alpha=0.7)\n",
    "plt.scatter(['Drive']*len(results_gdf), results_gdf[\"drive_mean_distance\"], label='Drive Mean Distance', alpha=0.7)\n",
    "\n",
    "# Drawing lines between matching IDs or indexes\n",
    "for i in range(len(results_gdf)):\n",
    "    plt.plot(['Walk', 'Drive'], [results_gdf.iloc[i][\"walk_mean_distance\"], results_gdf.iloc[i][\"drive_mean_distance\"]], c='gray', alpha=0.5)\n",
    "    plt.text('Walk', results_gdf.iloc[i][\"walk_mean_distance\"], results_gdf.iloc[i][\"ID\"], ha='center', va='bottom', color='red')\n",
    "    plt.text('Drive', results_gdf.iloc[i][\"drive_mean_distance\"], results_gdf.iloc[i][\"ID\"], ha='center', va='bottom', color='red')\n",
    "\n",
    "plt.xlabel('Transportation Mode')\n",
    "plt.ylabel('Mean Distance')\n",
    "plt.title('Scatter Plot of Mean Distance for Walk and Drive with Connecting Lines and ID Labels')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assuming you have a GeoDataFrame named results_gdf\n",
    "# with columns \"total_filter_types\" and \"mean_distance_diff\"\n",
    "\n",
    "# Calculate Pearson correlation coefficient and p-value\n",
    "correlation_coefficient, p_value = pearsonr(results_gdf[\"total_filter_types\"], results_gdf[\"mean_distance_diff\"])\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {correlation_coefficient}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Assuming you have a GeoDataFrame named results_gdf\n",
    "# with columns \"total_filter_types\" and \"mean_distance_diff\"\n",
    "\n",
    "# Calculate Spearman's rank correlation coefficient and p-value\n",
    "spearman_coefficient, p_value_spearman = spearmanr(results_gdf[\"total_filter_types\"], results_gdf[\"mean_distance_diff\"])\n",
    "\n",
    "print(f\"Spearman's rank correlation coefficient: {spearman_coefficient}\")\n",
    "print(f\"P-value (Spearman): {p_value_spearman}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_streets_gdf = ox.graph_to_gdfs(walk_streets, nodes = False, edges = True)\n",
    "drive_streets_gdf = ox.graph_to_gdfs(drive_streets, nodes = False, edges = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_edge_bc = nx.edge_betweenness_centrality(walk_streets, normalized=True, weight='length')\n",
    "drive_edge_bc = nx.edge_betweenness_centrality(drive_streets, normalized = True, weight = 'length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_edge_bc_df = pd.DataFrame(list(walk_edge_bc.items()), columns=['edge', 'walk_edge_betweenness'])\n",
    "drive_edge_bc_df = pd.DataFrame(list(drive_edge_bc.items()), columns=['edge', 'drive_edge_betweenness'])\n",
    "\n",
    "# Extract u and v columns from the edge column for walk_streets_gdf\n",
    "walk_edge_bc_df[['u', 'v', 'key']] = pd.DataFrame(walk_edge_bc_df['edge'].tolist(), index=walk_edge_bc_df.index)\n",
    "\n",
    "# Extract u and v columns from the edge column for drive_streets_gdf\n",
    "drive_edge_bc_df[['u', 'v', 'key']] = pd.DataFrame(drive_edge_bc_df['edge'].tolist(), index=drive_edge_bc_df.index)\n",
    "\n",
    "# Merge the edge betweenness DataFrames with walk_streets_gdf and drive_streets_gdf\n",
    "walk_streets_gdf = pd.merge(walk_streets_gdf, walk_edge_bc_df[['u', 'v', 'walk_edge_betweenness']], how='left', on=['u', 'v'])\n",
    "drive_streets_gdf = pd.merge(drive_streets_gdf, drive_edge_bc_df[['u', 'v', 'drive_edge_betweenness']], how='left', on=['u', 'v'])\n",
    "\n",
    "# Perform a spatial join based on geometry to associate edges with polygons for walk_streets_gdf\n",
    "joined_data_walk = gpd.sjoin(walk_streets_gdf, results_gdf, how='right', op='within')\n",
    "\n",
    "# Group by the polygon ID and calculate the mean and max of walk_edge_betweenness\n",
    "grouped_data_walk = joined_data_walk.groupby('ID_column')['walk_edge_betweenness'].agg(walk_bc_mean='mean', walk_bc_max='max').reset_index()\n",
    "\n",
    "# Merge the results back into results_gdf\n",
    "results = pd.merge(results_gdf, grouped_data_walk, how='left', left_on='ID', right_on='ID_column')\n",
    "\n",
    "# Perform a spatial join based on geometry to associate edges with polygons for drive_streets_gdf\n",
    "joined_data_drive = gpd.sjoin(drive_streets_gdf, results_gdf, how='right', op='within')\n",
    "\n",
    "# Group by the polygon ID and calculate the mean and max of drive_edge_betweenness\n",
    "grouped_data_drive = joined_data_drive.groupby('ID_column')['drive_edge_betweenness'].agg(drive_bc_mean='mean', drive_bc_max='max').reset_index()\n",
    "\n",
    "# Merge the results back into results_gdf\n",
    "results = pd.merge(results, grouped_data_drive, how='left', on='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'walk_bc_mean' and 'drive_bc_mean' columns exist in your GeoDataFrame\n",
    "results[['walk_bc_mean', 'drive_bc_mean']].plot(figsize=(10, 6), legend=True)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Polygon ID')\n",
    "plt.ylabel('Betweenness Centrality Mean')\n",
    "plt.title('Comparison of Walk and Drive Betweenness Centrality Mean')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import momepy\n",
    " \n",
    "\n",
    "primal = momepy.closeness_centrality(all_streets, radius=300, name='closeness400', distance='length', weight='length')\n",
    "\n",
    "nodes = momepy.nx_to_gdf(primal, lines=False)\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "nodes.plot(ax=ax, column='closeness400', cmap='Spectral_r', scheme='quantiles', k=15, alpha=0.6)\n",
    "ax.set_axis_off()\n",
    "ax.set_title('closeness400')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rat runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_streets_gdf_nodes = ox.graph_to_gdfs(drive_streets, nodes = True, edges = False)\n",
    "\n",
    "drive_streets_gdf = drive_streets_gdf.to_crs('27700')\n",
    "drive_streets_gdf_nodes = drive_streets_gdf_nodes.to_crs('27700')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create buffer of bounding roads \n",
    "boundary_roads_2m_buffer =  boundary_roads['geometry'].buffer(5)\n",
    "\n",
    "\n",
    "drive_streets_gdf['buffered_geometry'] = drive_streets_gdf['geometry'].buffer(5)\n",
    "drive_streets_gdf = drive_streets_gdf.set_geometry('buffered_geometry')\n",
    "\n",
    "\n",
    "drive_streets_gdf.explore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip \n",
    "drive_streets_gdf_clipped = gpd.clip(drive_streets_gdf, boundary_roads_2m_buffer)\n",
    "\n",
    "# reset geometry\n",
    "drive_streets_gdf_clipped = drive_streets_gdf_clipped.set_geometry('geometry')\n",
    "drive_streets_gdf = drive_streets_gdf.set_geometry('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_streets_gdf_clipped = gpd.clip(drive_streets_gdf, boundary_roads_2m_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def fancy_code_cascade(lines=20, delay=0.1):\n",
    "    characters = \"qwertyuiopasdfghjklzxcvbnm!£$%^&*(){}~@:>?<\\|,/.';lp[l]\"\n",
    "    code_width = 50\n",
    "\n",
    "    for _ in range(lines):\n",
    "        line = \"\".join(random.choice(characters) for _ in range(code_width))\n",
    "        print(line)\n",
    "        time.sleep(delay)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initiating Fancy Code Cascade...\")\n",
    "    time.sleep(1)\n",
    "    fancy_code_cascade()\n",
    "    print(\"Code execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
