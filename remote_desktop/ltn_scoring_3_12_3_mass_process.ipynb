{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "This notebook will allow for the detection of modal filters, rat runs, and analysis of neighbourhood accessiablity within a single notebook. The output of this code is a set of neighbourhoods scored on their plausiablity to be a \"Low Traffic Neighbourhood\", which is written to a geopackage. To run this code you will need the OS Open Roads dataset available on the OS website: https://osdatahub.os.uk/downloads/open/OpenRoads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General set up\n",
    "Import libraries, set location etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up python\n",
    "## Library imports\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import momepy\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import pandas as pd\n",
    "import overpy\n",
    "from shapely.geometry import LineString\n",
    "from shapely.geometry import Point\n",
    "import requests\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import Polygon\n",
    "import statistics\n",
    "from shapely.ops import unary_union\n",
    "import random\n",
    "import overpy\n",
    "import os \n",
    "import math\n",
    "from itertools import count\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "#from osmnx._errors import InsufficientResponseError\n",
    "from owslib.wms import WebMapService\n",
    "from rasterio.mask import mask as rio_mask  \n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape, mapping\n",
    "from rasterio.io import MemoryFile\n",
    "import numpy as np\n",
    "from shapely.ops import unary_union\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "\n",
    "\n",
    "## Mute warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=ShapelyDeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "\n",
    "## Update settings\n",
    "# update osmnx settings\n",
    "useful_tags_ways = ox.settings.useful_tags_way + ['cycleway'] + ['bicycle'] + ['motor_vehicle'] + ['railway'] + ['tunnel'] + ['barrier'] + ['bus'] + ['access'] + ['oneway'] + ['oneway:bicycle'] + ['covered'] + ['waterway']\n",
    "ox.config(use_cache=True, \n",
    "          log_console=True,\n",
    "          useful_tags_way=useful_tags_ways\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in place names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your text file\n",
    "file_path = r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\test.txt'\n",
    "\n",
    "# Initialize an empty list to store the lines\n",
    "places = []\n",
    "\n",
    "# Open the file and read each line\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Strip the newline character and any surrounding whitespace\n",
    "        place = line.strip()\n",
    "        # Append the line to the list\n",
    "        places.append(place)\n",
    "\n",
    "# Print the list of places\n",
    "print(\"List of places read from test.txt:\")\n",
    "for place in places:\n",
    "    print(place)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default is 1000\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default values are 1, 1, 1\n",
    "through_route_weighting = 1\n",
    "permiablity_weighting = 1\n",
    "modal_filter_weighting = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in OS Roads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OS_roads():\n",
    "        \"\"\"\n",
    "        Reads in OS Open Road data from a GeoPackage file.\n",
    "\n",
    "        Returns:\n",
    "        os_open_roads (GeoDataFrame): A GeoDataFrame containing road data.\n",
    "        \"\"\"\n",
    "        os_open_roads = gpd.read_file(r\"C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\oproad_gpkg_gb\\Data\\oproad_roads_only.gpkg\")\n",
    "        return os_open_roads\n",
    "\n",
    "os_open_roads = get_OS_roads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in places:\n",
    "\n",
    "    # get boundary\n",
    "    def set_location_boundary(place):\n",
    "        \"\"\"\n",
    "        Sets up the location boundary by geocoding the given place and buffering it.\n",
    "\n",
    "        Parameters:\n",
    "        place (str): The name or address of the place to geocode.\n",
    "\n",
    "        Returns:\n",
    "        geopandas.GeoDataFrame: The buffered boundary of the location.\n",
    "        \"\"\"\n",
    "        # Set location and get boundary\n",
    "        boundary = ox.geocode_to_gdf(place)\n",
    "        boundary = boundary.to_crs('EPSG:27700')\n",
    "\n",
    "        # Buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "        boundary_buffered = boundary.buffer(50)\n",
    "\n",
    "        return boundary_buffered, boundary\n",
    "\n",
    "    boundary_buffered, boundary = set_location_boundary(place)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This code retrieves street nodes and edges for walking and driving from OpenStreetMap within our area boundary, and loads the OS Open Roads network dataset.\n",
    "\n",
    "    Functions:\n",
    "    - get_street_networks: Retrieves street networks for all, walking, and driving modes within the specified boundary.\n",
    "    \"\"\"\n",
    "\n",
    "  \n",
    "\n",
    "    def get_OSM_street_networks(boundary_buffered):\n",
    "        \"\"\"\n",
    "        Retrieves street networks for all, walking, and driving modes within the specified boundary.\n",
    "\n",
    "        Parameters:\n",
    "        - boundary_buffered: A GeoDataFrame representing the boundary of the area of interest.\n",
    "\n",
    "        Returns:\n",
    "        - all_edges: A GeoDataFrame containing the edges (streets) of the entire street network.\n",
    "        - all_nodes: A GeoDataFrame containing the nodes (intersections) of the entire street network.\n",
    "        - walk_edges: A GeoDataFrame containing the edges (streets) of the walking street network.\n",
    "        - walk_nodes: A GeoDataFrame containing the nodes (intersections) of the walking street network.\n",
    "        - drive_edges: A GeoDataFrame containing the edges (streets) of the driving street network.\n",
    "        - drive_nodes: A GeoDataFrame containing the nodes (intersections) of the driving street network.\n",
    "        - common_nodes_gdf: A GeoDataFrame containing the common nodes between the driving and walking street networks.\n",
    "        \"\"\"\n",
    "\n",
    "        # Reset boundary_buffered crs for passing to OSM\n",
    "        boundary_buffered_4326 = boundary_buffered.to_crs('4326')\n",
    "\n",
    "        # Get street networks\n",
    "        all_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='all', simplify=False)\n",
    "        walk_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='walk', simplify=True)\n",
    "        drive_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='drive', simplify=False)\n",
    "\n",
    "        all_edges = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "        all_nodes = ox.graph_to_gdfs(all_streets, nodes=True, edges=False)\n",
    "\n",
    "        walk_edges = ox.graph_to_gdfs(walk_streets, nodes=False, edges=True)\n",
    "        walk_nodes = ox.graph_to_gdfs(walk_streets, nodes=True, edges=False)\n",
    "\n",
    "        drive_edges = ox.graph_to_gdfs(drive_streets, nodes=False, edges=True)\n",
    "        drive_nodes = ox.graph_to_gdfs(drive_streets, nodes=True, edges=False)\n",
    "\n",
    "        # Find the common nodes between networks\n",
    "        # This ensures that shortest paths between points should always be able to be calculated\n",
    "        common_nodes = drive_nodes.merge(walk_nodes, on='osmid', suffixes=('_drive', '_walk'))\n",
    "        common_nodes_gdf = gpd.GeoDataFrame(common_nodes, geometry='geometry_drive')\n",
    "\n",
    "        return all_edges, all_nodes, walk_edges, walk_nodes, drive_edges, drive_nodes, common_nodes_gdf, all_streets, walk_streets, drive_streets\n",
    "\n",
    "\n",
    "    # get street networks\n",
    "    all_edges, all_nodes, walk_edges, walk_nodes, drive_edges, drive_nodes, common_nodes_gdf, all_streets, walk_streets, drive_streets = get_OSM_street_networks(boundary_buffered)\n",
    "    #os_open_roads = get_OS_roads()  this is now got at the start of the code to avoid re-reading\n",
    "\n",
    "\n",
    "    def retrieve_osm_features(polygon, tags):\n",
    "        \"\"\"\n",
    "        Retrieves OpenStreetMap features based on the specified polygon and tags.\n",
    "\n",
    "        Args:\n",
    "            polygon (Polygon): The polygon to retrieve features within.\n",
    "            tags (dict): The tags to filter the features.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: The retrieved OpenStreetMap features.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            features = ox.features_from_polygon(polygon=polygon, tags=tags)\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            if \"There are no data elements in the server response\" in error_message:\n",
    "                print(\"No data elements found for the specified location/tags.\")\n",
    "                features = gpd.GeoDataFrame()  # Create an empty GeoDataFrame\n",
    "            else:\n",
    "                # Handle other exceptions here if needed\n",
    "                print(\"An error occurred:\", error_message)\n",
    "                features = None\n",
    "        return features\n",
    "\n",
    "\n",
    "    def get_railways(place):\n",
    "        \"\"\"\n",
    "        This retrievies and processes OpenStreetMap (OSM) railways data for a specified place.\n",
    "\n",
    "        Args:\n",
    "            place (str): The name of the place to retrieve OSM features for.\n",
    "\n",
    "        Returns:\n",
    "            railways (geopandas.GeoDataFrame): A GeoDataFrame containing the railways within the specified place.\n",
    "        \"\"\"\n",
    "\n",
    "        # for unknown reasons, using rail = ox.graph_from_place(place, custom_filter='[\"railway\"]')\n",
    "        # doesn't ALWAYS retrive the full rail network, hence why multiple lines are used to achive the same result\n",
    "\n",
    "        # Define railway types to retrieve\n",
    "        railway_types = [\"\", \"rail\", \"light_rail\", \"narrow_gauge\", \"subway\", \"tram\"]\n",
    "\n",
    "        # Initialize an empty graph\n",
    "        combined_railways = nx.MultiDiGraph()\n",
    "\n",
    "        for railway_type in railway_types:\n",
    "            try:\n",
    "                # Fetch the railway network for the specified type\n",
    "                network = ox.graph_from_place(place, simplify=False, custom_filter=f'[\"railway\"~\"{railway_type}\"]')\n",
    "\n",
    "                # Ensure the fetched network is a MultiDiGraph\n",
    "                if not isinstance(network, nx.MultiDiGraph):\n",
    "                    network = nx.MultiDiGraph(network)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"No railway data found for '{railway_type}'.\")\n",
    "                network = nx.MultiDiGraph()\n",
    "\n",
    "            # Compose the networks\n",
    "            combined_railways = nx.compose(combined_railways, network)\n",
    "\n",
    "        # Convert to GeoDataFrame\n",
    "        railways = ox.graph_to_gdfs(combined_railways, nodes=False, edges=True)\n",
    "\n",
    "        # Drop any other railway types that aren't needed\n",
    "        railways = railways.loc[(~railways[\"railway\"].isin([\"tunnel\", \"abandoned\", \"razed\", \"disused\", \"funicular\", \"monorail\", \"miniature\"]))]\n",
    "\n",
    "        # Drop rows where any of the specified columns have values \"True\" or \"yes\"\n",
    "        columns_to_check = ['tunnel', 'abandoned', 'razed', 'disused', 'funicular', 'monorail', 'miniature']\n",
    "        railways = railways.loc[~railways[railways.columns.intersection(columns_to_check)].isin(['True', 'yes']).any(axis=1)]\n",
    "\n",
    "        # Set railways CRS\n",
    "        railways = railways.to_crs('EPSG:27700')\n",
    "\n",
    "        return railways\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ## get urban footprints from GUF\n",
    "\n",
    "    def get_guf(place):\n",
    "        \"\"\"\n",
    "        Retrieves a clipped GeoDataFrame of GUF urban areas within a specified place boundary.\n",
    "\n",
    "        Parameters:\n",
    "        - place (str): The name or address of the place to retrieve urban areas for.\n",
    "\n",
    "        Returns:\n",
    "        - gdf_clipped (GeoDataFrame): A GeoDataFrame containing the clipped urban areas within the specified place boundary.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Access the WMS Service\n",
    "        wms_url = 'https://geoservice.dlr.de/eoc/land/wms?GUF04_DLR_v1_Mosaic'\n",
    "        wms = WebMapService(wms_url, version='1.1.1')\n",
    "\n",
    "        # Step 2: Identify the Layer with ID 102. This is the Global Urban Footprint layer GUF\n",
    "        for layer_name, layer in wms.contents.items():\n",
    "            if '102' in layer_name:\n",
    "                print(f\"Layer ID 102 found: {layer_name}\")\n",
    "\n",
    "        # Assuming 'GUF04_DLR_v1_Mosaic' is the layer with ID 102\n",
    "        layer = 'GUF04_DLR_v1_Mosaic'  # Replace with the actual layer name if different\n",
    "\n",
    "        # Step 3: Get the polygon boundary using osmnx\n",
    "        boundary_gdf = ox.geocode_to_gdf(place)\n",
    "        boundary = boundary_gdf.to_crs('EPSG:27700')\n",
    "        # buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "        boundary_buffered = boundary.buffer(100)\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "        boundary_polygon = boundary_gdf.geometry[0]\n",
    "        wms_boundary = boundary_buffered.geometry[0]\n",
    "\n",
    "        # Convert the polygon to a bounding box\n",
    "        minx, miny, maxx, maxy = wms_boundary.bounds\n",
    "\n",
    "        # Step 4: Request the data from WMS using the bounding box\n",
    "        width = 1024\n",
    "        height = 1024\n",
    "        response = wms.getmap(\n",
    "            layers=[layer],\n",
    "            srs='EPSG:4326',\n",
    "            bbox=(minx, miny, maxx, maxy),\n",
    "            size=(width, height),\n",
    "            format='image/geotiff'\n",
    "        )\n",
    "\n",
    "        # Step 5: Load the Raster Data into Rasterio\n",
    "        with MemoryFile(response.read()) as memfile:\n",
    "            with memfile.open() as src:\n",
    "                image = src.read(1)  # Read the first band\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "\n",
    "                # Clip the raster data to the polygon\n",
    "                out_image, out_transform = rio_mask(src, [mapping(wms_boundary)], crop=True)  # Use renamed mask function\n",
    "                out_meta = src.meta.copy()\n",
    "                out_meta.update({\"driver\": \"GTiff\",\n",
    "                                \"height\": out_image.shape[1],\n",
    "                                \"width\": out_image.shape[2],\n",
    "                                \"transform\": out_transform,\n",
    "                                \"crs\": crs})\n",
    "\n",
    "        # Step 6: Convert Raster to Vector\n",
    "        mask_arr = (out_image[0] != 0).astype(np.uint8)  # Assuming non-zero values are urban areas\n",
    "\n",
    "        shapes_gen = shapes(mask_arr, mask=mask_arr, transform=out_transform)\n",
    "\n",
    "        polygons = []\n",
    "        for geom, value in shapes_gen:\n",
    "            polygons.append(shape(geom))\n",
    "\n",
    "        # Create a GeoDataFrame from the polygons\n",
    "        gdf = gpd.GeoDataFrame({'geometry': polygons}, crs=crs)\n",
    "\n",
    "        # Step 7: Create Buffers Around Urban Areas\n",
    "        buffer_distance = 100  # Buffer distance in meters (adjust as needed)\n",
    "        gdf_buffered = gdf.copy()\n",
    "        gdf_buffered['geometry'] = gdf['geometry'].buffer(buffer_distance)\n",
    "\n",
    "        # Step 8: Clip the GeoDataFrame to the boundary of the place\n",
    "        gdf_clipped = gpd.clip(gdf, boundary_gdf)\n",
    "\n",
    "        return gdf_clipped\n",
    "\n",
    "\n",
    "    guf = get_guf(place)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## get residential areas\n",
    "    def get_residential_areas(polygon):\n",
    "        polygon = polygon.to_crs('EPSG:4326')\n",
    "        # Retrieve features from OpenStreetMap\n",
    "        features = ox.features_from_polygon(polygon.iloc[0], tags={'landuse': 'residential'})\n",
    "        \n",
    "        # Convert features to a GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame.from_features(features)\n",
    "        gdf = gdf.set_crs('EPSG:4326')\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    residential_areas = get_residential_areas(boundary_buffered)\n",
    "        \n",
    "        \n",
    "\n",
    "    ## join urban foot prints and residential areas\n",
    "    # this is to create a single polygon of where neighbourhoods can be found within\n",
    "\n",
    "    def join_geodataframes(gdf1, gdf2):\n",
    "        # Ensure both GeoDataFrames have the exact same CRS\n",
    "        target_crs = 'EPSG:4326'  # WGS 84\n",
    "        gdf1 = gdf1.to_crs(target_crs)\n",
    "        gdf2 = gdf2.to_crs(target_crs)\n",
    "        \n",
    "        # Concatenate GeoDataFrames\n",
    "        joined_gdf = pd.concat([gdf1, gdf2], ignore_index=True)\n",
    "        \n",
    "        return gpd.GeoDataFrame(joined_gdf, crs=target_crs)\n",
    "\n",
    "\n",
    "    guf_residential_gdf = join_geodataframes(guf, residential_areas)\n",
    "\n",
    "\n",
    "    ## create a small buffer to ensure all areas a captured correctly\n",
    "\n",
    "    def buffer_geometries_in_meters(gdf, distance):\n",
    "        # Define the World Mercator projected CRS\n",
    "        projected_crs = 'EPSG:3395'  # World Mercator\n",
    "\n",
    "        # Project to the new CRS\n",
    "        gdf_projected = gdf.to_crs(projected_crs)\n",
    "        \n",
    "        # Buffer the geometries\n",
    "        gdf_projected['geometry'] = gdf_projected['geometry'].buffer(distance)\n",
    "        \n",
    "        # Reproject back to the original CRS\n",
    "        gdf_buffered = gdf_projected.to_crs(gdf.crs)\n",
    "        \n",
    "        return gdf_buffered\n",
    "\n",
    "\n",
    "    guf_residential_gdf = buffer_geometries_in_meters(guf_residential_gdf, 100)  # Buffer by 100 meters\n",
    "\n",
    "\n",
    "    ## union into one gdf\n",
    "\n",
    "    def unary_union_polygons(gdf):\n",
    "        # Combine all geometries into a single geometry\n",
    "        unified_geometry = unary_union(gdf['geometry'])\n",
    "        \n",
    "        # Create a new GeoDataFrame with a single row containing the unified geometry\n",
    "        combined_gdf = gpd.GeoDataFrame({'geometry': [unified_geometry]}, crs=gdf.crs)\n",
    "        \n",
    "        return combined_gdf\n",
    "\n",
    "\n",
    "    guf_residential_gdf = unary_union_polygons(guf_residential_gdf)\n",
    "\n",
    "    # set to BNG\n",
    "    guf_residential_gdf = guf_residential_gdf.to_crs(\"27700\")\n",
    "\n",
    "    # Function to remove holes from neighbourhoods\n",
    "    def remove_holes(polygon):\n",
    "        if polygon.geom_type == 'Polygon':\n",
    "            return Polygon(polygon.exterior)\n",
    "        else:\n",
    "            return polygon\n",
    "\n",
    "    # remove holes from urban footprint\n",
    "    guf_residential_gdf['geometry'] = guf_residential_gdf['geometry'].apply(remove_holes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_rivers(boundary_buffered):\n",
    "        \"\"\"\n",
    "        Retrieves river features within a given boundary.\n",
    "\n",
    "        Args:\n",
    "            boundary_buffered (GeoDataFrame): A GeoDataFrame representing the buffered boundary.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: A GeoDataFrame containing the river features within the boundary.\n",
    "        \"\"\"\n",
    "        # Ensure the boundary is in the correct CRS for the query\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "        # Check the content of boundary_buffered to ensure it's not empty and correctly transformed\n",
    "        if boundary_buffered.empty:\n",
    "            raise ValueError(\"The provided boundary is empty.\")\n",
    "\n",
    "        # Define the tags for waterways\n",
    "        tags = {\"waterway\": [\"river\", \"rapids\"]}\n",
    "\n",
    "        try:\n",
    "            # Fetch features from OSM using the boundary geometry\n",
    "            polygon = boundary_buffered.geometry.iloc[0]\n",
    "            rivers = ox.features_from_polygon(polygon=polygon, tags=tags)\n",
    "\n",
    "            # Dropping rows where 'tunnel' is equal to 'culvert'\n",
    "            if 'tunnel' in rivers.columns:\n",
    "                rivers = rivers[rivers['tunnel'] != 'culvert']\n",
    "\n",
    "            # Convert the CRS back to the desired one\n",
    "            rivers = rivers.to_crs('EPSG:27700')\n",
    "\n",
    "            # Set the geometry column explicitly\n",
    "            rivers = rivers.set_geometry('geometry')\n",
    "\n",
    "            return rivers\n",
    "\n",
    "        except InsufficientResponseError:\n",
    "            print(\"No data elements found for the given boundary and tags.\")\n",
    "            empty_geometry = {'geometry': [LineString()]}\n",
    "            rivers = gpd.GeoDataFrame(empty_geometry, crs='EPSG:27700')\n",
    "            return rivers # Return an empty GeoDataFrame if no data found\n",
    "\n",
    "\n",
    "\n",
    "    def get_landuse(boundary_buffered):\n",
    "        \"\"\"\n",
    "        Retrieves the landuse features based on the specified boundary.\n",
    "\n",
    "        Args:\n",
    "            boundary_buffered (GeoDataFrame): The buffered boundary polygon.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: The landuse features.\n",
    "        \"\"\"\n",
    "        # reset boundary crs to allow for features to be found\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "        # Define tags\n",
    "        tags = {\"landuse\": [\"industrial\", \"railway\", \"brownfield\", \"commercial\", \"farmland\", \"meadow\"]}\n",
    "        # Use ox.features_from_polygon to find features matching the specified tags\n",
    "        landuse = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "        # set/reset crs\n",
    "        landuse = landuse.to_crs('27700')\n",
    "\n",
    "        ## get unsuitable \"nature\" types\n",
    "        # Define tags\n",
    "        tags = {\"natural\": [\"wood\", \"water\", \"scrub\", \"coastline\", \"beach\"]}\n",
    "        # Use ox.features_from_polygon to find features matching the specified tags\n",
    "        nature = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "        # set/reset crs\n",
    "        nature = nature.to_crs('27700')\n",
    "\n",
    "        ## get unsuitable \"lesiure\" types. This is mainly for golfcourses\n",
    "        # Define tags\n",
    "        tags = {\"leisure\": [\"golf_course\", \"track\", \"park\"]}\n",
    "        # Use ox.features_from_polygon to find features matching the specified tags\n",
    "        leisure = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "        # set/reset crs\n",
    "        leisure = leisure.to_crs('27700')\n",
    "        # Define the tags for aeroway\n",
    "        aeroway_tags = {\"aeroway\": [\"aerodrome\"]}\n",
    "        # Use the function to retrieve aeroway features\n",
    "        aeroway = retrieve_osm_features(polygon=boundary_buffered.iloc[0], tags=aeroway_tags)\n",
    "        # Check if any features were retrieved\n",
    "        if aeroway is not None:\n",
    "            if not aeroway.empty:\n",
    "                # set/reset crs\n",
    "                aeroway = aeroway.to_crs('27700')\n",
    "\n",
    "        # concat\n",
    "        landuse = pd.concat([landuse, nature, leisure, aeroway])\n",
    "\n",
    "        ## resest boundary crs\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:27700')\n",
    "\n",
    "        return landuse\n",
    "\n",
    "\n",
    "    def get_bus_routes(boundary_buffered):\n",
    "            \"\"\"\n",
    "            Retrieves bus routes from OSM/NAPTAN within a given boundary.\n",
    "\n",
    "            Args:\n",
    "                    boundary_buffered (GeoDataFrame): A GeoDataFrame representing the boundary.\n",
    "\n",
    "            Returns:\n",
    "                    bus_routes (GeoDataFrame): A GeoDataFrame containing the bus routes.\n",
    "\n",
    "            Raises:\n",
    "                    Exception: If there is an error fetching the data from the Overpass API.\n",
    "            \"\"\"\n",
    "            # reset boundary crs to allow for features to be found\n",
    "            boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "            # Calculate the bounding box for XML query\n",
    "            bounding_box = boundary_buffered.bounds\n",
    "\n",
    "            # Extract the minimum and maximum coordinates\n",
    "            minx = bounding_box['minx'].min()\n",
    "            miny = bounding_box['miny'].min()\n",
    "            maxx = bounding_box['maxx'].max()\n",
    "            maxy = bounding_box['maxy'].max()\n",
    "\n",
    "            # Create a list of four elements representing the bounding box\n",
    "            bbox = [minx, miny, maxx, maxy]\n",
    "\n",
    "            # reset boundary_buffer crs\n",
    "            boundary_buffered = boundary_buffered.to_crs('27700')\n",
    "\n",
    "            # Define the Overpass API endpoint\n",
    "            overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "            # Define the XML query\n",
    "            xml_query = f\"\"\"\n",
    "            <osm-script output=\"json\" output-config=\"\" timeout=\"160\">\n",
    "                <union into=\"_\">\n",
    "                    <query into=\"_\" type=\"node\">\n",
    "                        <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                        <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                    </query>\n",
    "                    <query into=\"_\" type=\"way\">\n",
    "                        <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                        <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                    </query>\n",
    "                    <query into=\"_\" type=\"relation\">\n",
    "                        <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                        <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                    </query>\n",
    "                </union>\n",
    "                <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"body\" n=\"\" order=\"id\" s=\"\" w=\"\"/>\n",
    "                <recurse from=\"_\" into=\"_\" type=\"down\"/>\n",
    "                <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"skeleton\" n=\"\" order=\"quadtile\" s=\"\" w=\"\"/>\n",
    "            </osm-script>\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # Initialize lists to store data\n",
    "            geometries = []\n",
    "            element_data = []\n",
    "\n",
    "            # Make the Overpass API request\n",
    "            response = requests.post(overpass_url, data=xml_query)\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "\n",
    "                    # Access the data from the response\n",
    "                    for element in data.get(\"elements\", []):\n",
    "                            if element.get('type') == 'way' and 'geometry' in element:\n",
    "                                    # Extract geometry coordinates from 'geometry' field\n",
    "                                    coordinates = [(node['lon'], node['lat']) for node in element['geometry']]\n",
    "                                    # Create a LineString geometry\n",
    "                                    line = LineString(coordinates)\n",
    "                                    geometries.append(line)\n",
    "                                    element_data.append(element)\n",
    "\n",
    "                    # Create a GeoDataFrame\n",
    "                    bus_routes = gpd.GeoDataFrame(element_data, geometry=geometries)\n",
    "\n",
    "                    # Set CRS\n",
    "                    bus_routes = bus_routes.set_crs('4326')\n",
    "                    bus_routes = bus_routes.to_crs('27700')\n",
    "\n",
    "                    return bus_routes\n",
    "\n",
    "            else:\n",
    "                    raise Exception(f\"Error fetching data: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "\n",
    "    def clip_boundaries(os_open_roads, rivers, railways, landuse, bus_routes, boundary_buffered):\n",
    "        \"\"\"\n",
    "        Clips the geospatial data to the boundary_buffered extent.\n",
    "\n",
    "        Parameters:\n",
    "        - os_open_roads (GeoDataFrame): lines from OS Open roads.\n",
    "        - rivers (GeoDataFrame): lines of Rivers.\n",
    "        - railways (GeoDataFrame): lines of Railways.\n",
    "        - landuse (GeoDataFrame): Land use polygons.\n",
    "        - bus_routes (GeoDataFrame): lines of bus routes.\n",
    "        - boundary_buffered (GeoDataFrame): buffered boundary.\n",
    "\n",
    "        Returns:\n",
    "        - clipped versions of input geodataframes, aside from the bufferd boundary.\n",
    "        \"\"\"\n",
    "        os_open_roads_clip = gpd.clip(os_open_roads, boundary_buffered)\n",
    "        rivers_clip = gpd.clip(rivers, boundary_buffered)\n",
    "        railways_clip = gpd.clip(railways, boundary_buffered)\n",
    "        landuse_clip = gpd.clip(landuse, boundary_buffered)\n",
    "        bus_routes_clip = gpd.clip(bus_routes, boundary_buffered)\n",
    "\n",
    "        return os_open_roads_clip, rivers_clip, railways_clip, landuse_clip, bus_routes_clip\n",
    "\n",
    "\n",
    "    def process_bus_routes(bus_routes_clip, buffer_distance):\n",
    "        \"\"\"\n",
    "        Count the number of bus routes per road and remove roads with more than one bus route on them.\n",
    "        \n",
    "        Args:\n",
    "            bus_routes_clip (GeoDataFrame): The input GeoDataFrame containing bus routes.\n",
    "            buffer_distance (float): The buffer distance to convert roads to polygons, set in meters.\n",
    "        \n",
    "        Returns:\n",
    "            GeoDataFrame: The filtered GeoDataFrame containing roads with less than or equal to one bus route.\n",
    "        \"\"\"\n",
    "        # Create a new GeoDataFrame with the buffered geometries\n",
    "        bus_routes_buffered = bus_routes_clip.copy()  # Copy the original GeoDataFrame\n",
    "        bus_routes_buffered['geometry'] = bus_routes_buffered['geometry'].buffer(buffer_distance)\n",
    "\n",
    "        # count the number of overlapping bus routes\n",
    "        def count_overlapping_features(gdf):\n",
    "            \"\"\"\n",
    "            Count the number of overlapping features in a GeoDataFrame.\n",
    "            \n",
    "            Args:\n",
    "                gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "            \n",
    "            Returns:\n",
    "                GeoDataFrame: The input GeoDataFrame with an additional column 'Bus_routes_count' indicating the count of overlapping features.\n",
    "            \"\"\"\n",
    "            # Create an empty column to store the count of overlapping features\n",
    "            gdf['Bus_routes_count'] = 0\n",
    "\n",
    "            # Iterate through each row in the GeoDataFrame\n",
    "            for idx, row in gdf.iterrows():\n",
    "                # Get the geometry of the current row\n",
    "                geometry = row['geometry']\n",
    "                \n",
    "                # Use a spatial filter to find overlapping features\n",
    "                overlaps = gdf[gdf['geometry'].intersects(geometry)]\n",
    "                \n",
    "                # Update the Bus_routes_count column with the count of overlapping features\n",
    "                gdf.at[idx, 'Bus_routes_count'] = len(overlaps)\n",
    "            \n",
    "            return gdf\n",
    "\n",
    "        # call function\n",
    "        bus_routes_buffered_with_count = count_overlapping_features(bus_routes_buffered)\n",
    "\n",
    "        # drop any roads which have less than two bus routes on them\n",
    "        bus_routes_filtered = bus_routes_buffered_with_count[bus_routes_buffered_with_count['Bus_routes_count'] >= 2]\n",
    "        \n",
    "        return bus_routes_filtered\n",
    "\n",
    "\n",
    "\n",
    "    def filter_OS_boundary_roads(os_open_roads_clip):\n",
    "        \"\"\"\n",
    "        Filter the `os_open_roads_clip` DataFrame to select boundary roads.\n",
    "\n",
    "        This function filters the `os_open_roads_clip` DataFrame to select roads that are considered \"boundary\" roads. \n",
    "        The selection criteria include roads that have the following attributes:\n",
    "        - `primary_route` is True\n",
    "        - `trunk_road` is True\n",
    "        - `fictitious` is True\n",
    "        - `road_classification` is 'A Road' or 'B Road'\n",
    "        - `road_function` is 'Minor Road' or 'Motorway'\n",
    "\n",
    "        The filtered DataFrame is returned.\n",
    "\n",
    "        Note: The commented line `(os_open_roads_clip['road_function'] == 'Restricted Local Access Road')` is excluded from the selection.\n",
    "\n",
    "        Parameters:\n",
    "        - os_open_roads_clip (DataFrame): A DataFrame containing road data.\n",
    "\n",
    "        Returns:\n",
    "        - boundary_roads (DataFrame): A DataFrame containing the filtered boundary roads.\n",
    "\n",
    "        Example usage:\n",
    "            # Assuming `os_open_roads_clip` is a DataFrame containing road data\n",
    "            boundary_roads = filter_boundary_roads(os_open_roads_clip)\n",
    "        \"\"\"\n",
    "        boundary_roads = os_open_roads_clip.loc[((os_open_roads_clip['primary_route'] == 'True') |\n",
    "                        (os_open_roads_clip['trunk_road'] == 'True') |\n",
    "                        (os_open_roads_clip['fictitious'] == 'True') |\n",
    "                        (os_open_roads_clip['road_classification'] == 'A Road') | \n",
    "                        (os_open_roads_clip['road_classification'] == 'B Road') | \n",
    "                        (os_open_roads_clip['road_function'] == 'Minor Road') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Motorway') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Minor Road')  \n",
    "                        )]\n",
    "        return boundary_roads\n",
    "\n",
    "\n",
    "\n",
    "    ## buffering and dissolving functions\n",
    "    \n",
    "    def buffer_and_dissolve(input_gdf):\n",
    "        \"\"\"\n",
    "        Buffer and dissolve a GeoDataFrame.\n",
    "        \n",
    "        Args:\n",
    "            input_gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            GeoDataFrame: The buffered and dissolved GeoDataFrame.\n",
    "        \"\"\"\n",
    "        # Buffer around boundaries\n",
    "        buffered_gdf = input_gdf.copy()  # Create a copy to avoid modifying the original\n",
    "        buffered_gdf['geometry'] = buffered_gdf['geometry'].buffer(5) # set a 5 meter buffer\n",
    "\n",
    "        # Dissolve the geometries\n",
    "        dissolved_geo = buffered_gdf.unary_union\n",
    "\n",
    "        # Create a new GeoDataFrame with the dissolved geometry\n",
    "        dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "\n",
    "        # Set the CRS (Coordinate Reference System)\n",
    "        dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "        return dissolved_gdf\n",
    "\n",
    "\n",
    "    def dissolve_gdf(input_gdf):\n",
    "        # dissolve geometries\n",
    "        dissolved_geo = input_gdf.unary_union\n",
    "        dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "        dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "        return dissolved_gdf\n",
    "\n",
    "\n",
    "    def erase_boundary_features(boundary, boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd, guf_residential_gdf):\n",
    "        \"\"\"\n",
    "        Erases boundary features from the given boundary geometry.\n",
    "\n",
    "        Parameters:\n",
    "        - boundary: GeoDataFrame representing the boundary geometry\n",
    "        - boundary_rivers_bd: GeoDataFrame representing the rivers boundary features\n",
    "        - boundary_roads_bd: GeoDataFrame representing the roads boundary features\n",
    "        - boundary_rail_bd: GeoDataFrame representing the rail boundary features\n",
    "        - boundary_landuse_bd: GeoDataFrame representing the landuse boundary features\n",
    "        - boundary_bus_routes_bd: GeoDataFrame representing the bus routes boundary features\n",
    "\n",
    "        Returns:\n",
    "        - erased_boundary_gdf: GeoDataFrame containing the result of the \"Erase\" operation\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure that neighbourhoods fall only within urban footprint areas\n",
    "        boundary = gpd.clip(boundary, guf_residential_gdf)\n",
    "\n",
    "        # Join all boundary features\n",
    "        boundaries = pd.concat([boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd], ignore_index=True)\n",
    "        boundary_features = dissolve_gdf(boundaries)\n",
    "\n",
    "        # Use the `difference` method to perform the \"Erase\" operation\n",
    "        erased_boundary = boundary.difference(boundary_features.unary_union)\n",
    "\n",
    "        # Convert the GeoSeries to a single geometry using unary_union\n",
    "        erased_boundary = erased_boundary.unary_union\n",
    "\n",
    "        # Create a new GeoDataFrame with the result of \"Erase\" operation\n",
    "        erased_boundary_gdf = gpd.GeoDataFrame(geometry=[erased_boundary], crs=boundary.crs)\n",
    "\n",
    "        # Explode multipolygon to polygons\n",
    "        erased_boundary_gdf = erased_boundary_gdf.explode()\n",
    "\n",
    "        return erased_boundary_gdf\n",
    "\n",
    "\n",
    "    def drop_large_or_small_areas(neighbourhoods):\n",
    "        \"\"\"\n",
    "        Drops rows from the 'neighbourhoods' DataFrame where the area is less than 10,000 square units or greater than 5,000,000 square units.\n",
    "\n",
    "        Parameters:\n",
    "        - neighbourhoods (DataFrame): The input DataFrame containing neighbourhood data.\n",
    "\n",
    "        Returns:\n",
    "        - neighbourhoods (DataFrame): The updated DataFrame with small areas dropped.\n",
    "        \"\"\"\n",
    "        # Calculate area\n",
    "        neighbourhoods[\"area\"] = neighbourhoods.geometry.area\n",
    "\n",
    "        # Drop rows where area is less than 10,000 or greater than 5,000,000\n",
    "        neighbourhoods = neighbourhoods.loc[(neighbourhoods[\"area\"] >= 10000)]\n",
    "        neighbourhoods = neighbourhoods.loc[(neighbourhoods[\"area\"] <= 5000000)]\n",
    "\n",
    "        return neighbourhoods\n",
    "\n",
    "\n",
    "    def filter_neighbourhoods_by_roads(neighbourhoods, os_open_roads_clip, polygon_column_name):\n",
    "        \"\"\"\n",
    "        Count the number of roads within each polygon in a GeoDataFrame and filter the neighbourhoods based on road count and road density.\n",
    "        \n",
    "        Args:\n",
    "            neighbourhoods (GeoDataFrame): GeoDataFrame containing neighbourhood polygons.\n",
    "            os_open_roads_clip (GeoDataFrame): GeoDataFrame containing road data.\n",
    "            polygon_column_name (str): Name of the column in neighbourhoods to use for grouping.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: Updated neighbourhoods GeoDataFrame with filtered rows based on road count and road density.\n",
    "        \"\"\"\n",
    "        \n",
    "        def count_roads_within_polygons(polygons_gdf, roads_gdf, polygon_column_name):\n",
    "            \"\"\"\n",
    "            Count the number of roads within each polygon in a GeoDataFrame.\n",
    "            \n",
    "            Args:\n",
    "                polygons_gdf (GeoDataFrame): GeoDataFrame containing polygons.\n",
    "                roads_gdf (GeoDataFrame): GeoDataFrame containing roads.\n",
    "                polygon_column_name (str): Name of the column in polygons_gdf to use for grouping.\n",
    "\n",
    "            Returns:\n",
    "                GeoDataFrame: Original polygons GeoDataFrame with a \"road_count\" column added.\n",
    "            \"\"\"\n",
    "            \n",
    "            # spatial join\n",
    "            joined = gpd.sjoin(polygons_gdf, roads_gdf, how='left', op='intersects')\n",
    "            \n",
    "            # Group by the polygon column and count the number of roads in each\n",
    "            road_counts = joined.groupby(polygon_column_name).size().reset_index(name='road_count')\n",
    "            \n",
    "            # Merge the road counts back into the polygons GeoDataFrame\n",
    "            polygons_gdf = polygons_gdf.merge(road_counts, on=polygon_column_name, how='left')\n",
    "\n",
    "            # Calculate road density (area divided by road_count). It is multiplied by 10000 for ease of understanding the numbers involved with this\n",
    "            polygons_gdf['road_density'] = (polygons_gdf['road_count'] / polygons_gdf['area'] ) * 10000\n",
    "            \n",
    "            return polygons_gdf\n",
    "        \n",
    "        neighbourhoods = count_roads_within_polygons(neighbourhoods, os_open_roads_clip, polygon_column_name)\n",
    "\n",
    "        # Drop rows with road_density below 0.2 or less than 4 roads\n",
    "        neighbourhoods = neighbourhoods[(neighbourhoods['road_count'] > 2)]\n",
    "        neighbourhoods = neighbourhoods[(neighbourhoods['road_density'] > 0.2)]\n",
    "        \n",
    "        return neighbourhoods\n",
    "\n",
    "\n",
    "    def remove_holes(polygon):\n",
    "        \"\"\"\n",
    "        Removes holes from a polygon. Mostly for visual reasons.\n",
    "\n",
    "        Parameters:\n",
    "        polygon (Polygon): The polygon to remove holes from.\n",
    "\n",
    "        Returns:\n",
    "        Polygon: The polygon without holes.\n",
    "        \"\"\"\n",
    "        if polygon.geom_type == 'Polygon':\n",
    "            return Polygon(polygon.exterior)\n",
    "        else:\n",
    "            return polygon\n",
    "\n",
    "    landuse = get_landuse(boundary_buffered)\n",
    "    rivers = get_rivers(boundary_buffered)\n",
    "    railways = get_railways(place)\n",
    "    landuse = get_landuse(boundary_buffered)\n",
    "    bus_routes = get_bus_routes(boundary_buffered)\n",
    "    os_open_roads_clip, rivers_clip, railways_clip, landuse_clip, bus_routes_clip = clip_boundaries(os_open_roads, rivers, railways, landuse, bus_routes, boundary_buffered)\n",
    "    bus_routes_clip = process_bus_routes(bus_routes_clip, 0.2)\n",
    "    boundary_roads = filter_OS_boundary_roads(os_open_roads_clip)\n",
    "\n",
    "    ## buffer and dissolve \n",
    "    boundary_roads_bd = buffer_and_dissolve(boundary_roads)\n",
    "    boundary_rivers_bd = buffer_and_dissolve(rivers_clip)\n",
    "    boundary_rail_bd = buffer_and_dissolve(railways_clip)\n",
    "    boundary_landuse_bd = buffer_and_dissolve(landuse_clip)\n",
    "    boundary_bus_routes_bd = buffer_and_dissolve(bus_routes_clip)\n",
    "\n",
    "    ## geodataframe cleaning\n",
    "    erased_boundary_gdf = erase_boundary_features(boundary, boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd, guf_residential_gdf)\n",
    "    neighbourhoods = erased_boundary_gdf\n",
    "    neighbourhoods = drop_large_or_small_areas(neighbourhoods)\n",
    "\n",
    "    neighbourhoods = filter_neighbourhoods_by_roads(neighbourhoods, os_open_roads_clip, 'geometry')\n",
    "\n",
    "    ## create unique IDs\n",
    "    # simple number based ID\n",
    "    neighbourhoods['ID'] = range(1, len(neighbourhoods) + 1)\n",
    "\n",
    "    neighbourhoods['geometry'] = neighbourhoods['geometry'].apply(remove_holes)\n",
    "\n",
    "\n",
    "    ## filter neighbourhoods to only locations with more than 1 intersection (1 or fewer intersections indicates that all travel modes will be the same)\n",
    "    # reset neighbourhoods crs\n",
    "    neighbourhoods = neighbourhoods.to_crs('4326')\n",
    "\n",
    "    # Spatial join to count points within each neighborhood\n",
    "    spatial_join = gpd.sjoin(neighbourhoods, common_nodes_gdf, how='left', op='contains')\n",
    "\n",
    "    # Group by 'ID' and count the points within each neighborhood\n",
    "    point_counts = spatial_join.groupby('ID').size().reset_index(name='point_count')\n",
    "\n",
    "    # Filter out neighborhoods with 1 or 0 points\n",
    "    filtered_neighbourhood_ids = point_counts[point_counts['point_count'] > 1]['ID']\n",
    "\n",
    "    neighbourhoods= neighbourhoods[neighbourhoods['ID'].isin(filtered_neighbourhood_ids)]\n",
    "\n",
    "\n",
    "\n",
    "    ## we also need to join the length of the streets within the neighbourhood for further analysis\n",
    "    # Reset index of neighbourhoods\n",
    "    neighbourhoods = neighbourhoods.reset_index(drop=True)\n",
    "\n",
    "    # reset neighbourhoods crs\n",
    "    neighbourhoods = neighbourhoods.to_crs('27700')\n",
    "\n",
    "    # Perform a spatial join\n",
    "    joined_data = gpd.sjoin(os_open_roads_clip, neighbourhoods, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "    # Group by neighborhood and calculate total road length\n",
    "    road_lengths = joined_data.groupby('index_right')['length'].sum().reset_index()\n",
    "\n",
    "    # Merge road_lengths with neighbourhoods and drop 'index_right' column\n",
    "    neighbourhoods = neighbourhoods.merge(road_lengths, left_index=True, right_on='index_right', how='left').drop(columns=['index_right'])\n",
    "\n",
    "    # Rename the column\n",
    "    neighbourhoods.rename(columns={'length': 'road_lengths'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    ### find accessiablity\n",
    "\n",
    "    ## all to all\n",
    "    def calculate_distance_stats_from_points(points_gdf, network):\n",
    "        all_pairs_shortest_paths = {}\n",
    "        points_osmids = points_gdf.index.tolist()  # Assuming the 'osmid' is the index in the GeoDataFrame\n",
    "\n",
    "        for start_node in points_osmids:\n",
    "            shortest_paths = {}\n",
    "            try:\n",
    "                for end_node in points_osmids:\n",
    "                    if start_node != end_node:\n",
    "                        distance = nx.shortest_path_length(network, start_node, end_node, weight='length')\n",
    "                        shortest_paths[end_node] = distance\n",
    "                all_pairs_shortest_paths[start_node] = shortest_paths\n",
    "            except nx.NetworkXNoPath:\n",
    "                # If no path is found, skip adding to all_pairs_shortest_paths\n",
    "                continue\n",
    "\n",
    "        distances = [length for paths in all_pairs_shortest_paths.values() for length in paths.values()]\n",
    "\n",
    "        if not distances:\n",
    "            return {\n",
    "                \"mean_distance\": 0,\n",
    "                \"median_distance\": 0,\n",
    "                \"min_distance\": 0,\n",
    "                \"max_distance\": 0,\n",
    "                \"distance_range\": 0,\n",
    "                \"total_distance\": 0\n",
    "            }\n",
    "\n",
    "        mean_distance = statistics.mean(distances)\n",
    "        median_distance = statistics.median(distances)\n",
    "        min_distance = min(distances)\n",
    "        max_distance = max(distances)\n",
    "        distance_range = max_distance - min_distance\n",
    "        total_distance = sum(distances)\n",
    "\n",
    "        return {\n",
    "            \"mean_distance\": mean_distance,\n",
    "            \"median_distance\": median_distance,\n",
    "            \"min_distance\": min_distance,\n",
    "            \"max_distance\": max_distance,\n",
    "            \"distance_range\": distance_range,\n",
    "            \"total_distance\": total_distance\n",
    "        }\n",
    "\n",
    "    ## processing for all to all \n",
    "    results = []\n",
    "\n",
    "    for index, row in neighbourhoods.iterrows():\n",
    "        neighbourhood = neighbourhoods.loc[[index]]\n",
    "\n",
    "        ## get neighbourhood boundary and neighbourhood boundary buffer\n",
    "        # set crs\n",
    "        neighbourhood = neighbourhood.to_crs('3395')\n",
    "        # create a buffer neighbourhood\n",
    "        neighbourhood_buffer = neighbourhood['geometry'].buffer(15)\n",
    "        # convert back to a geodataframe (for later on)\n",
    "        neighbourhood_buffer = gpd.GeoDataFrame(geometry=neighbourhood_buffer)\n",
    "        # reset crs\n",
    "        neighbourhood, neighbourhood_buffer = neighbourhood.to_crs('4326'), neighbourhood_buffer.to_crs('4326')\n",
    "\n",
    "        ## get nodes which can be driven to and walked to within area\n",
    "        neighbourhood_nodes = gpd.clip(common_nodes_gdf, neighbourhood_buffer)\n",
    "\n",
    "        if neighbourhood_nodes.empty:\n",
    "            print(f\"No nodes found for neighbourhood {index}. Using default values.\")\n",
    "            walk_stats = {\n",
    "                \"mean_distance\": 0,\n",
    "                \"median_distance\": 0,\n",
    "                \"min_distance\": 0,\n",
    "                \"max_distance\": 0,\n",
    "                \"distance_range\": 0,\n",
    "                \"total_distance\": 0\n",
    "            }\n",
    "            drive_stats = walk_stats\n",
    "        else:\n",
    "            ## calculate neighbourhood distance stats for walking and driving\n",
    "            walk_stats = calculate_distance_stats_from_points(neighbourhood_nodes, walk_streets)\n",
    "            drive_stats = calculate_distance_stats_from_points(neighbourhood_nodes, drive_streets)\n",
    "\n",
    "        ## get length of total edges within the neighbourhood\n",
    "        edges_within_neighbourhood = gpd.sjoin(all_edges, neighbourhood, how=\"inner\", op=\"intersects\")\n",
    "        total_length = edges_within_neighbourhood['length'].sum()\n",
    "\n",
    "        ## Add the statistics to the GeoDataFrame\n",
    "        neighbourhood['walk_mean_distance'] = walk_stats['mean_distance']\n",
    "        neighbourhood['walk_median_distance'] = walk_stats['median_distance']\n",
    "        neighbourhood['walk_min_distance'] = walk_stats['min_distance']\n",
    "        neighbourhood['walk_max_distance'] = walk_stats['max_distance']\n",
    "        neighbourhood['walk_distance_range'] = walk_stats['distance_range']\n",
    "        neighbourhood['walk_total_distance'] = walk_stats['total_distance']\n",
    "\n",
    "        neighbourhood['drive_mean_distance'] = drive_stats['mean_distance']\n",
    "        neighbourhood['drive_median_distance'] = drive_stats['median_distance']\n",
    "        neighbourhood['drive_min_distance'] = drive_stats['min_distance']\n",
    "        neighbourhood['drive_max_distance'] = drive_stats['max_distance']\n",
    "        neighbourhood['drive_distance_range'] = drive_stats['distance_range']\n",
    "        neighbourhood['drive_total_distance'] = drive_stats['total_distance']\n",
    "\n",
    "        ## Store statistics along with neighborhood ID or other identifying information\n",
    "        result = {\n",
    "            'neighbourhood_id': neighbourhood['ID'].iloc[0],  # Assuming you have an ID column\n",
    "            'walk_mean_distance': walk_stats['mean_distance'],\n",
    "            'walk_median_distance': walk_stats['median_distance'],\n",
    "            'walk_total_distance': walk_stats['total_distance'],\n",
    "            \n",
    "            'drive_mean_distance': drive_stats['mean_distance'],\n",
    "            'drive_median_distance': drive_stats['median_distance'],\n",
    "            'drive_total_distance': drive_stats['total_distance'],\n",
    "\n",
    "            'total_edge_length': total_length\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    ## Convert the results to a new dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    ## calculate differences\n",
    "    results_df['mean_distance_diff'] = results_df['walk_mean_distance'] - results_df['drive_mean_distance']\n",
    "    results_df['median_distance_diff'] = results_df['walk_median_distance'] - results_df['drive_median_distance']\n",
    "    results_df['total_distance_diff'] = results_df['walk_total_distance'] - results_df['drive_total_distance']\n",
    "\n",
    "    merged_df = pd.merge(neighbourhoods, results_df, left_on=\"ID\", right_on=\"neighbourhood_id\")\n",
    "    access_results_gdf = gpd.GeoDataFrame(merged_df, geometry='geometry')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_barriers(boundary):\n",
    "        \"\"\"\n",
    "        Find modal filters within a given boundary.\n",
    "\n",
    "        Args:\n",
    "            boundary (geopandas.GeoDataFrame): A GeoDataFrame representing the boundary.\n",
    "\n",
    "        Returns:\n",
    "            barriers (geopandas.GeoDataFrame): A GeoDataFrame containing the modal filters.\n",
    "            streets_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing the streets from OSM.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # get the boundary in the correct CRS for OSMnx\n",
    "        boundary_4326 = boundary.to_crs('EPSG:4326')\n",
    "\n",
    "        # get the most \"basic\" filters mapped, the barriers/bollards etc\n",
    "        # Define tags\n",
    "        tags = {\"barrier\": [\"bollard\", \"bus_trap\", \"entrance\", \"planter\", \"sump_buster\", \"wedge\"]}\n",
    "\n",
    "        # Use ox.features_from_polygon to find features matching the specified tags\n",
    "        barriers = ox.features_from_polygon(polygon=boundary_4326.geometry.iloc[0], tags=tags)\n",
    "\n",
    "        # process any linestrings into point geometries\n",
    "        # Filter the GeoDataFrame to select only rows with \"linestring\" geometry\n",
    "        barriers_linestrings = barriers[barriers['geometry'].geom_type == 'LineString']\n",
    "\n",
    "        # Create an empty GeoDataFrame to store the individual points\n",
    "        points_gdf = gpd.GeoDataFrame(columns=list(barriers_linestrings.columns), crs=barriers_linestrings.crs)\n",
    "\n",
    "        # Iterate through each row in the GeoDataFrame with linestrings\n",
    "        for idx, row in barriers_linestrings.iterrows():\n",
    "            if isinstance(row['geometry'], LineString):\n",
    "                # Extract the individual points from the linestring\n",
    "                points = [Point(coord) for coord in list(row['geometry'].coords)]\n",
    "\n",
    "                # Create a GeoDataFrame from the individual points and copy the attributes\n",
    "                points_df = gpd.GeoDataFrame(geometry=points, crs=barriers_linestrings.crs)\n",
    "                for col in barriers_linestrings.columns:\n",
    "                    if col != 'geometry':\n",
    "                        points_df[col] = row[col]\n",
    "\n",
    "                # Rename the \"geometry\" column to \"merged_geometry\"\n",
    "                points_df = points_df.rename(columns={'geometry': 'merged_geometry'})\n",
    "\n",
    "                # Append the points to the points_gdf\n",
    "                points_gdf = pd.concat([points_gdf, points_df], ignore_index=True)\n",
    "\n",
    "        # Now, points_gdf contains all the individual points from the linestrings with inherited attributes\n",
    "\n",
    "        # Remove the \"geometry\" column from the points GeoDataFrame\n",
    "        points_gdf = points_gdf.drop(columns=['geometry'])\n",
    "\n",
    "        # Remove the linestring rows from the original GeoDataFrame\n",
    "        barriers = barriers[barriers['geometry'].geom_type != 'LineString']\n",
    "\n",
    "        # Rename the \"merged_geometry\" column to \"geometry\" in the points GeoDataFrame\n",
    "        points_gdf = points_gdf.rename(columns={'merged_geometry': 'geometry'})\n",
    "\n",
    "        # Concatenate the individual points GeoDataFrame to the original GeoDataFrame\n",
    "        barriers = pd.concat([barriers, points_gdf], ignore_index=True)\n",
    "\n",
    "        # Reset the index to ensure it is continuous\n",
    "        barriers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Create a new column \"previously_linestring\" and set it to False initially\n",
    "        barriers['previously_linestring'] = False\n",
    "\n",
    "        # Iterate through each row in the GeoDataFrame with linestrings\n",
    "        for idx, row in barriers_linestrings.iterrows():\n",
    "            if isinstance(row['geometry'], LineString):\n",
    "                # Extract the individual points from the linestring\n",
    "                points = [Point(coord) for coord in list(row['geometry'].coords)]\n",
    "\n",
    "                # Iterate through the points in the linestring\n",
    "                for point in points:\n",
    "                    # Check if the point's geometry intersects with any of the original linestrings\n",
    "                    mask = barriers['geometry'].intersects(point)\n",
    "                    if mask.any():\n",
    "                        # If the point intersects with any linestring, set \"previously_linestring\" to True\n",
    "                        barriers.loc[mask, 'previously_linestring'] = True\n",
    "\n",
    "        # add a unique ID\n",
    "        barriers['barrier_id'] = range(1, len(barriers) + 1)\n",
    "\n",
    "        # Convert the OSMnx graph to a GeoDataFrame of streets\n",
    "        streets_gdf = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "\n",
    "        # join the barriers to the streets\n",
    "        streets_gdf = gpd.sjoin(streets_gdf, barriers, how=\"left\", op=\"intersects\")\n",
    "\n",
    "        # clean geodataframe and drop streets without a barrier\n",
    "        streets_gdf.columns = streets_gdf.columns.str.replace(\"_right\", \"_barrier\").str.replace(\"_left\", \"_street\")\n",
    "        # we need to double check the name of \"barrier\"\n",
    "        streets_gdf['barrier_barrier'] = streets_gdf['barrier'] if 'barrier' in streets_gdf.columns else streets_gdf[\n",
    "            'barrier_barrier']\n",
    "\n",
    "        if 'name_street' in streets_gdf.columns:\n",
    "            streets_gdf = streets_gdf.rename(columns={'name_street': 'name'})\n",
    "        barrier_streets = streets_gdf.dropna(subset=['barrier_barrier'])\n",
    "\n",
    "        # add barrier tag\n",
    "        barrier_streets['filter_type'] = 'barrier or bollard'\n",
    "\n",
    "        ## extract points which are on/within 1m of streets only\n",
    "        streets_gdf['has_barrier'] = 'yes'\n",
    "\n",
    "        # reset crs before spatial join\n",
    "        barriers, streets_gdf = barriers.to_crs(3857), streets_gdf.to_crs(3857)\n",
    "\n",
    "        barriers = gpd.sjoin_nearest(barriers, streets_gdf, how=\"left\", max_distance=1)\n",
    "        barriers = barriers.dropna(subset=['has_barrier'])\n",
    "        barriers = barriers.reset_index(drop=True)  # Reset the index\n",
    "\n",
    "        # Dissolve based on the 'geometry' column\n",
    "        barriers = barriers.dissolve(by='barrier_id_right')\n",
    "\n",
    "        # add barrier tag\n",
    "        barriers['filter_type'] = 'barrier or bollard'\n",
    "\n",
    "        # Reset the index to remove multi-index\n",
    "        barriers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return barriers, streets_gdf\n",
    "\n",
    "\n",
    "        \n",
    "    def get_bus_gates(streets_gdf):\n",
    "        \"\"\"\n",
    "        Finds all the bus gates within the given streets GeoDataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        streets_gdf (GeoDataFrame): A GeoDataFrame containing street data.\n",
    "\n",
    "        Returns:\n",
    "        busgates (GeoDataFrame): A GeoDataFrame containing the bus gates found in the streets data.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if required columns are present, if not add them with default values\n",
    "        for col in ['access', 'bicycle', 'bus', 'motor_vehicle']:\n",
    "            if col not in streets_gdf.columns:\n",
    "                streets_gdf[col] = None\n",
    "\n",
    "        streets_gdf['access_street'] = streets_gdf['access'] if 'access' in streets_gdf.columns else streets_gdf['access_street']\n",
    "        streets_gdf['bicycle_street'] = streets_gdf['bicycle'] if 'bicycle' in streets_gdf.columns else streets_gdf['bicycle_street']\n",
    "        streets_gdf['bus'] = streets_gdf['bus_street'] if 'bus_street' in streets_gdf.columns else streets_gdf['bus']\n",
    "\n",
    "        if 'bus' in streets_gdf.columns:\n",
    "            busgates = streets_gdf[((streets_gdf[\"bus\"] == \"yes\") & (streets_gdf[\"access_street\"] == \"no\") & (streets_gdf[\"bicycle_street\"] == \"yes\")) |\n",
    "                                (streets_gdf[\"bus\"] == \"yes\") & (streets_gdf[\"motor_vehicle_street\"] == \"no\") & (streets_gdf[\"bicycle_street\"] == \"yes\")]\n",
    "            # add bus gate tag\n",
    "            busgates['filter_type'] = 'bus gate'\n",
    "        else:\n",
    "            print(\"Warning: 'bus' column not found in streets_gdf.\")\n",
    "            busgates = gpd.GeoDataFrame(columns=streets_gdf.columns, crs=streets_gdf.crs)\n",
    "\n",
    "        return busgates, streets_gdf\n",
    "\n",
    "        # add bus gate tag\n",
    "        busgates['filter_type'] = 'bus gate'\n",
    "\n",
    "        return busgates, streets_gdf\n",
    "\n",
    "    def get_contraflows(streets_gdf):\n",
    "        \"\"\"\n",
    "        Finds the unrestricted one-way streets for cycling but restricted for cars.\n",
    "\n",
    "        Parameters:\n",
    "        streets_gdf (GeoDataFrame): A GeoDataFrame containing street data.\n",
    "\n",
    "        Returns:\n",
    "        GeoDataFrame: A GeoDataFrame containing the unrestricted one-way streets for cycling.\n",
    "        \"\"\"\n",
    "        ## one-way streets also can act as modal filters. lets find where cycling is unrestricted but cars are\n",
    "        if 'oneway:bicycle' in streets_gdf.columns:\n",
    "            oneways = streets_gdf[(streets_gdf[\"oneway\"] == True) & (streets_gdf[\"oneway:bicycle\"] == \"no\")]\n",
    "\n",
    "            # Convert values in the \"name\" column to strings if they are not already\n",
    "            oneways['name'] = oneways['name'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "            # Perform dissolve \n",
    "            oneways = oneways.dissolve(by='name')\n",
    "\n",
    "            # Reset the index \n",
    "            oneways = oneways.reset_index()\n",
    "\n",
    "            # add one way tag\n",
    "            oneways['filter_type'] = 'one-way bike'\n",
    "        else:\n",
    "            print(\"Warning: 'oneway:bicycle' column not found in streets_gdf.\")\n",
    "            oneways = gpd.GeoDataFrame(columns=streets_gdf.columns, crs=streets_gdf.crs)\n",
    "\n",
    "        return oneways\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def filter_streets_continuations(input_gdf):\n",
    "        ## clean dataframe\n",
    "        # Check if 'highway_street' column exists and rename it to 'highway'\n",
    "        if 'highway_street' in input_gdf.columns:\n",
    "            input_gdf.rename(columns={'highway_street': 'highway'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # filter dataframe \n",
    "        ## remove indoor roads, these are likey pedestrian only however often don't have any \"cycling\" related tag\n",
    "        if 'covered' in input_gdf.columns:\n",
    "            input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'covered' in str(x))]\n",
    "            input_gdf = input_gdf[input_gdf['covered'] != 'yes']\n",
    "        ## also remove footways and steps, as these are almost pedestrain only, never cyclable\n",
    "        input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'footway' in str(x))]\n",
    "        input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'steps' in str(x))]\n",
    "\n",
    "\n",
    "\n",
    "        ## clean dataframe\n",
    "        input_gdf['name'] = input_gdf['name'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "        input_gdf['highway'] = input_gdf['highway'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## perform street continunation filtering\n",
    "        # Grouping by 'name' and checking for groups with 'pedestrian' and another highway type\n",
    "        grouped = input_gdf.groupby('name').filter(lambda x: any('pedestrian' in val for val in x['highway']) and len(x['highway'].unique()) > 1)\n",
    "        street_continuations_gdf = grouped[grouped['highway'].str.contains('pedestrian', case=False, na=False)] # Extracting the rows containing 'pedestrian' in the highway column\n",
    "\n",
    "        ## deal with nan names\n",
    "\n",
    "\n",
    "        ## dissolve lines that are very very close to each other\n",
    "        if not street_continuations_gdf.empty:\n",
    "            street_continuations_gdf = street_continuations_gdf.to_crs('27700')\n",
    "            street_continuations_gdf['buffer'] = street_continuations_gdf.geometry.buffer(1)\n",
    "            dissolved = street_continuations_gdf.dissolve(by='name')\n",
    "            \n",
    "            # If a MultiPolygon is formed, convert it to individual polygons\n",
    "            if isinstance(dissolved.geometry.iloc[0], MultiPolygon):\n",
    "                dissolved = dissolved.explode()\n",
    "            \n",
    "            # Remove the buffer column\n",
    "            dissolved = dissolved.drop(columns='buffer')\n",
    "            street_continuations_gdf = dissolved.to_crs('4326')\n",
    "\n",
    "        return street_continuations_gdf\n",
    "\n",
    "\n",
    "\n",
    "    barriers, streets_gdf = get_barriers(boundary)\n",
    "    busgates, streets_gdf = get_bus_gates(streets_gdf)\n",
    "    oneways = get_contraflows(streets_gdf)\n",
    "    streets_continuations_gdf = filter_streets_continuations(streets_gdf)\n",
    "\n",
    "    # add street conitinuation tag\n",
    "    streets_continuations_gdf['filter_type'] = 'street continuation'\n",
    "\n",
    "\n",
    "    ## ensure correct crs\n",
    "    barriers, busgates, oneways, streets_continuations_gdf = barriers.to_crs('4326'), busgates.to_crs('4326'), oneways.to_crs('4326'), streets_continuations_gdf.to_crs('4326')\n",
    "\n",
    "    filters = gpd.GeoDataFrame(pd.concat([barriers, busgates, oneways, streets_continuations_gdf], ignore_index=True))\n",
    "\n",
    "\n",
    "\n",
    "    ## alter neighbourhoods before joining\n",
    "    # Reset neighbourhood CRS\n",
    "    filters_results_gdf  = neighbourhoods.to_crs('EPSG:27700')\n",
    "\n",
    "    # Buffer to ensure all filters are captured\n",
    "    filters_results_gdf['geometry'] = filters_results_gdf['geometry'].buffer(5)\n",
    "\n",
    "    # Reset neighbourhood CRS\n",
    "    filters_results_gdf  = filters_results_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "    ## Spatial join\n",
    "    # Perform a spatial join between neighbourhoods and filters\n",
    "    joined_data = gpd.sjoin(filters_results_gdf, filters, how=\"left\", predicate=\"intersects\", lsuffix='_neigh', rsuffix='_filt')\n",
    "\n",
    "    # Count the number of each filter within each neighbourhood\n",
    "    filter_type_counts = joined_data.groupby(['ID', 'filter_type']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Reset the index to make it more readable\n",
    "    filter_type_counts = filter_type_counts.reset_index()\n",
    "\n",
    "    # Merge the filter_type_counts DataFrame with the neighbourhoods GeoDataFrame on the ID column\n",
    "    filters_results_gdf = filters_results_gdf.merge(filter_type_counts, on='ID', how='left')\n",
    "\n",
    "    # Define the columns to sum\n",
    "    columns_to_sum = ['barrier or bollard', 'one-way bike', 'bus gate', 'street continuation']\n",
    "\n",
    "    # Filter out columns that exist in the DataFrame\n",
    "    existing_columns = [col for col in columns_to_sum if col in filters_results_gdf.columns]\n",
    "\n",
    "    # Sum the values in the existing columns per row\n",
    "    filters_results_gdf['total_filter_types'] = filters_results_gdf[existing_columns].sum(axis=1)\n",
    "\n",
    "    # Fill NaN values with 0 if necessary\n",
    "    filters_results_gdf = filters_results_gdf.fillna(0)\n",
    "\n",
    "    # Find locations where filters are found dense\n",
    "    # Convert road density to numeric if not already\n",
    "    filters_results_gdf['road_density'] = pd.to_numeric(filters_results_gdf['road_density'], errors='coerce')\n",
    "\n",
    "    # Create new column to hold filters * density value\n",
    "    filters_results_gdf['filter_road_density'] = filters_results_gdf['total_filter_types'] * filters_results_gdf['road_density']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### rat runs\n",
    "\n",
    "\n",
    "    drive_g = ox.graph_from_polygon(bouding, network_type='drive', simplify=True)\n",
    "\n",
    "    ## Clean graph and calculate travel times along edges\n",
    "\n",
    "    # Function to clean 'maxspeed' values\n",
    "    def clean_maxspeed(maxspeed):\n",
    "        if maxspeed is None:\n",
    "            return 30  # Replace None with a default value of 30\n",
    "        elif isinstance(maxspeed, str) and ' mph' in maxspeed:\n",
    "            return float(maxspeed.replace(' mph', ''))\n",
    "        elif isinstance(maxspeed, list):  # Handle cases where 'maxspeed' is a list\n",
    "            return [float(speed.replace(' mph', '')) for speed in maxspeed]\n",
    "        else:\n",
    "            return maxspeed\n",
    "\n",
    "    # Apply the function to 'maxspeed' in each edge attribute\n",
    "    for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "        if 'maxspeed' in data:\n",
    "            data['maxspeed'] = clean_maxspeed(data['maxspeed'])\n",
    "        else:\n",
    "            data['maxspeed'] = 30  # Assign default value of 30 if 'maxspeed' is missing\n",
    "\n",
    "    # Function to convert 'maxspeed' to a numeric value\n",
    "    def convert_maxspeed(maxspeed):\n",
    "        if isinstance(maxspeed, list) and maxspeed:  # Check if 'maxspeed' is a non-empty list\n",
    "            # If 'maxspeed' is a list, convert the first value to a numeric value\n",
    "            return convert_single_maxspeed(maxspeed[0])\n",
    "        else:\n",
    "            # If 'maxspeed' is not a list or an empty list, convert the single value to a numeric value\n",
    "            return convert_single_maxspeed(maxspeed)\n",
    "\n",
    "    # Helper function to convert a single maxspeed value to a numeric value\n",
    "    def convert_single_maxspeed(maxspeed):\n",
    "        if maxspeed is None:\n",
    "            return 30  # Replace None with a default value of 30\n",
    "\n",
    "        if isinstance(maxspeed, str):\n",
    "            # Extract numeric part of the string using regular expression\n",
    "            numeric_part = ''.join(c for c in maxspeed if c.isdigit() or c == '.')\n",
    "            return float(numeric_part) if numeric_part else 30  # Default value if no numeric part found\n",
    "        elif isinstance(maxspeed, (int, float)):\n",
    "            return maxspeed\n",
    "        else:\n",
    "            return 30  # Default value if the type is unknown\n",
    "\n",
    "    # Function to calculate travel time\n",
    "    def calculate_travel_time(length, maxspeed):\n",
    "        # Convert 'maxspeed' to a numeric value\n",
    "        maxspeed_value = convert_maxspeed(maxspeed)\n",
    "\n",
    "        # Convert maxspeed to meters per second\n",
    "        speed_mps = maxspeed_value * 0.44704  # 1 mph = 0.44704 m/s\n",
    "\n",
    "        # Calculate travel time in seconds using the formula: time = distance/speed\n",
    "        if length is not None and speed_mps > 0:\n",
    "            travel_time = length / speed_mps\n",
    "            return travel_time\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply the function to 'length' and 'maxspeed' in each edge attribute\n",
    "    for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "        if 'length' in data:\n",
    "            data['travel_time'] = calculate_travel_time(data.get('length'), data.get('maxspeed'))\n",
    "\n",
    "\n",
    "\n",
    "    def get_sparse_graph(drive_g):\n",
    "        \"\"\"\n",
    "        Create a sparse graph from bounding roads.\n",
    "\n",
    "        Args:\n",
    "            drive_g (networkx.Graph): The original graph.\n",
    "\n",
    "        Returns:\n",
    "            networkx.Graph: The sparse graph.\n",
    "        \"\"\"\n",
    "        # Create a copy of the original graph\n",
    "        sparse_drive_g = drive_g.copy()\n",
    "\n",
    "        # Define the conditions for keeping edges\n",
    "        conditions = [\n",
    "            (\n",
    "                data.get('highway') in ['trunk', 'trunk_link', 'motorway', 'motorway_link', 'primary', 'primary_link',\n",
    "                                        'secondary', 'secondary_link', 'tertiary', 'tertiary_link']\n",
    "            ) or (\n",
    "                data.get('maxspeed') in ['60', '70', '40', ('20', '50'), ('30', '60'), ('30', '50'), ('70', '50'),\n",
    "                                        ('40', '60'), ('70', '60'), ('60', '40'), ('50', '40'), ('30', '40'),\n",
    "                                        ('20', '60'), ('70 ', '40 '), ('30 ', '70')]\n",
    "            )\n",
    "            for u, v, k, data in sparse_drive_g.edges(keys=True, data=True)\n",
    "        ]\n",
    "\n",
    "        # Keep only the edges that satisfy the conditions\n",
    "        edges_to_remove = [\n",
    "            (u, v, k) for (u, v, k), condition in zip(sparse_drive_g.edges(keys=True), conditions) if not condition\n",
    "        ]\n",
    "        sparse_drive_g.remove_edges_from(edges_to_remove)\n",
    "\n",
    "        # Clean nodes by removing isolated nodes from the graph\n",
    "        isolated_nodes = list(nx.isolates(sparse_drive_g))\n",
    "        sparse_drive_g.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "        return sparse_drive_g\n",
    "\n",
    "\n",
    "\n",
    "    sparse_drive_g = get_sparse_graph(drive_g)\n",
    "\n",
    "\n",
    "    #print(\"Number of edges in the sparse graph:\", sparse_drive_g.number_of_edges())\n",
    "\n",
    "\n",
    "\n",
    "    ## create a partitioned network (using the full graph and the sparse graph)\n",
    "\n",
    "    # Make a copy of the original graph\n",
    "    drive_g_copy = drive_g.copy()\n",
    "\n",
    "    ## Remove edges \n",
    "    drive_g_copy.remove_edges_from(sparse_drive_g.edges)\n",
    "\n",
    "    ## Remove nodes\n",
    "    # Convert nodes to strings\n",
    "    sparse_drive_nodes_str = [str(node) for node in sparse_drive_g.nodes]\n",
    "    drive_g_copy.remove_nodes_from(sparse_drive_nodes_str)\n",
    "\n",
    "    # clean nodes by removing isolated nodes from the graph\n",
    "    isolated_nodes = list(nx.isolates(drive_g_copy))\n",
    "    drive_g_copy.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "    len(drive_g_copy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## partition the full graph, by removing the sparse graph from it.\n",
    "\n",
    "    # first nodes shared between sparse_drive_g and drive_g (these nodes are the connection between neighbourhoods and boundary roads)\n",
    "    shared_nodes = set(sparse_drive_g.nodes).intersection(drive_g_copy.nodes)\n",
    "\n",
    "\n",
    "    # we then need to remove nodes where junctions between two neighbourhood nodes and sparse graphs are present. \n",
    "    # we do this by adding new nodes the end of edges which intersect with the sparse graph, to split these junctions up\n",
    "    # Initialize a counter to generate unique indices for new nodes\n",
    "    node_counter = Counter()\n",
    "    # Iterate through shared nodes\n",
    "    for shared_node in shared_nodes:\n",
    "        # Find edges in drive_g connected to the shared node\n",
    "        drive_g_edges = list(drive_g_copy.edges(shared_node, data=True, keys=True))\n",
    "\n",
    "        # Find edges in sparse_drive_g connected to the shared node\n",
    "        sparse_drive_g_edges = list(sparse_drive_g.edges(shared_node, data=True, keys=True))\n",
    "\n",
    "        # Iterate through edges in drive_g connected to the shared node\n",
    "        for u, v, key, data in drive_g_edges:\n",
    "            # Check if the corresponding edge is not in sparse_drive_g\n",
    "            if (u, v, key) not in sparse_drive_g_edges:\n",
    "                # Create new end nodes for the edge in drive_g\n",
    "                new_u = f\"new_{u}\" if u == shared_node else u\n",
    "                new_v = f\"new_{v}\" if v == shared_node else v\n",
    "\n",
    "                # Generate a unique index for each new node ID\n",
    "                new_u_id = f\"{new_u}_{key}_{node_counter[new_u]}\" if new_u != u else new_u\n",
    "                new_v_id = f\"{new_v}_{key}_{node_counter[new_v]}\" if new_v != v else new_v\n",
    "\n",
    "                # Increment the counter for each new node\n",
    "                node_counter[new_u] += 1\n",
    "                node_counter[new_v] += 1\n",
    "\n",
    "                # Add new nodes and update the edge\n",
    "                drive_g_copy.add_node(new_u_id, **drive_g_copy.nodes[u])\n",
    "                drive_g_copy.add_node(new_v_id, **drive_g_copy.nodes[v])\n",
    "\n",
    "                drive_g_copy.add_edge(new_u_id, new_v_id, key=key, **data)\n",
    "\n",
    "                # Check if the reverse edge already exists in drive_g_copy\n",
    "                if not drive_g_copy.has_edge(new_v_id, new_u_id, key):\n",
    "                    # Create the reverse edge with new nodes\n",
    "                    drive_g_copy.add_edge(new_v_id, new_u_id, key=key, **data)\n",
    "\n",
    "                # Disconnect the shared node from the new edge\n",
    "                drive_g_copy.remove_edge(u, v, key)\n",
    "\n",
    "        # Remove the shared node\n",
    "        drive_g_copy.remove_node(shared_node)\n",
    "\n",
    "\n",
    "\n",
    "    # Find strongly connected components in the modified drive_g graph\n",
    "    drive_g_scc = list(nx.strongly_connected_components(drive_g_copy))\n",
    "\n",
    "    # Create a color mapping for edges in each strongly connected component using random colors\n",
    "    edge_colors = {}\n",
    "    for i, component in enumerate(drive_g_scc):\n",
    "        color = (random.random(), random.random(), random.random())  # RGB tuple with random values\n",
    "        for edge in drive_g_copy.edges:\n",
    "            if edge[0] in component and edge[1] in component:\n",
    "                edge_colors[edge] = color\n",
    "\n",
    "    # Plot the graph with edge colors and without nodes\n",
    "    #fig, ax = ox.plot_graph(drive_g_copy, edge_color=[edge_colors.get(edge, (0, 0, 0)) for edge in drive_g_copy.edges], node_size=0, show=False, close=False, figsize=(20, 20))\n",
    "    #ox.plot_graph(sparse_drive_g, ax=ax, edge_color='red', edge_linewidth=2, node_size=0, show=True)\n",
    "    #fig.show()\n",
    "\n",
    "\n",
    "    ## add ssc index to each neighbourhood\n",
    "\n",
    "    # Create a mapping from nodes to their SCC index\n",
    "    node_scc_mapping = {node: i for i, scc in enumerate(drive_g_scc) for node in scc}\n",
    "\n",
    "    # Add SCC attribute to edges\n",
    "    for u, v, key, data in drive_g_copy.edges(keys=True, data=True):\n",
    "        scc_index_u = node_scc_mapping.get(u, None)\n",
    "        scc_index_v = node_scc_mapping.get(v, None)\n",
    "        \n",
    "        # Add the SCC index as an attribute to the edge\n",
    "        drive_g_copy[u][v][key]['scc_index'] = scc_index_u if scc_index_u is not None else scc_index_v\n",
    "\n",
    "\n",
    "    ## join neighbourhood mapping to orignial driving graph\n",
    "\n",
    "    # Add SCC index attribute to drive_g\n",
    "    for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "        scc_index_u = node_scc_mapping.get(u, None)\n",
    "        scc_index_v = node_scc_mapping.get(v, None)\n",
    "        \n",
    "        # Add the SCC index as an attribute to the edge\n",
    "        drive_g[u][v][key]['scc_index'] = scc_index_u if scc_index_u is not None else scc_index_v\n",
    "\n",
    "\n",
    "\n",
    "    ## get random nodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Function to get random nodes present in both graphs for each node\n",
    "    def get_random_nodes_for_each(graph1, graph2):\n",
    "        random_nodes_for_each = {}\n",
    "        common_nodes = set(graph1.nodes()) & set(graph2.nodes())\n",
    "        total_common_nodes = len(common_nodes)\n",
    "        num_nodes = min(iterations, max(1, int(total_common_nodes * 0.9)))  # 10% less than the total number of common nodes, capped at the input max iterations\n",
    "\n",
    "        for node in common_nodes:\n",
    "            neighbors = list(set(graph1.neighbors(node)) & set(graph2.neighbors(node)))\n",
    "            if len(neighbors) >= num_nodes:\n",
    "                random_neighbors = random.sample(neighbors, num_nodes)\n",
    "            else:\n",
    "                random_neighbors = neighbors + random.sample(list(common_nodes - set(neighbors)), num_nodes - len(neighbors))\n",
    "            random_nodes_for_each[node] = random_neighbors\n",
    "        return random_nodes_for_each\n",
    "\n",
    "\n",
    "\n",
    "    # Get random nodes for each common node\n",
    "    random_nodes_for_each = get_random_nodes_for_each(drive_g, sparse_drive_g)\n",
    "\n",
    "\n",
    "\n",
    "    # Print random nodes for each common node\n",
    "    #for node, random_neighbors in random_nodes_for_each.items():\n",
    "        #print(f\"Random nodes for node {node}: {random_neighbors}\")\n",
    "\n",
    "\n",
    "    ## Find shortest paths \n",
    "\n",
    "    # Convert the dictionary of nodes into a list of tuples\n",
    "    nodes_list = [(key, value) for key, values in random_nodes_for_each.items() for value in values]\n",
    "\n",
    "    # Find shortest paths and store them in a dictionary\n",
    "    shortest_paths = {}\n",
    "    for start_node, end_node in nodes_list:\n",
    "        try:\n",
    "            shortest_path = nx.shortest_path(drive_g, start_node, end_node, weight='travel_time')\n",
    "            shortest_paths[(start_node, end_node)] = shortest_path\n",
    "        except nx.NetworkXNoPath:\n",
    "            print(f\"No path found between {start_node} and {end_node}. Skipping...\")\n",
    "\n",
    "    # Print the shortest paths\n",
    "    #for key, value in shortest_paths.items():\n",
    "    #print(f\"Shortest path from {key[0]} to {key[1]}: {value}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## find edges passed through\n",
    "\n",
    "    edges_passed_through = set()\n",
    "\n",
    "    for path in shortest_paths.values():\n",
    "        # Pair consecutive nodes to create edges\n",
    "        path_edges = [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "        \n",
    "        # Check if each edge exists in the graph\n",
    "        for edge in path_edges:\n",
    "            if edge in drive_g.edges:\n",
    "                edges_passed_through.add(edge)\n",
    "\n",
    "    # Convert the set of edges to a list if needed\n",
    "    edges_passed_through = list(edges_passed_through)\n",
    "\n",
    "    for u, v, data in drive_g.edges(data=True):\n",
    "        if (u, v) in edges_passed_through or (v, u) in edges_passed_through:\n",
    "            data['rat_run'] = True\n",
    "        else:\n",
    "            data['rat_run'] = False\n",
    "\n",
    "\n",
    "    # Convert the NetworkX graph to a GeoDataFrame\n",
    "    drive_gdf_nodes, drive_gdf_edges = ox.graph_to_gdfs(drive_g)\n",
    "\n",
    "    drive_gdf_edges = drive_gdf_edges.to_crs(27700)\n",
    "    drive_gdf_nodes = drive_gdf_nodes.to_crs(27700)\n",
    "\n",
    "\n",
    "    # Filter drive_gdf_edges to only include edges with 'rat_run' = True\n",
    "    rat_run_edges = drive_gdf_edges[drive_gdf_edges['rat_run'] == True]\n",
    "\n",
    "    # reset crs\n",
    "    neighbourhoods = neighbourhoods.to_crs(27700)\n",
    "\n",
    "    # Perform spatial join between neighbourhoods and rat_run_edges\n",
    "    join_result = gpd.sjoin(neighbourhoods, rat_run_edges, how='left', op='intersects')\n",
    "\n",
    "    # Group by neighbourhood index and count the number of rat_run edges in each\n",
    "    rat_run_edge_count = join_result.groupby(join_result.index)['ID'].count().reset_index(name='rat_run_edge_count')\n",
    "\n",
    "    # Group by neighbourhood index and count the number of rat_run edges in each\n",
    "    rat_run_edge_count = join_result.groupby(join_result.index)['ID'].count().reset_index(name='rat_run_edge_count')\n",
    "\n",
    "    # reset crs\n",
    "    neighbourhoods = neighbourhoods.to_crs(27700)\n",
    "\n",
    "    # Join rat_run_edge_count with neighbourhoods based on index\n",
    "    neighbourhoods = neighbourhoods.join(rat_run_edge_count.set_index('index'))\n",
    "\n",
    "    ## now we should have filters_results_gdf and access_results_gdf, and neighbourhoods with rat run counts joined\n",
    "\n",
    "    filters_results_gdf, access_results_gdf, neighbourhoods\n",
    "\n",
    "    ## join all together\n",
    "\n",
    "    results_gdf = gpd.GeoDataFrame(filters_results_gdf.merge(access_results_gdf, on=\"ID\", suffixes=('_filters', \"_access\")))\n",
    "    results_gdf = results_gdf.set_geometry('geometry_access')\n",
    "    final_results_gdf = results_gdf.merge(neighbourhoods[['ID', 'rat_run_edge_count']], on='ID', how='left')\n",
    "    final_results_gdf['geometry'] = final_results_gdf['geometry_filters']\n",
    "    final_results_gdf = final_results_gdf.set_geometry('geometry')\n",
    "    final_results_gdf.drop(columns=['geometry_filters', 'geometry_access'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Define the scoring function for \"rat_run_edge_count\"\n",
    "    def score_rat_run_edge_count(value):\n",
    "        if value <= 1:\n",
    "            return 100\n",
    "        else:\n",
    "            return 100 / (2 ** value) # Exponetial scoring\n",
    "\n",
    "    # Apply the scoring function to the \"rat_run_edge_count\" column\n",
    "    final_results_gdf[\"rat_run_score\"] = final_results_gdf[\"rat_run_edge_count\"].apply(score_rat_run_edge_count)\n",
    "\n",
    "    import math\n",
    "\n",
    "    def score_mean_distance_diff(value):\n",
    "        if value >= 0:\n",
    "            return 0\n",
    "        elif value <= -750: # set a 750m cut off\n",
    "            return 100\n",
    "        else:\n",
    "            normalized_value = abs(value) / 750  # Normalize the value between 0 and 1\n",
    "            score = 100 * (1 - math.exp(-5 * normalized_value))  # Exponential increase\n",
    "            return score\n",
    "\n",
    "    # Apply the modified scoring function to the \"mean_distance_diff\" column\n",
    "    final_results_gdf[\"mean_distance_diff_score\"] = final_results_gdf[\"mean_distance_diff\"].apply(score_mean_distance_diff)\n",
    "\n",
    "    def score_road_density_filters(value):\n",
    "        if value <= 0:\n",
    "            return 0\n",
    "        elif value >= 40:\n",
    "            return 100\n",
    "        else:\n",
    "            return (value / 40) * 100\n",
    "\n",
    "    # Apply the scoring function to the \"road_density_filters\" column\n",
    "    final_results_gdf[\"filter_road_density_score\"] = final_results_gdf[\"filter_road_density\"].apply(score_road_density_filters)\n",
    "\n",
    "    # Create the \"scored_neighbourhoods\" geodataframe with the necessary columns\n",
    "    scored_neighbourhoods = final_results_gdf[[\"geometry\", \"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]]\n",
    "\n",
    "    # Calculate overall score\n",
    "    scored_neighbourhoods[\"overall_score\"] = (scored_neighbourhoods[\"rat_run_score\"] + scored_neighbourhoods[\"mean_distance_diff_score\"] + scored_neighbourhoods[\"filter_road_density_score\"]) / 3\n",
    "\n",
    "    # Define weights for each score\n",
    "    weight_rat_run_score = 1\n",
    "    weight_mean_distance_diff_score = 1\n",
    "    weight_road_density_filters_score = 1\n",
    "\n",
    "    weight_rat_run_score = through_route_weighting\n",
    "    weight_mean_distance_diff_score = permiablity_weighting\n",
    "    weight_road_density_filters_score = modal_filter_weighting\n",
    "\n",
    "    # Calculate overall score with weights\n",
    "    scored_neighbourhoods[\"overall_score\"] = (\n",
    "        (weight_rat_run_score * scored_neighbourhoods[\"rat_run_score\"]) +\n",
    "        (weight_mean_distance_diff_score * scored_neighbourhoods[\"mean_distance_diff_score\"]) +\n",
    "        (weight_road_density_filters_score * scored_neighbourhoods[\"filter_road_density_score\"])\n",
    "    ) / (weight_rat_run_score + weight_mean_distance_diff_score + weight_road_density_filters_score)\n",
    "\n",
    "\n",
    "\n",
    "    ## find elbow point for k-means clustering\n",
    "\n",
    "    # Selecting the features for clustering\n",
    "    X = scored_neighbourhoods[[\"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]]\n",
    "\n",
    "    # Initialize a list to store the within-cluster sum of squares (WCSS) for different values of K\n",
    "    wcss = []\n",
    "\n",
    "    # Define the range of K values to try\n",
    "    k_values = range(1, 11)\n",
    "\n",
    "    # Calculate WCSS for each value of K\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    # Plotting the elbow curve\n",
    "    plt.plot(k_values, wcss, marker='o')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.xticks(k_values)\n",
    "    plt.show()\n",
    "\n",
    "    ## Run k-means clustering\n",
    "    # Define the number of clusters\n",
    "    k = 2\n",
    "\n",
    "    # Select the features for clustering\n",
    "    features = [\"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]\n",
    "\n",
    "    # Extract the features from the dataframe\n",
    "    X = scored_neighbourhoods[features]\n",
    "\n",
    "    # Initialize the KMeans model\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Get the cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Add the cluster labels to the dataframe\n",
    "    scored_neighbourhoods[\"cluster_label\"] = cluster_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## maps\n",
    "    ## adjust geodataframe contents for plotting purposes\n",
    "\n",
    "    replacement_map = {\n",
    "        'barrier or bollard': 'Barrier or Bollard',\n",
    "        'bus gate': 'Bus Gate',\n",
    "        'one-way bike': 'Cycle Contraflow',\n",
    "        'street continuation': 'Street Continuation'\n",
    "    }\n",
    "\n",
    "    # Replace filter types in the DataFrame\n",
    "    filters['filter_type'] = filters['filter_type'].map(replacement_map).fillna(filters['filter_type'])\n",
    "\n",
    "\n",
    "    import folium\n",
    "    import branca.colormap as cm\n",
    "    from folium.plugins import MarkerCluster\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the centroid of the scored_neighbourhoods GeoDataFrame\n",
    "    centroid = scored_neighbourhoods.geometry.centroid.iloc[0]\n",
    "    center_latitude, center_longitude = centroid.y, centroid.x\n",
    "\n",
    "    # Create a Folium map centered around the centroid of scored_neighbourhoods\n",
    "    m = folium.Map(location=[center_latitude, center_longitude], zoom_start=12)\n",
    "\n",
    "    # Define the colormap using cm.linear.viridis\n",
    "    cmap = cm.linear.viridis.scale(scored_neighbourhoods['overall_score'].min(), scored_neighbourhoods['overall_score'].max())\n",
    "\n",
    "    # Plot scored_neighbourhoods using the Viridis colormap\n",
    "    folium.GeoJson(scored_neighbourhoods,\n",
    "                name= \"Scored Neighbourhoods\",\n",
    "                style_function=lambda x: {'fillColor': cmap(x['properties']['overall_score']),\n",
    "                                            'color': cmap(x['properties']['overall_score']),\n",
    "                                            'weight': 1, 'fillOpacity': 0.7},\n",
    "                tooltip=folium.features.GeoJsonTooltip(\n",
    "                    fields=['rat_run_score', 'mean_distance_diff_score', 'filter_road_density_score', 'overall_score', 'cluster_label'],\n",
    "                    aliases=['Rat Run Score', 'Mean Distance Diff Score', 'Filter Road Density Score', 'Overall Score', 'Cluster Label'])\n",
    "                ).add_to(m)\n",
    "\n",
    "    # Plot streets_gdf on the map with default blue color and slightly transparent\n",
    "    streets_layer = folium.GeoJson(drive_gdf_edges,\n",
    "                                name=\"Streets\",\n",
    "                                style_function=lambda x: {'color': 'lightgreen', 'weight': 1, 'fillOpacity': 0.7}\n",
    "                                ).add_to(m)\n",
    "\n",
    "    # Plot rat_run_edges on the map with red color\n",
    "    rat_runs_layer = folium.GeoJson(rat_run_edges,\n",
    "                                    name=\"Rat Runs\",\n",
    "                                    style_function=lambda x: {'color': 'red', 'weight': 1.5, 'fillOpacity': 0.7}\n",
    "                                ).add_to(m)\n",
    "\n",
    "    # Plot boundary_roads on the map with orange color and thicker weight\n",
    "    boundary_roads_layer = folium.GeoJson(boundary_roads,\n",
    "                                        name=\"Busy Roads\",\n",
    "                                        style_function=lambda x: {'color': 'orange', 'weight': 3, 'fillOpacity': 0.7}\n",
    "                                        ).add_to(m)\n",
    "\n",
    "\n",
    "\n",
    "    # Create a feature group for each type of layer\n",
    "    point_group = folium.FeatureGroup(name='Modal Filtering Points', show=True)\n",
    "    line_group = folium.FeatureGroup(name='Modal Filtering Streets', show=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot purple point markers for filters with tooltips\n",
    "    for _, row in filters.iterrows():\n",
    "        if row.geometry.type == 'Point':\n",
    "            tooltip_text = f\"Filter type: {row['filter_type']}\"  # Concatenating \"Filter type:\" with the 'filter_type' value\n",
    "            folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=2, color='purple', fill=True, fill_color='purple', tooltip=tooltip_text).add_to(point_group)\n",
    "        elif row.geometry.type == 'MultiLineString' or row.geometry.type == 'LineString':\n",
    "            tooltip_text = f\"Filter type: {row['filter_type']}\"  # Concatenating \"Filter type:\" with the 'filter_type' value\n",
    "            folium.GeoJson(row.geometry, style_function=lambda x: {'color': 'purple', 'weight': 1.5, 'fillOpacity': 0.7}, tooltip=tooltip_text).add_to(line_group)\n",
    "\n",
    "\n",
    "    # Add layer groups to the map\n",
    "    point_group.add_to(m)\n",
    "    line_group.add_to(m)\n",
    "\n",
    "    # Add layer control\n",
    "    folium.LayerControl(autoZIndex=True).add_to(m)\n",
    "\n",
    "    cmap.caption = 'LTN Plausiblity Scores (Possible range: 0-100)'\n",
    "    cmap.add_to(m)\n",
    "\n",
    "\n",
    "    # add text\n",
    "    from folium import IFrame\n",
    "\n",
    "    # Define the HTML content for the text\n",
    "    html_text = \"\"\"\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 50px; width: 300px; height: 200px; \n",
    "                background-color: rgba(255, 255, 255, 0.6);\n",
    "                border:2px solid grey; z-index:9999;\n",
    "                font-size:14px;\n",
    "                \">\n",
    "        <p style=\"padding: 10px;\">Scored neighbourhoods show a LTN 'Plausibility' score which incorporates metrics based on the presence of rat-runs, modal filters and measures of neighbourhood permeability. Map results are experimental, and should be treated as such. Get in touch via c.larkin@newcastle.ac.uk or <a href=\"https://github.com/Froguin99/LTN-Detection\" target=\"_blank\">https://github.com/Froguin99/LTN-Detection</a>.</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Add the HTML content to the map\n",
    "    folium.MacroElement().add_to(m)\n",
    "    m.get_root().html.add_child(folium.Element(html_text))\n",
    "\n",
    "\n",
    "    # save to geopackage\n",
    "\n",
    "    # Extract place name without \", United Kingdom\"\n",
    "    place_name = place.replace(\", United Kingdom\", \"\").strip()\n",
    "\n",
    "    # Create the file paths\n",
    "    map_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\Examples\\maps', f'{place_name}_example.html')\n",
    "    geopackage_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\scored_neighbourhoods', f'scored_neighbourhoods_{place_name}.gpkg')\n",
    "\n",
    "    # Export map\n",
    "    m.save(map_file_path)\n",
    "\n",
    "    # Send to geopackage \n",
    "    geometry_column = scored_neighbourhoods.geometry.name\n",
    "\n",
    "    # Iterate through the columns and convert them to strings\n",
    "    for column in scored_neighbourhoods.columns:\n",
    "        if column != geometry_column:\n",
    "            scored_neighbourhoods[column] = scored_neighbourhoods[column].astype(str)\n",
    "\n",
    "    scored_neighbourhoods.to_file(geopackage_file_path, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "    ## export rat runs \n",
    "    geopackage_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\rat_runs', f'rat_runs_{place_name}.gpkg')\n",
    "\n",
    "\n",
    "    # Send to geopackage \n",
    "    geometry_column = rat_run_edges.geometry.name\n",
    "\n",
    "    # Iterate through the columns and convert them to strings\n",
    "    for column in rat_run_edges.columns:\n",
    "        if column != geometry_column:\n",
    "            rat_run_edges[column] = rat_run_edges[column].astype(str)\n",
    "\n",
    "    rat_run_edges.to_file(geopackage_file_path, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "\n",
    "    ## export modal filters\n",
    "    geopackage_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\filters', f'filters_{place_name}.gpkg')\n",
    "\n",
    "\n",
    "    # Send to geopackage \n",
    "    geometry_column = filters.geometry.name\n",
    "\n",
    "    # Iterate through the columns and convert them to strings\n",
    "    for column in filters.columns:\n",
    "        if column != geometry_column:\n",
    "            filters[column] = filters[column].astype(str)\n",
    "\n",
    "    filters.to_file(geopackage_file_path, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "\n",
    "    # Display the map\n",
    "    #m\n",
    "\n",
    "    print(\"Finished\", place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = \"Borough of Wyre, United Kingdom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get boundary\n",
    "def set_location_boundary(place):\n",
    "    \"\"\"\n",
    "    Sets up the location boundary by geocoding the given place and buffering it.\n",
    "\n",
    "    Parameters:\n",
    "    place (str): The name or address of the place to geocode.\n",
    "\n",
    "    Returns:\n",
    "    geopandas.GeoDataFrame: The buffered boundary of the location.\n",
    "    \"\"\"\n",
    "    # Set location and get boundary\n",
    "    boundary = ox.geocode_to_gdf(place)\n",
    "    boundary = boundary.to_crs('EPSG:27700')\n",
    "\n",
    "    # Buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "    boundary_buffered = boundary.buffer(50)\n",
    "\n",
    "    return boundary_buffered, boundary\n",
    "\n",
    "boundary_buffered, boundary = set_location_boundary(place)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This code retrieves street nodes and edges for walking and driving from OpenStreetMap within our area boundary, and loads the OS Open Roads network dataset.\n",
    "\n",
    "Functions:\n",
    "- get_street_networks: Retrieves street networks for all, walking, and driving modes within the specified boundary.\n",
    "\"\"\"\n",
    "\n",
    "def get_OSM_street_networks(boundary_buffered):\n",
    "    \"\"\"\n",
    "    Retrieves street networks for all, walking, and driving modes within the specified boundary.\n",
    "\n",
    "    Parameters:\n",
    "    - boundary_buffered: A GeoDataFrame representing the boundary of the area of interest.\n",
    "\n",
    "    Returns:\n",
    "    - all_edges: A GeoDataFrame containing the edges (streets) of the entire street network.\n",
    "    - all_nodes: A GeoDataFrame containing the nodes (intersections) of the entire street network.\n",
    "    - walk_edges: A GeoDataFrame containing the edges (streets) of the walking street network.\n",
    "    - walk_nodes: A GeoDataFrame containing the nodes (intersections) of the walking street network.\n",
    "    - drive_edges: A GeoDataFrame containing the edges (streets) of the driving street network.\n",
    "    - drive_nodes: A GeoDataFrame containing the nodes (intersections) of the driving street network.\n",
    "    - common_nodes_gdf: A GeoDataFrame containing the common nodes between the driving and walking street networks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reset boundary_buffered crs for passing to OSM\n",
    "    boundary_buffered_4326 = boundary_buffered.to_crs('4326')\n",
    "\n",
    "    # Get street networks\n",
    "    all_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='all', simplify=False)\n",
    "    walk_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='walk', simplify=True)\n",
    "    drive_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='drive', simplify=False)\n",
    "\n",
    "    all_edges = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "    all_nodes = ox.graph_to_gdfs(all_streets, nodes=True, edges=False)\n",
    "\n",
    "    walk_edges = ox.graph_to_gdfs(walk_streets, nodes=False, edges=True)\n",
    "    walk_nodes = ox.graph_to_gdfs(walk_streets, nodes=True, edges=False)\n",
    "\n",
    "    drive_edges = ox.graph_to_gdfs(drive_streets, nodes=False, edges=True)\n",
    "    drive_nodes = ox.graph_to_gdfs(drive_streets, nodes=True, edges=False)\n",
    "\n",
    "    # Find the common nodes between networks\n",
    "    # This ensures that shortest paths between points should always be able to be calculated\n",
    "    common_nodes = drive_nodes.merge(walk_nodes, on='osmid', suffixes=('_drive', '_walk'))\n",
    "    common_nodes_gdf = gpd.GeoDataFrame(common_nodes, geometry='geometry_drive')\n",
    "\n",
    "    return all_edges, all_nodes, walk_edges, walk_nodes, drive_edges, drive_nodes, common_nodes_gdf, all_streets, walk_streets, drive_streets\n",
    "\n",
    "\n",
    "# get street networks\n",
    "all_edges, all_nodes, walk_edges, walk_nodes, drive_edges, drive_nodes, common_nodes_gdf, all_streets, walk_streets, drive_streets = get_OSM_street_networks(boundary_buffered)\n",
    "#os_open_roads = get_OS_roads()  this is now got at the start of the code to avoid re-reading\n",
    "\n",
    "\n",
    "def retrieve_osm_features(polygon, tags):\n",
    "    \"\"\"\n",
    "    Retrieves OpenStreetMap features based on the specified polygon and tags.\n",
    "\n",
    "    Args:\n",
    "        polygon (Polygon): The polygon to retrieve features within.\n",
    "        tags (dict): The tags to filter the features.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: The retrieved OpenStreetMap features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        features = ox.features_from_polygon(polygon=polygon, tags=tags)\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        if \"There are no data elements in the server response\" in error_message:\n",
    "            print(\"No data elements found for the specified location/tags.\")\n",
    "            features = gpd.GeoDataFrame()  # Create an empty GeoDataFrame\n",
    "        else:\n",
    "            # Handle other exceptions here if needed\n",
    "            print(\"An error occurred:\", error_message)\n",
    "            features = None\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_railways(place):\n",
    "    \"\"\"\n",
    "    This retrievies and processes OpenStreetMap (OSM) railways data for a specified place.\n",
    "\n",
    "    Args:\n",
    "        place (str): The name of the place to retrieve OSM features for.\n",
    "\n",
    "    Returns:\n",
    "        railways (geopandas.GeoDataFrame): A GeoDataFrame containing the railways within the specified place.\n",
    "    \"\"\"\n",
    "\n",
    "    # for unknown reasons, using rail = ox.graph_from_place(place, custom_filter='[\"railway\"]')\n",
    "    # doesn't ALWAYS retrive the full rail network, hence why multiple lines are used to achive the same result\n",
    "\n",
    "    # Define railway types to retrieve\n",
    "    railway_types = [\"\", \"rail\", \"light_rail\", \"narrow_gauge\", \"subway\", \"tram\"]\n",
    "\n",
    "    # Initialize an empty graph\n",
    "    combined_railways = nx.MultiDiGraph()\n",
    "\n",
    "    for railway_type in railway_types:\n",
    "        try:\n",
    "            # Fetch the railway network for the specified type\n",
    "            network = ox.graph_from_place(place, simplify=False, custom_filter=f'[\"railway\"~\"{railway_type}\"]')\n",
    "\n",
    "            # Ensure the fetched network is a MultiDiGraph\n",
    "            if not isinstance(network, nx.MultiDiGraph):\n",
    "                network = nx.MultiDiGraph(network)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No railway data found for '{railway_type}'.\")\n",
    "            network = nx.MultiDiGraph()\n",
    "\n",
    "        # Compose the networks\n",
    "        combined_railways = nx.compose(combined_railways, network)\n",
    "\n",
    "    # Convert to GeoDataFrame\n",
    "    railways = ox.graph_to_gdfs(combined_railways, nodes=False, edges=True)\n",
    "\n",
    "    # Drop any other railway types that aren't needed\n",
    "    railways = railways.loc[(~railways[\"railway\"].isin([\"tunnel\", \"abandoned\", \"razed\", \"disused\", \"funicular\", \"monorail\", \"miniature\"]))]\n",
    "\n",
    "    # Drop rows where any of the specified columns have values \"True\" or \"yes\"\n",
    "    columns_to_check = ['tunnel', 'abandoned', 'razed', 'disused', 'funicular', 'monorail', 'miniature']\n",
    "    railways = railways.loc[~railways[railways.columns.intersection(columns_to_check)].isin(['True', 'yes']).any(axis=1)]\n",
    "\n",
    "    # Set railways CRS\n",
    "    railways = railways.to_crs('EPSG:27700')\n",
    "\n",
    "    return railways\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## get urban footprints from GUF\n",
    "\n",
    "def get_guf(place):\n",
    "    \"\"\"\n",
    "    Retrieves a clipped GeoDataFrame of GUF urban areas within a specified place boundary.\n",
    "\n",
    "    Parameters:\n",
    "    - place (str): The name or address of the place to retrieve urban areas for.\n",
    "\n",
    "    Returns:\n",
    "    - gdf_clipped (GeoDataFrame): A GeoDataFrame containing the clipped urban areas within the specified place boundary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Access the WMS Service\n",
    "    wms_url = 'https://geoservice.dlr.de/eoc/land/wms?GUF04_DLR_v1_Mosaic'\n",
    "    wms = WebMapService(wms_url, version='1.1.1')\n",
    "\n",
    "    # Step 2: Identify the Layer with ID 102. This is the Global Urban Footprint layer GUF\n",
    "    for layer_name, layer in wms.contents.items():\n",
    "        if '102' in layer_name:\n",
    "            print(f\"Layer ID 102 found: {layer_name}\")\n",
    "\n",
    "    # Assuming 'GUF04_DLR_v1_Mosaic' is the layer with ID 102\n",
    "    layer = 'GUF04_DLR_v1_Mosaic'  # Replace with the actual layer name if different\n",
    "\n",
    "    # Step 3: Get the polygon boundary using osmnx\n",
    "    boundary_gdf = ox.geocode_to_gdf(place)\n",
    "    boundary = boundary_gdf.to_crs('EPSG:27700')\n",
    "    # buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "    boundary_buffered = boundary.buffer(100)\n",
    "    boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "    boundary_polygon = boundary_gdf.geometry[0]\n",
    "    wms_boundary = boundary_buffered.geometry[0]\n",
    "\n",
    "    # Convert the polygon to a bounding box\n",
    "    minx, miny, maxx, maxy = wms_boundary.bounds\n",
    "\n",
    "    # Step 4: Request the data from WMS using the bounding box\n",
    "    width = 1024\n",
    "    height = 1024\n",
    "    response = wms.getmap(\n",
    "        layers=[layer],\n",
    "        srs='EPSG:4326',\n",
    "        bbox=(minx, miny, maxx, maxy),\n",
    "        size=(width, height),\n",
    "        format='image/geotiff'\n",
    "    )\n",
    "\n",
    "    # Step 5: Load the Raster Data into Rasterio\n",
    "    with MemoryFile(response.read()) as memfile:\n",
    "        with memfile.open() as src:\n",
    "            image = src.read(1)  # Read the first band\n",
    "            transform = src.transform\n",
    "            crs = src.crs\n",
    "\n",
    "            # Clip the raster data to the polygon\n",
    "            out_image, out_transform = rio_mask(src, [mapping(wms_boundary)], crop=True)  # Use renamed mask function\n",
    "            out_meta = src.meta.copy()\n",
    "            out_meta.update({\"driver\": \"GTiff\",\n",
    "                            \"height\": out_image.shape[1],\n",
    "                            \"width\": out_image.shape[2],\n",
    "                            \"transform\": out_transform,\n",
    "                            \"crs\": crs})\n",
    "\n",
    "    # Step 6: Convert Raster to Vector\n",
    "    mask_arr = (out_image[0] != 0).astype(np.uint8)  # Assuming non-zero values are urban areas\n",
    "\n",
    "    shapes_gen = shapes(mask_arr, mask=mask_arr, transform=out_transform)\n",
    "\n",
    "    polygons = []\n",
    "    for geom, value in shapes_gen:\n",
    "        polygons.append(shape(geom))\n",
    "\n",
    "    # Create a GeoDataFrame from the polygons\n",
    "    gdf = gpd.GeoDataFrame({'geometry': polygons}, crs=crs)\n",
    "\n",
    "    # Step 7: Create Buffers Around Urban Areas\n",
    "    buffer_distance = 100  # Buffer distance in meters (adjust as needed)\n",
    "    gdf_buffered = gdf.copy()\n",
    "    gdf_buffered['geometry'] = gdf['geometry'].buffer(buffer_distance)\n",
    "\n",
    "    # Step 8: Clip the GeoDataFrame to the boundary of the place\n",
    "    gdf_clipped = gpd.clip(gdf, boundary_gdf)\n",
    "\n",
    "    return gdf_clipped\n",
    "\n",
    "\n",
    "guf = get_guf(place)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## get residential areas\n",
    "def get_residential_areas(polygon):\n",
    "    polygon = polygon.to_crs('EPSG:4326')\n",
    "    # Retrieve features from OpenStreetMap\n",
    "    features = ox.features_from_polygon(polygon.iloc[0], tags={'landuse': 'residential'})\n",
    "    \n",
    "    # Convert features to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame.from_features(features)\n",
    "    gdf = gdf.set_crs('EPSG:4326')\n",
    "    \n",
    "    return gdf\n",
    "    \n",
    "residential_areas = get_residential_areas(boundary_buffered)\n",
    "    \n",
    "    \n",
    "\n",
    "## join urban foot prints and residential areas\n",
    "# this is to create a single polygon of where neighbourhoods can be found within\n",
    "\n",
    "def join_geodataframes(gdf1, gdf2):\n",
    "    # Ensure both GeoDataFrames have the exact same CRS\n",
    "    target_crs = 'EPSG:4326'  # WGS 84\n",
    "    gdf1 = gdf1.to_crs(target_crs)\n",
    "    gdf2 = gdf2.to_crs(target_crs)\n",
    "    \n",
    "    # Concatenate GeoDataFrames\n",
    "    joined_gdf = pd.concat([gdf1, gdf2], ignore_index=True)\n",
    "    \n",
    "    return gpd.GeoDataFrame(joined_gdf, crs=target_crs)\n",
    "\n",
    "\n",
    "guf_residential_gdf = join_geodataframes(guf, residential_areas)\n",
    "\n",
    "\n",
    "## create a small buffer to ensure all areas a captured correctly\n",
    "\n",
    "def buffer_geometries_in_meters(gdf, distance):\n",
    "    # Define the World Mercator projected CRS\n",
    "    projected_crs = 'EPSG:3395'  # World Mercator\n",
    "\n",
    "    # Project to the new CRS\n",
    "    gdf_projected = gdf.to_crs(projected_crs)\n",
    "    \n",
    "    # Buffer the geometries\n",
    "    gdf_projected['geometry'] = gdf_projected['geometry'].buffer(distance)\n",
    "    \n",
    "    # Reproject back to the original CRS\n",
    "    gdf_buffered = gdf_projected.to_crs(gdf.crs)\n",
    "    \n",
    "    return gdf_buffered\n",
    "\n",
    "\n",
    "guf_residential_gdf = buffer_geometries_in_meters(guf_residential_gdf, 100)  # Buffer by 100 meters\n",
    "\n",
    "\n",
    "## union into one gdf\n",
    "\n",
    "def unary_union_polygons(gdf):\n",
    "    # Combine all geometries into a single geometry\n",
    "    unified_geometry = unary_union(gdf['geometry'])\n",
    "    \n",
    "    # Create a new GeoDataFrame with a single row containing the unified geometry\n",
    "    combined_gdf = gpd.GeoDataFrame({'geometry': [unified_geometry]}, crs=gdf.crs)\n",
    "    \n",
    "    return combined_gdf\n",
    "\n",
    "\n",
    "guf_residential_gdf = unary_union_polygons(guf_residential_gdf)\n",
    "\n",
    "# set to BNG\n",
    "guf_residential_gdf = guf_residential_gdf.to_crs(\"27700\")\n",
    "\n",
    "# Function to remove holes from neighbourhoods\n",
    "def remove_holes(polygon):\n",
    "    if polygon.geom_type == 'Polygon':\n",
    "        return Polygon(polygon.exterior)\n",
    "    else:\n",
    "        return polygon\n",
    "\n",
    "# remove holes from urban footprint\n",
    "guf_residential_gdf['geometry'] = guf_residential_gdf['geometry'].apply(remove_holes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_rivers(boundary_buffered):\n",
    "    \"\"\"\n",
    "    Retrieves river features within a given boundary.\n",
    "\n",
    "    Args:\n",
    "        boundary_buffered (GeoDataFrame): A GeoDataFrame representing the buffered boundary.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: A GeoDataFrame containing the river features within the boundary.\n",
    "    \"\"\"\n",
    "    # Ensure the boundary is in the correct CRS for the query\n",
    "    boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "    # Check the content of boundary_buffered to ensure it's not empty and correctly transformed\n",
    "    if boundary_buffered.empty:\n",
    "        raise ValueError(\"The provided boundary is empty.\")\n",
    "\n",
    "    # Define the tags for waterways\n",
    "    tags = {\"waterway\": [\"river\", \"rapids\"]}\n",
    "\n",
    "    try:\n",
    "        # Fetch features from OSM using the boundary geometry\n",
    "        polygon = boundary_buffered.geometry.iloc[0]\n",
    "        rivers = ox.features_from_polygon(polygon=polygon, tags=tags)\n",
    "\n",
    "        # Dropping rows where 'tunnel' is equal to 'culvert'\n",
    "        if 'tunnel' in rivers.columns:\n",
    "            rivers = rivers[rivers['tunnel'] != 'culvert']\n",
    "\n",
    "        # Convert the CRS back to the desired one\n",
    "        rivers = rivers.to_crs('EPSG:27700')\n",
    "\n",
    "        # Set the geometry column explicitly\n",
    "        rivers = rivers.set_geometry('geometry')\n",
    "\n",
    "        return rivers\n",
    "\n",
    "    except InsufficientResponseError:\n",
    "        print(\"No data elements found for the given boundary and tags.\")\n",
    "        empty_geometry = {'geometry': [LineString()]}\n",
    "        rivers = gpd.GeoDataFrame(empty_geometry, crs='EPSG:27700')\n",
    "        return rivers # Return an empty GeoDataFrame if no data found\n",
    "\n",
    "\n",
    "\n",
    "def get_landuse(boundary_buffered):\n",
    "    \"\"\"\n",
    "    Retrieves the landuse features based on the specified boundary.\n",
    "\n",
    "    Args:\n",
    "        boundary_buffered (GeoDataFrame): The buffered boundary polygon.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: The landuse features.\n",
    "    \"\"\"\n",
    "    # reset boundary crs to allow for features to be found\n",
    "    boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "    # Define tags\n",
    "    tags = {\"landuse\": [\"industrial\", \"railway\", \"brownfield\", \"commercial\", \"farmland\", \"meadow\"]}\n",
    "    # Use ox.features_from_polygon to find features matching the specified tags\n",
    "    landuse = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "    # set/reset crs\n",
    "    landuse = landuse.to_crs('27700')\n",
    "\n",
    "    ## get unsuitable \"nature\" types\n",
    "    # Define tags\n",
    "    tags = {\"natural\": [\"wood\", \"water\", \"scrub\", \"coastline\", \"beach\"]}\n",
    "    # Use ox.features_from_polygon to find features matching the specified tags\n",
    "    nature = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "    # set/reset crs\n",
    "    nature = nature.to_crs('27700')\n",
    "\n",
    "    ## get unsuitable \"lesiure\" types. This is mainly for golfcourses\n",
    "    # Define tags\n",
    "    tags = {\"leisure\": [\"golf_course\", \"track\", \"park\"]}\n",
    "    # Use ox.features_from_polygon to find features matching the specified tags\n",
    "    leisure = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "    # set/reset crs\n",
    "    leisure = leisure.to_crs('27700')\n",
    "    # Define the tags for aeroway\n",
    "    aeroway_tags = {\"aeroway\": [\"aerodrome\"]}\n",
    "    # Use the function to retrieve aeroway features\n",
    "    aeroway = retrieve_osm_features(polygon=boundary_buffered.iloc[0], tags=aeroway_tags)\n",
    "    # Check if any features were retrieved\n",
    "    if aeroway is not None:\n",
    "        if not aeroway.empty:\n",
    "            # set/reset crs\n",
    "            aeroway = aeroway.to_crs('27700')\n",
    "\n",
    "    # concat\n",
    "    landuse = pd.concat([landuse, nature, leisure, aeroway])\n",
    "\n",
    "    ## resest boundary crs\n",
    "    boundary_buffered = boundary_buffered.to_crs('EPSG:27700')\n",
    "\n",
    "    return landuse\n",
    "\n",
    "\n",
    "def get_bus_routes(boundary_buffered):\n",
    "        \"\"\"\n",
    "        Retrieves bus routes from OSM/NAPTAN within a given boundary.\n",
    "\n",
    "        Args:\n",
    "                boundary_buffered (GeoDataFrame): A GeoDataFrame representing the boundary.\n",
    "\n",
    "        Returns:\n",
    "                bus_routes (GeoDataFrame): A GeoDataFrame containing the bus routes.\n",
    "\n",
    "        Raises:\n",
    "                Exception: If there is an error fetching the data from the Overpass API.\n",
    "        \"\"\"\n",
    "        # reset boundary crs to allow for features to be found\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "        # Calculate the bounding box for XML query\n",
    "        bounding_box = boundary_buffered.bounds\n",
    "\n",
    "        # Extract the minimum and maximum coordinates\n",
    "        minx = bounding_box['minx'].min()\n",
    "        miny = bounding_box['miny'].min()\n",
    "        maxx = bounding_box['maxx'].max()\n",
    "        maxy = bounding_box['maxy'].max()\n",
    "\n",
    "        # Create a list of four elements representing the bounding box\n",
    "        bbox = [minx, miny, maxx, maxy]\n",
    "\n",
    "        # reset boundary_buffer crs\n",
    "        boundary_buffered = boundary_buffered.to_crs('27700')\n",
    "\n",
    "        # Define the Overpass API endpoint\n",
    "        overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "        # Define the XML query\n",
    "        xml_query = f\"\"\"\n",
    "        <osm-script output=\"json\" output-config=\"\" timeout=\"160\">\n",
    "            <union into=\"_\">\n",
    "                <query into=\"_\" type=\"node\">\n",
    "                    <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                    <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                </query>\n",
    "                <query into=\"_\" type=\"way\">\n",
    "                    <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                    <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                </query>\n",
    "                <query into=\"_\" type=\"relation\">\n",
    "                    <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                    <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                </query>\n",
    "            </union>\n",
    "            <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"body\" n=\"\" order=\"id\" s=\"\" w=\"\"/>\n",
    "            <recurse from=\"_\" into=\"_\" type=\"down\"/>\n",
    "            <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"skeleton\" n=\"\" order=\"quadtile\" s=\"\" w=\"\"/>\n",
    "        </osm-script>\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize lists to store data\n",
    "        geometries = []\n",
    "        element_data = []\n",
    "\n",
    "        # Make the Overpass API request\n",
    "        response = requests.post(overpass_url, data=xml_query)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                # Access the data from the response\n",
    "                for element in data.get(\"elements\", []):\n",
    "                        if element.get('type') == 'way' and 'geometry' in element:\n",
    "                                # Extract geometry coordinates from 'geometry' field\n",
    "                                coordinates = [(node['lon'], node['lat']) for node in element['geometry']]\n",
    "                                # Create a LineString geometry\n",
    "                                line = LineString(coordinates)\n",
    "                                geometries.append(line)\n",
    "                                element_data.append(element)\n",
    "\n",
    "                # Create a GeoDataFrame\n",
    "                bus_routes = gpd.GeoDataFrame(element_data, geometry=geometries)\n",
    "\n",
    "                # Set CRS\n",
    "                bus_routes = bus_routes.set_crs('4326')\n",
    "                bus_routes = bus_routes.to_crs('27700')\n",
    "\n",
    "                return bus_routes\n",
    "\n",
    "        else:\n",
    "                raise Exception(f\"Error fetching data: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "\n",
    "def clip_boundaries(os_open_roads, rivers, railways, landuse, bus_routes, boundary_buffered):\n",
    "    \"\"\"\n",
    "    Clips the geospatial data to the boundary_buffered extent.\n",
    "\n",
    "    Parameters:\n",
    "    - os_open_roads (GeoDataFrame): lines from OS Open roads.\n",
    "    - rivers (GeoDataFrame): lines of Rivers.\n",
    "    - railways (GeoDataFrame): lines of Railways.\n",
    "    - landuse (GeoDataFrame): Land use polygons.\n",
    "    - bus_routes (GeoDataFrame): lines of bus routes.\n",
    "    - boundary_buffered (GeoDataFrame): buffered boundary.\n",
    "\n",
    "    Returns:\n",
    "    - clipped versions of input geodataframes, aside from the bufferd boundary.\n",
    "    \"\"\"\n",
    "    os_open_roads_clip = gpd.clip(os_open_roads, boundary_buffered)\n",
    "    rivers_clip = gpd.clip(rivers, boundary_buffered)\n",
    "    railways_clip = gpd.clip(railways, boundary_buffered)\n",
    "    landuse_clip = gpd.clip(landuse, boundary_buffered)\n",
    "    bus_routes_clip = gpd.clip(bus_routes, boundary_buffered)\n",
    "\n",
    "    return os_open_roads_clip, rivers_clip, railways_clip, landuse_clip, bus_routes_clip\n",
    "\n",
    "\n",
    "def process_bus_routes(bus_routes_clip, buffer_distance):\n",
    "    \"\"\"\n",
    "    Count the number of bus routes per road and remove roads with more than one bus route on them.\n",
    "    \n",
    "    Args:\n",
    "        bus_routes_clip (GeoDataFrame): The input GeoDataFrame containing bus routes.\n",
    "        buffer_distance (float): The buffer distance to convert roads to polygons, set in meters.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: The filtered GeoDataFrame containing roads with less than or equal to one bus route.\n",
    "    \"\"\"\n",
    "    # Create a new GeoDataFrame with the buffered geometries\n",
    "    bus_routes_buffered = bus_routes_clip.copy()  # Copy the original GeoDataFrame\n",
    "    bus_routes_buffered['geometry'] = bus_routes_buffered['geometry'].buffer(buffer_distance)\n",
    "\n",
    "    # count the number of overlapping bus routes\n",
    "    def count_overlapping_features(gdf):\n",
    "        \"\"\"\n",
    "        Count the number of overlapping features in a GeoDataFrame.\n",
    "        \n",
    "        Args:\n",
    "            gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            GeoDataFrame: The input GeoDataFrame with an additional column 'Bus_routes_count' indicating the count of overlapping features.\n",
    "        \"\"\"\n",
    "        # Create an empty column to store the count of overlapping features\n",
    "        gdf['Bus_routes_count'] = 0\n",
    "\n",
    "        # Iterate through each row in the GeoDataFrame\n",
    "        for idx, row in gdf.iterrows():\n",
    "            # Get the geometry of the current row\n",
    "            geometry = row['geometry']\n",
    "            \n",
    "            # Use a spatial filter to find overlapping features\n",
    "            overlaps = gdf[gdf['geometry'].intersects(geometry)]\n",
    "            \n",
    "            # Update the Bus_routes_count column with the count of overlapping features\n",
    "            gdf.at[idx, 'Bus_routes_count'] = len(overlaps)\n",
    "        \n",
    "        return gdf\n",
    "\n",
    "    # call function\n",
    "    bus_routes_buffered_with_count = count_overlapping_features(bus_routes_buffered)\n",
    "\n",
    "    # drop any roads which have less than two bus routes on them\n",
    "    bus_routes_filtered = bus_routes_buffered_with_count[bus_routes_buffered_with_count['Bus_routes_count'] >= 2]\n",
    "    \n",
    "    return bus_routes_filtered\n",
    "\n",
    "\n",
    "\n",
    "def filter_OS_boundary_roads(os_open_roads_clip):\n",
    "    \"\"\"\n",
    "    Filter the `os_open_roads_clip` DataFrame to select boundary roads.\n",
    "\n",
    "    This function filters the `os_open_roads_clip` DataFrame to select roads that are considered \"boundary\" roads. \n",
    "    The selection criteria include roads that have the following attributes:\n",
    "    - `primary_route` is True\n",
    "    - `trunk_road` is True\n",
    "    - `fictitious` is True\n",
    "    - `road_classification` is 'A Road' or 'B Road'\n",
    "    - `road_function` is 'Minor Road' or 'Motorway'\n",
    "\n",
    "    The filtered DataFrame is returned.\n",
    "\n",
    "    Note: The commented line `(os_open_roads_clip['road_function'] == 'Restricted Local Access Road')` is excluded from the selection.\n",
    "\n",
    "    Parameters:\n",
    "    - os_open_roads_clip (DataFrame): A DataFrame containing road data.\n",
    "\n",
    "    Returns:\n",
    "    - boundary_roads (DataFrame): A DataFrame containing the filtered boundary roads.\n",
    "\n",
    "    Example usage:\n",
    "        # Assuming `os_open_roads_clip` is a DataFrame containing road data\n",
    "        boundary_roads = filter_boundary_roads(os_open_roads_clip)\n",
    "    \"\"\"\n",
    "    boundary_roads = os_open_roads_clip.loc[((os_open_roads_clip['primary_route'] == 'True') |\n",
    "                    (os_open_roads_clip['trunk_road'] == 'True') |\n",
    "                    (os_open_roads_clip['fictitious'] == 'True') |\n",
    "                    (os_open_roads_clip['road_classification'] == 'A Road') | \n",
    "                    (os_open_roads_clip['road_classification'] == 'B Road') | \n",
    "                    (os_open_roads_clip['road_function'] == 'Minor Road') |\n",
    "                    (os_open_roads_clip['road_function'] == 'Motorway') |\n",
    "                    (os_open_roads_clip['road_function'] == 'Minor Road')  \n",
    "                    )]\n",
    "    return boundary_roads\n",
    "\n",
    "\n",
    "\n",
    "## buffering and dissolving functions\n",
    "\n",
    "def buffer_and_dissolve(input_gdf):\n",
    "    \"\"\"\n",
    "    Buffer and dissolve a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        input_gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame: The buffered and dissolved GeoDataFrame.\n",
    "    \"\"\"\n",
    "    # Buffer around boundaries\n",
    "    buffered_gdf = input_gdf.copy()  # Create a copy to avoid modifying the original\n",
    "    buffered_gdf['geometry'] = buffered_gdf['geometry'].buffer(5) # set a 5 meter buffer\n",
    "\n",
    "    # Dissolve the geometries\n",
    "    dissolved_geo = buffered_gdf.unary_union\n",
    "\n",
    "    # Create a new GeoDataFrame with the dissolved geometry\n",
    "    dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "\n",
    "    # Set the CRS (Coordinate Reference System)\n",
    "    dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "    return dissolved_gdf\n",
    "\n",
    "\n",
    "def dissolve_gdf(input_gdf):\n",
    "    # dissolve geometries\n",
    "    dissolved_geo = input_gdf.unary_union\n",
    "    dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "    dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "    return dissolved_gdf\n",
    "\n",
    "\n",
    "def erase_boundary_features(boundary, boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd, guf_residential_gdf):\n",
    "    \"\"\"\n",
    "    Erases boundary features from the given boundary geometry.\n",
    "\n",
    "    Parameters:\n",
    "    - boundary: GeoDataFrame representing the boundary geometry\n",
    "    - boundary_rivers_bd: GeoDataFrame representing the rivers boundary features\n",
    "    - boundary_roads_bd: GeoDataFrame representing the roads boundary features\n",
    "    - boundary_rail_bd: GeoDataFrame representing the rail boundary features\n",
    "    - boundary_landuse_bd: GeoDataFrame representing the landuse boundary features\n",
    "    - boundary_bus_routes_bd: GeoDataFrame representing the bus routes boundary features\n",
    "\n",
    "    Returns:\n",
    "    - erased_boundary_gdf: GeoDataFrame containing the result of the \"Erase\" operation\n",
    "    \"\"\"\n",
    "\n",
    "    # ensure that neighbourhoods fall only within urban footprint areas\n",
    "    boundary = gpd.clip(boundary, guf_residential_gdf)\n",
    "\n",
    "    # Join all boundary features\n",
    "    boundaries = pd.concat([boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd], ignore_index=True)\n",
    "    boundary_features = dissolve_gdf(boundaries)\n",
    "\n",
    "    # Use the `difference` method to perform the \"Erase\" operation\n",
    "    erased_boundary = boundary.difference(boundary_features.unary_union)\n",
    "\n",
    "    # Convert the GeoSeries to a single geometry using unary_union\n",
    "    erased_boundary = erased_boundary.unary_union\n",
    "\n",
    "    # Create a new GeoDataFrame with the result of \"Erase\" operation\n",
    "    erased_boundary_gdf = gpd.GeoDataFrame(geometry=[erased_boundary], crs=boundary.crs)\n",
    "\n",
    "    # Explode multipolygon to polygons\n",
    "    erased_boundary_gdf = erased_boundary_gdf.explode()\n",
    "\n",
    "    return erased_boundary_gdf\n",
    "\n",
    "\n",
    "def drop_large_or_small_areas(neighbourhoods):\n",
    "    \"\"\"\n",
    "    Drops rows from the 'neighbourhoods' DataFrame where the area is less than 10,000 square units or greater than 5,000,000 square units.\n",
    "\n",
    "    Parameters:\n",
    "    - neighbourhoods (DataFrame): The input DataFrame containing neighbourhood data.\n",
    "\n",
    "    Returns:\n",
    "    - neighbourhoods (DataFrame): The updated DataFrame with small areas dropped.\n",
    "    \"\"\"\n",
    "    # Calculate area\n",
    "    neighbourhoods[\"area\"] = neighbourhoods.geometry.area\n",
    "\n",
    "    # Drop rows where area is less than 10,000 or greater than 5,000,000\n",
    "    neighbourhoods = neighbourhoods.loc[(neighbourhoods[\"area\"] >= 10000)]\n",
    "    neighbourhoods = neighbourhoods.loc[(neighbourhoods[\"area\"] <= 5000000)]\n",
    "\n",
    "    return neighbourhoods\n",
    "\n",
    "\n",
    "def filter_neighbourhoods_by_roads(neighbourhoods, os_open_roads_clip, polygon_column_name):\n",
    "    \"\"\"\n",
    "    Count the number of roads within each polygon in a GeoDataFrame and filter the neighbourhoods based on road count and road density.\n",
    "    \n",
    "    Args:\n",
    "        neighbourhoods (GeoDataFrame): GeoDataFrame containing neighbourhood polygons.\n",
    "        os_open_roads_clip (GeoDataFrame): GeoDataFrame containing road data.\n",
    "        polygon_column_name (str): Name of the column in neighbourhoods to use for grouping.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Updated neighbourhoods GeoDataFrame with filtered rows based on road count and road density.\n",
    "    \"\"\"\n",
    "    \n",
    "    def count_roads_within_polygons(polygons_gdf, roads_gdf, polygon_column_name):\n",
    "        \"\"\"\n",
    "        Count the number of roads within each polygon in a GeoDataFrame.\n",
    "        \n",
    "        Args:\n",
    "            polygons_gdf (GeoDataFrame): GeoDataFrame containing polygons.\n",
    "            roads_gdf (GeoDataFrame): GeoDataFrame containing roads.\n",
    "            polygon_column_name (str): Name of the column in polygons_gdf to use for grouping.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: Original polygons GeoDataFrame with a \"road_count\" column added.\n",
    "        \"\"\"\n",
    "        \n",
    "        # spatial join\n",
    "        joined = gpd.sjoin(polygons_gdf, roads_gdf, how='left', op='intersects')\n",
    "        \n",
    "        # Group by the polygon column and count the number of roads in each\n",
    "        road_counts = joined.groupby(polygon_column_name).size().reset_index(name='road_count')\n",
    "        \n",
    "        # Merge the road counts back into the polygons GeoDataFrame\n",
    "        polygons_gdf = polygons_gdf.merge(road_counts, on=polygon_column_name, how='left')\n",
    "\n",
    "        # Calculate road density (area divided by road_count). It is multiplied by 10000 for ease of understanding the numbers involved with this\n",
    "        polygons_gdf['road_density'] = (polygons_gdf['road_count'] / polygons_gdf['area'] ) * 10000\n",
    "        \n",
    "        return polygons_gdf\n",
    "    \n",
    "    neighbourhoods = count_roads_within_polygons(neighbourhoods, os_open_roads_clip, polygon_column_name)\n",
    "\n",
    "    # Drop rows with road_density below 0.2 or less than 4 roads\n",
    "    neighbourhoods = neighbourhoods[(neighbourhoods['road_count'] > 2)]\n",
    "    neighbourhoods = neighbourhoods[(neighbourhoods['road_density'] > 0.2)]\n",
    "    \n",
    "    return neighbourhoods\n",
    "\n",
    "\n",
    "def remove_holes(polygon):\n",
    "    \"\"\"\n",
    "    Removes holes from a polygon. Mostly for visual reasons.\n",
    "\n",
    "    Parameters:\n",
    "    polygon (Polygon): The polygon to remove holes from.\n",
    "\n",
    "    Returns:\n",
    "    Polygon: The polygon without holes.\n",
    "    \"\"\"\n",
    "    if polygon.geom_type == 'Polygon':\n",
    "        return Polygon(polygon.exterior)\n",
    "    else:\n",
    "        return polygon\n",
    "\n",
    "landuse = get_landuse(boundary_buffered)\n",
    "rivers = get_rivers(boundary_buffered)\n",
    "railways = get_railways(place)\n",
    "landuse = get_landuse(boundary_buffered)\n",
    "bus_routes = get_bus_routes(boundary_buffered)\n",
    "os_open_roads_clip, rivers_clip, railways_clip, landuse_clip, bus_routes_clip = clip_boundaries(os_open_roads, rivers, railways, landuse, bus_routes, boundary_buffered)\n",
    "bus_routes_clip = process_bus_routes(bus_routes_clip, 0.2)\n",
    "boundary_roads = filter_OS_boundary_roads(os_open_roads_clip)\n",
    "\n",
    "## buffer and dissolve \n",
    "boundary_roads_bd = buffer_and_dissolve(boundary_roads)\n",
    "boundary_rivers_bd = buffer_and_dissolve(rivers_clip)\n",
    "boundary_rail_bd = buffer_and_dissolve(railways_clip)\n",
    "boundary_landuse_bd = buffer_and_dissolve(landuse_clip)\n",
    "boundary_bus_routes_bd = buffer_and_dissolve(bus_routes_clip)\n",
    "\n",
    "## geodataframe cleaning\n",
    "erased_boundary_gdf = erase_boundary_features(boundary, boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd, guf_residential_gdf)\n",
    "neighbourhoods = erased_boundary_gdf\n",
    "neighbourhoods = drop_large_or_small_areas(neighbourhoods)\n",
    "\n",
    "neighbourhoods = filter_neighbourhoods_by_roads(neighbourhoods, os_open_roads_clip, 'geometry')\n",
    "\n",
    "## create unique IDs\n",
    "# simple number based ID\n",
    "neighbourhoods['ID'] = range(1, len(neighbourhoods) + 1)\n",
    "\n",
    "neighbourhoods['geometry'] = neighbourhoods['geometry'].apply(remove_holes)\n",
    "\n",
    "\n",
    "## filter neighbourhoods to only locations with more than 1 intersection (1 or fewer intersections indicates that all travel modes will be the same)\n",
    "# reset neighbourhoods crs\n",
    "neighbourhoods = neighbourhoods.to_crs('4326')\n",
    "\n",
    "# Spatial join to count points within each neighborhood\n",
    "spatial_join = gpd.sjoin(neighbourhoods, common_nodes_gdf, how='left', op='contains')\n",
    "\n",
    "# Group by 'ID' and count the points within each neighborhood\n",
    "point_counts = spatial_join.groupby('ID').size().reset_index(name='point_count')\n",
    "\n",
    "# Filter out neighborhoods with 1 or 0 points\n",
    "filtered_neighbourhood_ids = point_counts[point_counts['point_count'] > 1]['ID']\n",
    "\n",
    "neighbourhoods= neighbourhoods[neighbourhoods['ID'].isin(filtered_neighbourhood_ids)]\n",
    "\n",
    "\n",
    "\n",
    "## we also need to join the length of the streets within the neighbourhood for further analysis\n",
    "# Reset index of neighbourhoods\n",
    "neighbourhoods = neighbourhoods.reset_index(drop=True)\n",
    "\n",
    "# reset neighbourhoods crs\n",
    "neighbourhoods = neighbourhoods.to_crs('27700')\n",
    "\n",
    "# Perform a spatial join\n",
    "joined_data = gpd.sjoin(os_open_roads_clip, neighbourhoods, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "# Group by neighborhood and calculate total road length\n",
    "road_lengths = joined_data.groupby('index_right')['length'].sum().reset_index()\n",
    "\n",
    "# Merge road_lengths with neighbourhoods and drop 'index_right' column\n",
    "neighbourhoods = neighbourhoods.merge(road_lengths, left_index=True, right_on='index_right', how='left').drop(columns=['index_right'])\n",
    "\n",
    "# Rename the column\n",
    "neighbourhoods.rename(columns={'length': 'road_lengths'}, inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OSM_street_networks(boundary_buffered):\n",
    "    \"\"\"\n",
    "    Retrieves street networks for all, walking, and driving modes within the specified boundary.\n",
    "\n",
    "    Parameters:\n",
    "    - boundary_buffered: A GeoDataFrame representing the boundary of the area of interest.\n",
    "\n",
    "    Returns:\n",
    "    - all_edges: A GeoDataFrame containing the edges (streets) of the entire street network.\n",
    "    - all_nodes: A GeoDataFrame containing the nodes (intersections) of the entire street network.\n",
    "    - walk_edges: A GeoDataFrame containing the edges (streets) of the walking street network.\n",
    "    - walk_nodes: A GeoDataFrame containing the nodes (intersections) of the walking street network.\n",
    "    - drive_edges: A GeoDataFrame containing the edges (streets) of the driving street network.\n",
    "    - drive_nodes: A GeoDataFrame containing the nodes (intersections) of the driving street network.\n",
    "    - common_nodes_gdf: A GeoDataFrame containing the common nodes between the driving and walking street networks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reset boundary_buffered crs for passing to OSM\n",
    "    boundary_buffered_4326 = boundary_buffered.to_crs('4326')\n",
    "\n",
    "    # Get street networks\n",
    "    all_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='all', simplify=False)\n",
    "    walk_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='walk', simplify=True)\n",
    "    drive_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='drive', simplify=False)\n",
    "\n",
    "    all_edges = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "    all_nodes = ox.graph_to_gdfs(all_streets, nodes=True, edges=False)\n",
    "\n",
    "    walk_edges = ox.graph_to_gdfs(walk_streets, nodes=False, edges=True)\n",
    "    walk_nodes = ox.graph_to_gdfs(walk_streets, nodes=True, edges=False)\n",
    "\n",
    "    drive_edges = ox.graph_to_gdfs(drive_streets, nodes=False, edges=True)\n",
    "    drive_nodes = ox.graph_to_gdfs(drive_streets, nodes=True, edges=False)\n",
    "\n",
    "    # Find the common nodes between networks\n",
    "    # This ensures that shortest paths between points should always be able to be calculated\n",
    "    common_nodes = drive_nodes.merge(walk_nodes, on='osmid', suffixes=('_drive', '_walk'))\n",
    "    common_nodes_gdf = gpd.GeoDataFrame(common_nodes, geometry='geometry_drive')\n",
    "\n",
    "    return all_edges, all_nodes, walk_edges, walk_nodes, drive_edges, drive_nodes, common_nodes_gdf, all_streets, walk_streets, drive_streets\n",
    "\n",
    "\n",
    "# get street networks\n",
    "all_edges, all_nodes, walk_edges, walk_nodes, drive_edges, drive_nodes, common_nodes_gdf, all_streets, walk_streets, drive_streets = get_OSM_street_networks(boundary_buffered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_buffered_4326.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_name = \"Borough of Wyre, United Kingdom\"\n",
    "\n",
    "# Download and create GeoDataFrames for streets, buildings, and other features\n",
    "# For streets\n",
    "#gdf_streets = ox.graph_to_gdfs(ox.graph_from_polygon(boundary_buffered_4326.iloc[0], network_type='all'), nodes=False) # actual boundary\n",
    "gdf_streets = ox.graph_to_gdfs(ox.graph_from_polygon(buffered_bounding_box_gdf_4326.iloc[0].geometry, network_type='all'), nodes=False) # bounding box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_streets.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY OF FULL CODE BUT WITH BOUNDING BOX INSTEAD OF ACTUAL BOUNDARY\n",
    "\n",
    "\n",
    "for place in places:\n",
    "\n",
    "    # get boundary\n",
    "    def set_location_boundary(place):\n",
    "        \"\"\"\n",
    "        Sets up the location boundary by geocoding the given place and buffering it.\n",
    "\n",
    "        Parameters:\n",
    "        place (str): The name or address of the place to geocode.\n",
    "\n",
    "        Returns:\n",
    "        geopandas.GeoDataFrame: The buffered boundary of the location.\n",
    "        \"\"\"\n",
    "        # Set location and get boundary\n",
    "        boundary = ox.geocode_to_gdf(place)\n",
    "        boundary = boundary.to_crs('EPSG:27700')\n",
    "\n",
    "        # Buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "        boundary_buffered = boundary.buffer(50)\n",
    "\n",
    "        return boundary_buffered, boundary\n",
    "\n",
    "    boundary_buffered, boundary = set_location_boundary(place)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This code retrieves street nodes and edges for walking and driving from OpenStreetMap within our area boundary, and loads the OS Open Roads network dataset.\n",
    "\n",
    "    Functions:\n",
    "    - get_street_networks: Retrieves street networks for all, walking, and driving modes within the specified boundary.\n",
    "    - boundary_bounding_box: Buffers a bounding box around the boundary and returns it as a GeoDataFrame in EPSG:4326. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def buffer_bounding_box(boundary_gdf, buffer_distance):\n",
    "        \"\"\"\n",
    "        Buffers a bounding box around the boundary and returns it as a GeoDataFrame in EPSG:4326.\n",
    "        \n",
    "        Parameters:\n",
    "        boundary_gdf (GeoDataFrame): Boundary geometry in EPSG:4326.\n",
    "        buffer_distance (float): Buffer distance in meters. Default is 500 meters.\n",
    "        \n",
    "        Returns:\n",
    "        GeoDataFrame: Buffered bounding box in EPSG:4326.\n",
    "        \"\"\"\n",
    "        # Create a bounding box from the boundary\n",
    "        bbox_4326 = gpd.GeoDataFrame(geometry=[box(*boundary_gdf.total_bounds)], crs=boundary_gdf.crs)\n",
    "        \n",
    "        # Reproject to Web Mercator for buffering\n",
    "        bbox_web_mercator = bbox_4326.to_crs(\"EPSG:3857\")\n",
    "        \n",
    "        # Buffer in Web Mercator\n",
    "        buffered_bounding_box_web_mercator = bbox_web_mercator.buffer(buffer_distance)\n",
    "        \n",
    "        # Convert back to EPSG:4326\n",
    "        buffered_bounding_box_4326 = buffered_bounding_box_web_mercator.to_crs(\"EPSG:4326\")\n",
    "        \n",
    "        return buffered_bounding_box_4326\n",
    "\n",
    "    # get bounding box\n",
    "    buffered_bounding_box_gdf = buffer_bounding_box(boundary_buffered_4326, buffer_distance=1000)\n",
    "\n",
    "    \n",
    "\n",
    "    def get_OSM_street_networks(boundary_buffered, bounding_box):\n",
    "        \"\"\"\n",
    "        Retrieves street networks for all, walking, and driving modes within the specified bounding box, \n",
    "        then clips them to the specified boundary.\n",
    "\n",
    "        Parameters:\n",
    "        - boundary_buffered: A GeoDataFrame representing the original boundary of the area of interest.\n",
    "        - bounding_box: A GeoDataFrame representing the buffered bounding box around the boundary.\n",
    "\n",
    "        Returns:\n",
    "        - all_edges: A GeoDataFrame containing the edges (streets) of the entire street network.\n",
    "        - all_nodes: A GeoDataFrame containing the nodes (intersections) of the entire street network.\n",
    "        - walk_edges: A GeoDataFrame containing the edges (streets) of the walking street network.\n",
    "        - walk_nodes: A GeoDataFrame containing the nodes (intersections) of the walking street network.\n",
    "        - drive_edges: A GeoDataFrame containing the edges (streets) of the driving street network.\n",
    "        - drive_nodes: A GeoDataFrame containing the nodes (intersections) of the driving street network.\n",
    "        - common_nodes_gdf: A GeoDataFrame containing the common nodes between the driving and walking street networks.\n",
    "        - all_streets: The full street graph for all modes.\n",
    "        - walk_streets: The full street graph for walking mode.\n",
    "        - drive_streets: The full street graph for driving mode.\n",
    "        \"\"\"\n",
    "        # Ensure both boundary and bounding box are in EPSG:4326 for compatibility with OSM\n",
    "        boundary_buffered_4326 = boundary_buffered.to_crs('EPSG:4326')\n",
    "        bounding_box_4326 = bounding_box.to_crs('EPSG:4326')\n",
    "\n",
    "        # Get street networks using the bounding box\n",
    "        all_streets = ox.graph_from_polygon(bounding_box_4326.geometry.iloc[0], network_type='all', simplify=False)\n",
    "        walk_streets = ox.graph_from_polygon(bounding_box_4326.geometry.iloc[0], network_type='walk', simplify=True)\n",
    "        drive_streets = ox.graph_from_polygon(bounding_box_4326.geometry.iloc[0], network_type='drive', simplify=False)\n",
    "\n",
    "        # Convert graphs to GeoDataFrames\n",
    "        all_edges = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "        all_nodes = ox.graph_to_gdfs(all_streets, nodes=True, edges=False)\n",
    "\n",
    "        walk_edges = ox.graph_to_gdfs(walk_streets, nodes=False, edges=True)\n",
    "        walk_nodes = ox.graph_to_gdfs(walk_streets, nodes=True, edges=False)\n",
    "\n",
    "        drive_edges = ox.graph_to_gdfs(drive_streets, nodes=False, edges=True)\n",
    "        drive_nodes = ox.graph_to_gdfs(drive_streets, nodes=True, edges=False)\n",
    "\n",
    "        # Clip street networks to the original boundary\n",
    "        all_edges_clipped = all_edges.clip(boundary_buffered_4326)\n",
    "        walk_edges_clipped = walk_edges.clip(boundary_buffered_4326)\n",
    "        drive_edges_clipped = drive_edges.clip(boundary_buffered_4326)\n",
    "\n",
    "        # Clip nodes to the original boundary as well\n",
    "        all_nodes_clipped = all_nodes.clip(boundary_buffered_4326)\n",
    "        walk_nodes_clipped = walk_nodes.clip(boundary_buffered_4326)\n",
    "        drive_nodes_clipped = drive_nodes.clip(boundary_buffered_4326)\n",
    "\n",
    "        # Find the common nodes between the driving and walking networks\n",
    "        common_nodes = drive_nodes_clipped.merge(walk_nodes_clipped, on='osmid', suffixes=('_drive', '_walk'))\n",
    "        common_nodes_gdf = gpd.GeoDataFrame(common_nodes, geometry='geometry_drive')\n",
    "\n",
    "        return (\n",
    "            all_edges_clipped, all_nodes_clipped, walk_edges_clipped, walk_nodes_clipped,\n",
    "            drive_edges_clipped, drive_nodes_clipped, common_nodes_gdf,\n",
    "            all_streets, walk_streets, drive_streets\n",
    "        )\n",
    "\n",
    "    # get street networks\n",
    "    all_edges, all_nodes, walk_edges, walk_nodes, drive_edges, drive_nodes, common_nodes_gdf, all_streets, walk_streets, drive_streets = get_OSM_street_networks(boundary_buffered_4326, bounding_box_gdf)\n",
    "\n",
    "    #os_open_roads = get_OS_roads()  this is now got at the start of the code to avoid re-reading\n",
    "\n",
    "\n",
    "    def retrieve_osm_features(polygon, tags):\n",
    "        \"\"\"\n",
    "        Retrieves OpenStreetMap features based on the specified polygon and tags.\n",
    "\n",
    "        Args:\n",
    "            polygon (Polygon): The polygon to retrieve features within.\n",
    "            tags (dict): The tags to filter the features.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: The retrieved OpenStreetMap features.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            features = ox.features_from_polygon(polygon=polygon, tags=tags)\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            if \"There are no data elements in the server response\" in error_message:\n",
    "                print(\"No data elements found for the specified location/tags.\")\n",
    "                features = gpd.GeoDataFrame()  # Create an empty GeoDataFrame\n",
    "            else:\n",
    "                # Handle other exceptions here if needed\n",
    "                print(\"An error occurred:\", error_message)\n",
    "                features = None\n",
    "        return features\n",
    "\n",
    "\n",
    "    def get_railways(place):\n",
    "        \"\"\"\n",
    "        This retrievies and processes OpenStreetMap (OSM) railways data for a specified place.\n",
    "\n",
    "        Args:\n",
    "            place (str): The name of the place to retrieve OSM features for.\n",
    "\n",
    "        Returns:\n",
    "            railways (geopandas.GeoDataFrame): A GeoDataFrame containing the railways within the specified place.\n",
    "        \"\"\"\n",
    "\n",
    "        # for unknown reasons, using rail = ox.graph_from_place(place, custom_filter='[\"railway\"]')\n",
    "        # doesn't ALWAYS retrive the full rail network, hence why multiple lines are used to achive the same result\n",
    "\n",
    "        # Define railway types to retrieve\n",
    "        railway_types = [\"\", \"rail\", \"light_rail\", \"narrow_gauge\", \"subway\", \"tram\"]\n",
    "\n",
    "        # Initialize an empty graph\n",
    "        combined_railways = nx.MultiDiGraph()\n",
    "\n",
    "        for railway_type in railway_types:\n",
    "            try:\n",
    "                # Fetch the railway network for the specified type\n",
    "                network = ox.graph_from_place(place, simplify=False, custom_filter=f'[\"railway\"~\"{railway_type}\"]')\n",
    "\n",
    "                # Ensure the fetched network is a MultiDiGraph\n",
    "                if not isinstance(network, nx.MultiDiGraph):\n",
    "                    network = nx.MultiDiGraph(network)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"No railway data found for '{railway_type}'.\")\n",
    "                network = nx.MultiDiGraph()\n",
    "\n",
    "            # Compose the networks\n",
    "            combined_railways = nx.compose(combined_railways, network)\n",
    "\n",
    "        # Convert to GeoDataFrame\n",
    "        railways = ox.graph_to_gdfs(combined_railways, nodes=False, edges=True)\n",
    "\n",
    "        # Drop any other railway types that aren't needed\n",
    "        railways = railways.loc[(~railways[\"railway\"].isin([\"tunnel\", \"abandoned\", \"razed\", \"disused\", \"funicular\", \"monorail\", \"miniature\"]))]\n",
    "\n",
    "        # Drop rows where any of the specified columns have values \"True\" or \"yes\"\n",
    "        columns_to_check = ['tunnel', 'abandoned', 'razed', 'disused', 'funicular', 'monorail', 'miniature']\n",
    "        railways = railways.loc[~railways[railways.columns.intersection(columns_to_check)].isin(['True', 'yes']).any(axis=1)]\n",
    "\n",
    "        # Set railways CRS\n",
    "        railways = railways.to_crs('EPSG:27700')\n",
    "\n",
    "        return railways\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ## get urban footprints from GUF\n",
    "\n",
    "    def get_guf(place):\n",
    "        \"\"\"\n",
    "        Retrieves a clipped GeoDataFrame of GUF urban areas within a specified place boundary.\n",
    "\n",
    "        Parameters:\n",
    "        - place (str): The name or address of the place to retrieve urban areas for.\n",
    "\n",
    "        Returns:\n",
    "        - gdf_clipped (GeoDataFrame): A GeoDataFrame containing the clipped urban areas within the specified place boundary.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Access the WMS Service\n",
    "        wms_url = 'https://geoservice.dlr.de/eoc/land/wms?GUF04_DLR_v1_Mosaic'\n",
    "        wms = WebMapService(wms_url, version='1.1.1')\n",
    "\n",
    "        # Step 2: Identify the Layer with ID 102. This is the Global Urban Footprint layer GUF\n",
    "        for layer_name, layer in wms.contents.items():\n",
    "            if '102' in layer_name:\n",
    "                print(f\"Layer ID 102 found: {layer_name}\")\n",
    "\n",
    "        # Assuming 'GUF04_DLR_v1_Mosaic' is the layer with ID 102\n",
    "        layer = 'GUF04_DLR_v1_Mosaic'  # Replace with the actual layer name if different\n",
    "\n",
    "        # Step 3: Get the polygon boundary using osmnx\n",
    "        boundary_gdf = ox.geocode_to_gdf(place)\n",
    "        boundary = boundary_gdf.to_crs('EPSG:27700')\n",
    "        # buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "        boundary_buffered = boundary.buffer(100)\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "        boundary_polygon = boundary_gdf.geometry[0]\n",
    "        wms_boundary = boundary_buffered.geometry[0]\n",
    "\n",
    "        # Convert the polygon to a bounding box\n",
    "        minx, miny, maxx, maxy = wms_boundary.bounds\n",
    "\n",
    "        # Step 4: Request the data from WMS using the bounding box\n",
    "        width = 1024\n",
    "        height = 1024\n",
    "        response = wms.getmap(\n",
    "            layers=[layer],\n",
    "            srs='EPSG:4326',\n",
    "            bbox=(minx, miny, maxx, maxy),\n",
    "            size=(width, height),\n",
    "            format='image/geotiff'\n",
    "        )\n",
    "\n",
    "        # Step 5: Load the Raster Data into Rasterio\n",
    "        with MemoryFile(response.read()) as memfile:\n",
    "            with memfile.open() as src:\n",
    "                image = src.read(1)  # Read the first band\n",
    "                transform = src.transform\n",
    "                crs = src.crs\n",
    "\n",
    "                # Clip the raster data to the polygon\n",
    "                out_image, out_transform = rio_mask(src, [mapping(wms_boundary)], crop=True)  # Use renamed mask function\n",
    "                out_meta = src.meta.copy()\n",
    "                out_meta.update({\"driver\": \"GTiff\",\n",
    "                                \"height\": out_image.shape[1],\n",
    "                                \"width\": out_image.shape[2],\n",
    "                                \"transform\": out_transform,\n",
    "                                \"crs\": crs})\n",
    "\n",
    "        # Step 6: Convert Raster to Vector\n",
    "        mask_arr = (out_image[0] != 0).astype(np.uint8)  # Assuming non-zero values are urban areas\n",
    "\n",
    "        shapes_gen = shapes(mask_arr, mask=mask_arr, transform=out_transform)\n",
    "\n",
    "        polygons = []\n",
    "        for geom, value in shapes_gen:\n",
    "            polygons.append(shape(geom))\n",
    "\n",
    "        # Create a GeoDataFrame from the polygons\n",
    "        gdf = gpd.GeoDataFrame({'geometry': polygons}, crs=crs)\n",
    "\n",
    "        # Step 7: Create Buffers Around Urban Areas\n",
    "        buffer_distance = 100  # Buffer distance in meters (adjust as needed)\n",
    "        gdf_buffered = gdf.copy()\n",
    "        gdf_buffered['geometry'] = gdf['geometry'].buffer(buffer_distance)\n",
    "\n",
    "        # Step 8: Clip the GeoDataFrame to the boundary of the place\n",
    "        gdf_clipped = gpd.clip(gdf, boundary_gdf)\n",
    "\n",
    "        return gdf_clipped\n",
    "\n",
    "\n",
    "    guf = get_guf(place)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## get residential areas\n",
    "    def get_residential_areas(polygon):\n",
    "        polygon = polygon.to_crs('EPSG:4326')\n",
    "        # Retrieve features from OpenStreetMap\n",
    "        features = ox.features_from_polygon(polygon.iloc[0], tags={'landuse': 'residential'})\n",
    "        \n",
    "        # Convert features to a GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame.from_features(features)\n",
    "        gdf = gdf.set_crs('EPSG:4326')\n",
    "        \n",
    "        return gdf\n",
    "        \n",
    "    residential_areas = get_residential_areas(boundary_buffered)\n",
    "        \n",
    "        \n",
    "\n",
    "    ## join urban foot prints and residential areas\n",
    "    # this is to create a single polygon of where neighbourhoods can be found within\n",
    "\n",
    "    def join_geodataframes(gdf1, gdf2):\n",
    "        # Ensure both GeoDataFrames have the exact same CRS\n",
    "        target_crs = 'EPSG:4326'  # WGS 84\n",
    "        gdf1 = gdf1.to_crs(target_crs)\n",
    "        gdf2 = gdf2.to_crs(target_crs)\n",
    "        \n",
    "        # Concatenate GeoDataFrames\n",
    "        joined_gdf = pd.concat([gdf1, gdf2], ignore_index=True)\n",
    "        \n",
    "        return gpd.GeoDataFrame(joined_gdf, crs=target_crs)\n",
    "\n",
    "\n",
    "    guf_residential_gdf = join_geodataframes(guf, residential_areas)\n",
    "\n",
    "\n",
    "    ## create a small buffer to ensure all areas a captured correctly\n",
    "\n",
    "    def buffer_geometries_in_meters(gdf, distance):\n",
    "        # Define the World Mercator projected CRS\n",
    "        projected_crs = 'EPSG:3395'  # World Mercator\n",
    "\n",
    "        # Project to the new CRS\n",
    "        gdf_projected = gdf.to_crs(projected_crs)\n",
    "        \n",
    "        # Buffer the geometries\n",
    "        gdf_projected['geometry'] = gdf_projected['geometry'].buffer(distance)\n",
    "        \n",
    "        # Reproject back to the original CRS\n",
    "        gdf_buffered = gdf_projected.to_crs(gdf.crs)\n",
    "        \n",
    "        return gdf_buffered\n",
    "\n",
    "\n",
    "    guf_residential_gdf = buffer_geometries_in_meters(guf_residential_gdf, 100)  # Buffer by 100 meters\n",
    "\n",
    "\n",
    "    ## union into one gdf\n",
    "\n",
    "    def unary_union_polygons(gdf):\n",
    "        # Combine all geometries into a single geometry\n",
    "        unified_geometry = unary_union(gdf['geometry'])\n",
    "        \n",
    "        # Create a new GeoDataFrame with a single row containing the unified geometry\n",
    "        combined_gdf = gpd.GeoDataFrame({'geometry': [unified_geometry]}, crs=gdf.crs)\n",
    "        \n",
    "        return combined_gdf\n",
    "\n",
    "\n",
    "    guf_residential_gdf = unary_union_polygons(guf_residential_gdf)\n",
    "\n",
    "    # set to BNG\n",
    "    guf_residential_gdf = guf_residential_gdf.to_crs(\"27700\")\n",
    "\n",
    "    # Function to remove holes from neighbourhoods\n",
    "    def remove_holes(polygon):\n",
    "        if polygon.geom_type == 'Polygon':\n",
    "            return Polygon(polygon.exterior)\n",
    "        else:\n",
    "            return polygon\n",
    "\n",
    "    # remove holes from urban footprint\n",
    "    guf_residential_gdf['geometry'] = guf_residential_gdf['geometry'].apply(remove_holes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_rivers(boundary_buffered):\n",
    "        \"\"\"\n",
    "        Retrieves river features within a given boundary.\n",
    "\n",
    "        Args:\n",
    "            boundary_buffered (GeoDataFrame): A GeoDataFrame representing the buffered boundary.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: A GeoDataFrame containing the river features within the boundary.\n",
    "        \"\"\"\n",
    "        # Ensure the boundary is in the correct CRS for the query\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "        # Check the content of boundary_buffered to ensure it's not empty and correctly transformed\n",
    "        if boundary_buffered.empty:\n",
    "            raise ValueError(\"The provided boundary is empty.\")\n",
    "\n",
    "        # Define the tags for waterways\n",
    "        tags = {\"waterway\": [\"river\", \"rapids\"]}\n",
    "\n",
    "        try:\n",
    "            # Fetch features from OSM using the boundary geometry\n",
    "            polygon = boundary_buffered.geometry.iloc[0]\n",
    "            rivers = ox.features_from_polygon(polygon=polygon, tags=tags)\n",
    "\n",
    "            # Dropping rows where 'tunnel' is equal to 'culvert'\n",
    "            if 'tunnel' in rivers.columns:\n",
    "                rivers = rivers[rivers['tunnel'] != 'culvert']\n",
    "\n",
    "            # Convert the CRS back to the desired one\n",
    "            rivers = rivers.to_crs('EPSG:27700')\n",
    "\n",
    "            # Set the geometry column explicitly\n",
    "            rivers = rivers.set_geometry('geometry')\n",
    "\n",
    "            return rivers\n",
    "\n",
    "        except InsufficientResponseError:\n",
    "            print(\"No data elements found for the given boundary and tags.\")\n",
    "            empty_geometry = {'geometry': [LineString()]}\n",
    "            rivers = gpd.GeoDataFrame(empty_geometry, crs='EPSG:27700')\n",
    "            return rivers # Return an empty GeoDataFrame if no data found\n",
    "\n",
    "\n",
    "\n",
    "    def get_landuse(boundary_buffered):\n",
    "        \"\"\"\n",
    "        Retrieves the landuse features based on the specified boundary.\n",
    "\n",
    "        Args:\n",
    "            boundary_buffered (GeoDataFrame): The buffered boundary polygon.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: The landuse features.\n",
    "        \"\"\"\n",
    "        # reset boundary crs to allow for features to be found\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "        # Define tags\n",
    "        tags = {\"landuse\": [\"industrial\", \"railway\", \"brownfield\", \"commercial\", \"farmland\", \"meadow\"]}\n",
    "        # Use ox.features_from_polygon to find features matching the specified tags\n",
    "        landuse = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "        # set/reset crs\n",
    "        landuse = landuse.to_crs('27700')\n",
    "\n",
    "        ## get unsuitable \"nature\" types\n",
    "        # Define tags\n",
    "        tags = {\"natural\": [\"wood\", \"water\", \"scrub\", \"coastline\", \"beach\"]}\n",
    "        # Use ox.features_from_polygon to find features matching the specified tags\n",
    "        nature = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "        # set/reset crs\n",
    "        nature = nature.to_crs('27700')\n",
    "\n",
    "        ## get unsuitable \"lesiure\" types. This is mainly for golfcourses\n",
    "        # Define tags\n",
    "        tags = {\"leisure\": [\"golf_course\", \"track\", \"park\"]}\n",
    "        # Use ox.features_from_polygon to find features matching the specified tags\n",
    "        leisure = ox.features_from_polygon(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "        # set/reset crs\n",
    "        leisure = leisure.to_crs('27700')\n",
    "        # Define the tags for aeroway\n",
    "        aeroway_tags = {\"aeroway\": [\"aerodrome\"]}\n",
    "        # Use the function to retrieve aeroway features\n",
    "        aeroway = retrieve_osm_features(polygon=boundary_buffered.iloc[0], tags=aeroway_tags)\n",
    "        # Check if any features were retrieved\n",
    "        if aeroway is not None:\n",
    "            if not aeroway.empty:\n",
    "                # set/reset crs\n",
    "                aeroway = aeroway.to_crs('27700')\n",
    "\n",
    "        # concat\n",
    "        landuse = pd.concat([landuse, nature, leisure, aeroway])\n",
    "\n",
    "        ## resest boundary crs\n",
    "        boundary_buffered = boundary_buffered.to_crs('EPSG:27700')\n",
    "\n",
    "        return landuse\n",
    "\n",
    "\n",
    "    def get_bus_routes(boundary_buffered):\n",
    "            \"\"\"\n",
    "            Retrieves bus routes from OSM/NAPTAN within a given boundary.\n",
    "\n",
    "            Args:\n",
    "                    boundary_buffered (GeoDataFrame): A GeoDataFrame representing the boundary.\n",
    "\n",
    "            Returns:\n",
    "                    bus_routes (GeoDataFrame): A GeoDataFrame containing the bus routes.\n",
    "\n",
    "            Raises:\n",
    "                    Exception: If there is an error fetching the data from the Overpass API.\n",
    "            \"\"\"\n",
    "            # reset boundary crs to allow for features to be found\n",
    "            boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "            # Calculate the bounding box for XML query\n",
    "            bounding_box = boundary_buffered.bounds\n",
    "\n",
    "            # Extract the minimum and maximum coordinates\n",
    "            minx = bounding_box['minx'].min()\n",
    "            miny = bounding_box['miny'].min()\n",
    "            maxx = bounding_box['maxx'].max()\n",
    "            maxy = bounding_box['maxy'].max()\n",
    "\n",
    "            # Create a list of four elements representing the bounding box\n",
    "            bbox = [minx, miny, maxx, maxy]\n",
    "\n",
    "            # reset boundary_buffer crs\n",
    "            boundary_buffered = boundary_buffered.to_crs('27700')\n",
    "\n",
    "            # Define the Overpass API endpoint\n",
    "            overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "            # Define the XML query\n",
    "            xml_query = f\"\"\"\n",
    "            <osm-script output=\"json\" output-config=\"\" timeout=\"160\">\n",
    "                <union into=\"_\">\n",
    "                    <query into=\"_\" type=\"node\">\n",
    "                        <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                        <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                    </query>\n",
    "                    <query into=\"_\" type=\"way\">\n",
    "                        <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                        <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                    </query>\n",
    "                    <query into=\"_\" type=\"relation\">\n",
    "                        <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "                        <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "                    </query>\n",
    "                </union>\n",
    "                <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"body\" n=\"\" order=\"id\" s=\"\" w=\"\"/>\n",
    "                <recurse from=\"_\" into=\"_\" type=\"down\"/>\n",
    "                <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"skeleton\" n=\"\" order=\"quadtile\" s=\"\" w=\"\"/>\n",
    "            </osm-script>\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # Initialize lists to store data\n",
    "            geometries = []\n",
    "            element_data = []\n",
    "\n",
    "            # Make the Overpass API request\n",
    "            response = requests.post(overpass_url, data=xml_query)\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "\n",
    "                    # Access the data from the response\n",
    "                    for element in data.get(\"elements\", []):\n",
    "                            if element.get('type') == 'way' and 'geometry' in element:\n",
    "                                    # Extract geometry coordinates from 'geometry' field\n",
    "                                    coordinates = [(node['lon'], node['lat']) for node in element['geometry']]\n",
    "                                    # Create a LineString geometry\n",
    "                                    line = LineString(coordinates)\n",
    "                                    geometries.append(line)\n",
    "                                    element_data.append(element)\n",
    "\n",
    "                    # Create a GeoDataFrame\n",
    "                    bus_routes = gpd.GeoDataFrame(element_data, geometry=geometries)\n",
    "\n",
    "                    # Set CRS\n",
    "                    bus_routes = bus_routes.set_crs('4326')\n",
    "                    bus_routes = bus_routes.to_crs('27700')\n",
    "\n",
    "                    return bus_routes\n",
    "\n",
    "            else:\n",
    "                    raise Exception(f\"Error fetching data: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "\n",
    "    def clip_boundaries(os_open_roads, rivers, railways, landuse, bus_routes, boundary_buffered):\n",
    "        \"\"\"\n",
    "        Clips the geospatial data to the boundary_buffered extent.\n",
    "\n",
    "        Parameters:\n",
    "        - os_open_roads (GeoDataFrame): lines from OS Open roads.\n",
    "        - rivers (GeoDataFrame): lines of Rivers.\n",
    "        - railways (GeoDataFrame): lines of Railways.\n",
    "        - landuse (GeoDataFrame): Land use polygons.\n",
    "        - bus_routes (GeoDataFrame): lines of bus routes.\n",
    "        - boundary_buffered (GeoDataFrame): buffered boundary.\n",
    "\n",
    "        Returns:\n",
    "        - clipped versions of input geodataframes, aside from the bufferd boundary.\n",
    "        \"\"\"\n",
    "        os_open_roads_clip = gpd.clip(os_open_roads, boundary_buffered)\n",
    "        rivers_clip = gpd.clip(rivers, boundary_buffered)\n",
    "        railways_clip = gpd.clip(railways, boundary_buffered)\n",
    "        landuse_clip = gpd.clip(landuse, boundary_buffered)\n",
    "        bus_routes_clip = gpd.clip(bus_routes, boundary_buffered)\n",
    "\n",
    "        return os_open_roads_clip, rivers_clip, railways_clip, landuse_clip, bus_routes_clip\n",
    "\n",
    "\n",
    "    def process_bus_routes(bus_routes_clip, buffer_distance):\n",
    "        \"\"\"\n",
    "        Count the number of bus routes per road and remove roads with more than one bus route on them.\n",
    "        \n",
    "        Args:\n",
    "            bus_routes_clip (GeoDataFrame): The input GeoDataFrame containing bus routes.\n",
    "            buffer_distance (float): The buffer distance to convert roads to polygons, set in meters.\n",
    "        \n",
    "        Returns:\n",
    "            GeoDataFrame: The filtered GeoDataFrame containing roads with less than or equal to one bus route.\n",
    "        \"\"\"\n",
    "        # Create a new GeoDataFrame with the buffered geometries\n",
    "        bus_routes_buffered = bus_routes_clip.copy()  # Copy the original GeoDataFrame\n",
    "        bus_routes_buffered['geometry'] = bus_routes_buffered['geometry'].buffer(buffer_distance)\n",
    "\n",
    "        # count the number of overlapping bus routes\n",
    "        def count_overlapping_features(gdf):\n",
    "            \"\"\"\n",
    "            Count the number of overlapping features in a GeoDataFrame.\n",
    "            \n",
    "            Args:\n",
    "                gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "            \n",
    "            Returns:\n",
    "                GeoDataFrame: The input GeoDataFrame with an additional column 'Bus_routes_count' indicating the count of overlapping features.\n",
    "            \"\"\"\n",
    "            # Create an empty column to store the count of overlapping features\n",
    "            gdf['Bus_routes_count'] = 0\n",
    "\n",
    "            # Iterate through each row in the GeoDataFrame\n",
    "            for idx, row in gdf.iterrows():\n",
    "                # Get the geometry of the current row\n",
    "                geometry = row['geometry']\n",
    "                \n",
    "                # Use a spatial filter to find overlapping features\n",
    "                overlaps = gdf[gdf['geometry'].intersects(geometry)]\n",
    "                \n",
    "                # Update the Bus_routes_count column with the count of overlapping features\n",
    "                gdf.at[idx, 'Bus_routes_count'] = len(overlaps)\n",
    "            \n",
    "            return gdf\n",
    "\n",
    "        # call function\n",
    "        bus_routes_buffered_with_count = count_overlapping_features(bus_routes_buffered)\n",
    "\n",
    "        # drop any roads which have less than two bus routes on them\n",
    "        bus_routes_filtered = bus_routes_buffered_with_count[bus_routes_buffered_with_count['Bus_routes_count'] >= 2]\n",
    "        \n",
    "        return bus_routes_filtered\n",
    "\n",
    "\n",
    "\n",
    "    def filter_OS_boundary_roads(os_open_roads_clip):\n",
    "        \"\"\"\n",
    "        Filter the `os_open_roads_clip` DataFrame to select boundary roads.\n",
    "\n",
    "        This function filters the `os_open_roads_clip` DataFrame to select roads that are considered \"boundary\" roads. \n",
    "        The selection criteria include roads that have the following attributes:\n",
    "        - `primary_route` is True\n",
    "        - `trunk_road` is True\n",
    "        - `fictitious` is True\n",
    "        - `road_classification` is 'A Road' or 'B Road'\n",
    "        - `road_function` is 'Minor Road' or 'Motorway'\n",
    "\n",
    "        The filtered DataFrame is returned.\n",
    "\n",
    "        Note: The commented line `(os_open_roads_clip['road_function'] == 'Restricted Local Access Road')` is excluded from the selection.\n",
    "\n",
    "        Parameters:\n",
    "        - os_open_roads_clip (DataFrame): A DataFrame containing road data.\n",
    "\n",
    "        Returns:\n",
    "        - boundary_roads (DataFrame): A DataFrame containing the filtered boundary roads.\n",
    "\n",
    "        Example usage:\n",
    "            # Assuming `os_open_roads_clip` is a DataFrame containing road data\n",
    "            boundary_roads = filter_boundary_roads(os_open_roads_clip)\n",
    "        \"\"\"\n",
    "        boundary_roads = os_open_roads_clip.loc[((os_open_roads_clip['primary_route'] == 'True') |\n",
    "                        (os_open_roads_clip['trunk_road'] == 'True') |\n",
    "                        (os_open_roads_clip['fictitious'] == 'True') |\n",
    "                        (os_open_roads_clip['road_classification'] == 'A Road') | \n",
    "                        (os_open_roads_clip['road_classification'] == 'B Road') | \n",
    "                        (os_open_roads_clip['road_function'] == 'Minor Road') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Motorway') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Minor Road')  \n",
    "                        )]\n",
    "        return boundary_roads\n",
    "\n",
    "\n",
    "\n",
    "    ## buffering and dissolving functions\n",
    "    \n",
    "    def buffer_and_dissolve(input_gdf):\n",
    "        \"\"\"\n",
    "        Buffer and dissolve a GeoDataFrame.\n",
    "        \n",
    "        Args:\n",
    "            input_gdf (GeoDataFrame): The input GeoDataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            GeoDataFrame: The buffered and dissolved GeoDataFrame.\n",
    "        \"\"\"\n",
    "        # Buffer around boundaries\n",
    "        buffered_gdf = input_gdf.copy()  # Create a copy to avoid modifying the original\n",
    "        buffered_gdf['geometry'] = buffered_gdf['geometry'].buffer(5) # set a 5 meter buffer\n",
    "\n",
    "        # Dissolve the geometries\n",
    "        dissolved_geo = buffered_gdf.unary_union\n",
    "\n",
    "        # Create a new GeoDataFrame with the dissolved geometry\n",
    "        dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "\n",
    "        # Set the CRS (Coordinate Reference System)\n",
    "        dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "        return dissolved_gdf\n",
    "\n",
    "\n",
    "    def dissolve_gdf(input_gdf):\n",
    "        # dissolve geometries\n",
    "        dissolved_geo = input_gdf.unary_union\n",
    "        dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "        dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "        return dissolved_gdf\n",
    "\n",
    "\n",
    "    def erase_boundary_features(boundary, boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd, guf_residential_gdf):\n",
    "        \"\"\"\n",
    "        Erases boundary features from the given boundary geometry.\n",
    "\n",
    "        Parameters:\n",
    "        - boundary: GeoDataFrame representing the boundary geometry\n",
    "        - boundary_rivers_bd: GeoDataFrame representing the rivers boundary features\n",
    "        - boundary_roads_bd: GeoDataFrame representing the roads boundary features\n",
    "        - boundary_rail_bd: GeoDataFrame representing the rail boundary features\n",
    "        - boundary_landuse_bd: GeoDataFrame representing the landuse boundary features\n",
    "        - boundary_bus_routes_bd: GeoDataFrame representing the bus routes boundary features\n",
    "\n",
    "        Returns:\n",
    "        - erased_boundary_gdf: GeoDataFrame containing the result of the \"Erase\" operation\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure that neighbourhoods fall only within urban footprint areas\n",
    "        boundary = gpd.clip(boundary, guf_residential_gdf)\n",
    "\n",
    "        # Join all boundary features\n",
    "        boundaries = pd.concat([boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd], ignore_index=True)\n",
    "        boundary_features = dissolve_gdf(boundaries)\n",
    "\n",
    "        # Use the `difference` method to perform the \"Erase\" operation\n",
    "        erased_boundary = boundary.difference(boundary_features.unary_union)\n",
    "\n",
    "        # Convert the GeoSeries to a single geometry using unary_union\n",
    "        erased_boundary = erased_boundary.unary_union\n",
    "\n",
    "        # Create a new GeoDataFrame with the result of \"Erase\" operation\n",
    "        erased_boundary_gdf = gpd.GeoDataFrame(geometry=[erased_boundary], crs=boundary.crs)\n",
    "\n",
    "        # Explode multipolygon to polygons\n",
    "        erased_boundary_gdf = erased_boundary_gdf.explode()\n",
    "\n",
    "        return erased_boundary_gdf\n",
    "\n",
    "\n",
    "    def drop_large_or_small_areas(neighbourhoods):\n",
    "        \"\"\"\n",
    "        Drops rows from the 'neighbourhoods' DataFrame where the area is less than 10,000 square units or greater than 5,000,000 square units.\n",
    "\n",
    "        Parameters:\n",
    "        - neighbourhoods (DataFrame): The input DataFrame containing neighbourhood data.\n",
    "\n",
    "        Returns:\n",
    "        - neighbourhoods (DataFrame): The updated DataFrame with small areas dropped.\n",
    "        \"\"\"\n",
    "        # Calculate area\n",
    "        neighbourhoods[\"area\"] = neighbourhoods.geometry.area\n",
    "\n",
    "        # Drop rows where area is less than 10,000 or greater than 5,000,000\n",
    "        neighbourhoods = neighbourhoods.loc[(neighbourhoods[\"area\"] >= 10000)]\n",
    "        neighbourhoods = neighbourhoods.loc[(neighbourhoods[\"area\"] <= 5000000)]\n",
    "\n",
    "        return neighbourhoods\n",
    "\n",
    "\n",
    "    def filter_neighbourhoods_by_roads(neighbourhoods, os_open_roads_clip, polygon_column_name):\n",
    "        \"\"\"\n",
    "        Count the number of roads within each polygon in a GeoDataFrame and filter the neighbourhoods based on road count and road density.\n",
    "        \n",
    "        Args:\n",
    "            neighbourhoods (GeoDataFrame): GeoDataFrame containing neighbourhood polygons.\n",
    "            os_open_roads_clip (GeoDataFrame): GeoDataFrame containing road data.\n",
    "            polygon_column_name (str): Name of the column in neighbourhoods to use for grouping.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame: Updated neighbourhoods GeoDataFrame with filtered rows based on road count and road density.\n",
    "        \"\"\"\n",
    "        \n",
    "        def count_roads_within_polygons(polygons_gdf, roads_gdf, polygon_column_name):\n",
    "            \"\"\"\n",
    "            Count the number of roads within each polygon in a GeoDataFrame.\n",
    "            \n",
    "            Args:\n",
    "                polygons_gdf (GeoDataFrame): GeoDataFrame containing polygons.\n",
    "                roads_gdf (GeoDataFrame): GeoDataFrame containing roads.\n",
    "                polygon_column_name (str): Name of the column in polygons_gdf to use for grouping.\n",
    "\n",
    "            Returns:\n",
    "                GeoDataFrame: Original polygons GeoDataFrame with a \"road_count\" column added.\n",
    "            \"\"\"\n",
    "            \n",
    "            # spatial join\n",
    "            joined = gpd.sjoin(polygons_gdf, roads_gdf, how='left', op='intersects')\n",
    "            \n",
    "            # Group by the polygon column and count the number of roads in each\n",
    "            road_counts = joined.groupby(polygon_column_name).size().reset_index(name='road_count')\n",
    "            \n",
    "            # Merge the road counts back into the polygons GeoDataFrame\n",
    "            polygons_gdf = polygons_gdf.merge(road_counts, on=polygon_column_name, how='left')\n",
    "\n",
    "            # Calculate road density (area divided by road_count). It is multiplied by 10000 for ease of understanding the numbers involved with this\n",
    "            polygons_gdf['road_density'] = (polygons_gdf['road_count'] / polygons_gdf['area'] ) * 10000\n",
    "            \n",
    "            return polygons_gdf\n",
    "        \n",
    "        neighbourhoods = count_roads_within_polygons(neighbourhoods, os_open_roads_clip, polygon_column_name)\n",
    "\n",
    "        # Drop rows with road_density below 0.2 or less than 4 roads\n",
    "        neighbourhoods = neighbourhoods[(neighbourhoods['road_count'] > 2)]\n",
    "        neighbourhoods = neighbourhoods[(neighbourhoods['road_density'] > 0.2)]\n",
    "        \n",
    "        return neighbourhoods\n",
    "\n",
    "\n",
    "    def remove_holes(polygon):\n",
    "        \"\"\"\n",
    "        Removes holes from a polygon. Mostly for visual reasons.\n",
    "\n",
    "        Parameters:\n",
    "        polygon (Polygon): The polygon to remove holes from.\n",
    "\n",
    "        Returns:\n",
    "        Polygon: The polygon without holes.\n",
    "        \"\"\"\n",
    "        if polygon.geom_type == 'Polygon':\n",
    "            return Polygon(polygon.exterior)\n",
    "        else:\n",
    "            return polygon\n",
    "\n",
    "    landuse = get_landuse(boundary_buffered)\n",
    "    rivers = get_rivers(boundary_buffered)\n",
    "    railways = get_railways(place)\n",
    "    landuse = get_landuse(boundary_buffered)\n",
    "    bus_routes = get_bus_routes(boundary_buffered)\n",
    "    os_open_roads_clip, rivers_clip, railways_clip, landuse_clip, bus_routes_clip = clip_boundaries(os_open_roads, rivers, railways, landuse, bus_routes, boundary_buffered)\n",
    "    bus_routes_clip = process_bus_routes(bus_routes_clip, 0.2)\n",
    "    boundary_roads = filter_OS_boundary_roads(os_open_roads_clip)\n",
    "\n",
    "    ## buffer and dissolve \n",
    "    boundary_roads_bd = buffer_and_dissolve(boundary_roads)\n",
    "    boundary_rivers_bd = buffer_and_dissolve(rivers_clip)\n",
    "    boundary_rail_bd = buffer_and_dissolve(railways_clip)\n",
    "    boundary_landuse_bd = buffer_and_dissolve(landuse_clip)\n",
    "    boundary_bus_routes_bd = buffer_and_dissolve(bus_routes_clip)\n",
    "\n",
    "    ## geodataframe cleaning\n",
    "    erased_boundary_gdf = erase_boundary_features(boundary, boundary_rivers_bd, boundary_roads_bd, boundary_rail_bd, boundary_landuse_bd, boundary_bus_routes_bd, guf_residential_gdf)\n",
    "    neighbourhoods = erased_boundary_gdf\n",
    "    neighbourhoods = drop_large_or_small_areas(neighbourhoods)\n",
    "\n",
    "    neighbourhoods = filter_neighbourhoods_by_roads(neighbourhoods, os_open_roads_clip, 'geometry')\n",
    "\n",
    "    ## create unique IDs\n",
    "    # simple number based ID\n",
    "    neighbourhoods['ID'] = range(1, len(neighbourhoods) + 1)\n",
    "\n",
    "    neighbourhoods['geometry'] = neighbourhoods['geometry'].apply(remove_holes)\n",
    "\n",
    "\n",
    "    ## filter neighbourhoods to only locations with more than 1 intersection (1 or fewer intersections indicates that all travel modes will be the same)\n",
    "    # reset neighbourhoods crs\n",
    "    neighbourhoods = neighbourhoods.to_crs('4326')\n",
    "\n",
    "    # Spatial join to count points within each neighborhood\n",
    "    spatial_join = gpd.sjoin(neighbourhoods, common_nodes_gdf, how='left', op='contains')\n",
    "\n",
    "    # Group by 'ID' and count the points within each neighborhood\n",
    "    point_counts = spatial_join.groupby('ID').size().reset_index(name='point_count')\n",
    "\n",
    "    # Filter out neighborhoods with 1 or 0 points\n",
    "    filtered_neighbourhood_ids = point_counts[point_counts['point_count'] > 1]['ID']\n",
    "\n",
    "    neighbourhoods= neighbourhoods[neighbourhoods['ID'].isin(filtered_neighbourhood_ids)]\n",
    "\n",
    "\n",
    "\n",
    "    ## we also need to join the length of the streets within the neighbourhood for further analysis\n",
    "    # Reset index of neighbourhoods\n",
    "    neighbourhoods = neighbourhoods.reset_index(drop=True)\n",
    "\n",
    "    # reset neighbourhoods crs\n",
    "    neighbourhoods = neighbourhoods.to_crs('27700')\n",
    "\n",
    "    # Perform a spatial join\n",
    "    joined_data = gpd.sjoin(os_open_roads_clip, neighbourhoods, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "    # Group by neighborhood and calculate total road length\n",
    "    road_lengths = joined_data.groupby('index_right')['length'].sum().reset_index()\n",
    "\n",
    "    # Merge road_lengths with neighbourhoods and drop 'index_right' column\n",
    "    neighbourhoods = neighbourhoods.merge(road_lengths, left_index=True, right_on='index_right', how='left').drop(columns=['index_right'])\n",
    "\n",
    "    # Rename the column\n",
    "    neighbourhoods.rename(columns={'length': 'road_lengths'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    ### find accessiablity\n",
    "\n",
    "    ## all to all\n",
    "    def calculate_distance_stats_from_points(points_gdf, network):\n",
    "        all_pairs_shortest_paths = {}\n",
    "        points_osmids = points_gdf.index.tolist()  # Assuming the 'osmid' is the index in the GeoDataFrame\n",
    "\n",
    "        for start_node in points_osmids:\n",
    "            shortest_paths = {}\n",
    "            try:\n",
    "                for end_node in points_osmids:\n",
    "                    if start_node != end_node:\n",
    "                        distance = nx.shortest_path_length(network, start_node, end_node, weight='length')\n",
    "                        shortest_paths[end_node] = distance\n",
    "                all_pairs_shortest_paths[start_node] = shortest_paths\n",
    "            except nx.NetworkXNoPath:\n",
    "                # If no path is found, skip adding to all_pairs_shortest_paths\n",
    "                continue\n",
    "\n",
    "        distances = [length for paths in all_pairs_shortest_paths.values() for length in paths.values()]\n",
    "\n",
    "        if not distances:\n",
    "            return {\n",
    "                \"mean_distance\": 0,\n",
    "                \"median_distance\": 0,\n",
    "                \"min_distance\": 0,\n",
    "                \"max_distance\": 0,\n",
    "                \"distance_range\": 0,\n",
    "                \"total_distance\": 0\n",
    "            }\n",
    "\n",
    "        mean_distance = statistics.mean(distances)\n",
    "        median_distance = statistics.median(distances)\n",
    "        min_distance = min(distances)\n",
    "        max_distance = max(distances)\n",
    "        distance_range = max_distance - min_distance\n",
    "        total_distance = sum(distances)\n",
    "\n",
    "        return {\n",
    "            \"mean_distance\": mean_distance,\n",
    "            \"median_distance\": median_distance,\n",
    "            \"min_distance\": min_distance,\n",
    "            \"max_distance\": max_distance,\n",
    "            \"distance_range\": distance_range,\n",
    "            \"total_distance\": total_distance\n",
    "        }\n",
    "\n",
    "    ## processing for all to all \n",
    "    results = []\n",
    "\n",
    "    for index, row in neighbourhoods.iterrows():\n",
    "        neighbourhood = neighbourhoods.loc[[index]]\n",
    "\n",
    "        ## get neighbourhood boundary and neighbourhood boundary buffer\n",
    "        # set crs\n",
    "        neighbourhood = neighbourhood.to_crs('3395')\n",
    "        # create a buffer neighbourhood\n",
    "        neighbourhood_buffer = neighbourhood['geometry'].buffer(15)\n",
    "        # convert back to a geodataframe (for later on)\n",
    "        neighbourhood_buffer = gpd.GeoDataFrame(geometry=neighbourhood_buffer)\n",
    "        # reset crs\n",
    "        neighbourhood, neighbourhood_buffer = neighbourhood.to_crs('4326'), neighbourhood_buffer.to_crs('4326')\n",
    "\n",
    "        ## get nodes which can be driven to and walked to within area\n",
    "        neighbourhood_nodes = gpd.clip(common_nodes_gdf, neighbourhood_buffer)\n",
    "\n",
    "        if neighbourhood_nodes.empty:\n",
    "            print(f\"No nodes found for neighbourhood {index}. Using default values.\")\n",
    "            walk_stats = {\n",
    "                \"mean_distance\": 0,\n",
    "                \"median_distance\": 0,\n",
    "                \"min_distance\": 0,\n",
    "                \"max_distance\": 0,\n",
    "                \"distance_range\": 0,\n",
    "                \"total_distance\": 0\n",
    "            }\n",
    "            drive_stats = walk_stats\n",
    "        else:\n",
    "            ## calculate neighbourhood distance stats for walking and driving\n",
    "            walk_stats = calculate_distance_stats_from_points(neighbourhood_nodes, walk_streets)\n",
    "            drive_stats = calculate_distance_stats_from_points(neighbourhood_nodes, drive_streets)\n",
    "\n",
    "        ## get length of total edges within the neighbourhood\n",
    "        edges_within_neighbourhood = gpd.sjoin(all_edges, neighbourhood, how=\"inner\", op=\"intersects\")\n",
    "        total_length = edges_within_neighbourhood['length'].sum()\n",
    "\n",
    "        ## Add the statistics to the GeoDataFrame\n",
    "        neighbourhood['walk_mean_distance'] = walk_stats['mean_distance']\n",
    "        neighbourhood['walk_median_distance'] = walk_stats['median_distance']\n",
    "        neighbourhood['walk_min_distance'] = walk_stats['min_distance']\n",
    "        neighbourhood['walk_max_distance'] = walk_stats['max_distance']\n",
    "        neighbourhood['walk_distance_range'] = walk_stats['distance_range']\n",
    "        neighbourhood['walk_total_distance'] = walk_stats['total_distance']\n",
    "\n",
    "        neighbourhood['drive_mean_distance'] = drive_stats['mean_distance']\n",
    "        neighbourhood['drive_median_distance'] = drive_stats['median_distance']\n",
    "        neighbourhood['drive_min_distance'] = drive_stats['min_distance']\n",
    "        neighbourhood['drive_max_distance'] = drive_stats['max_distance']\n",
    "        neighbourhood['drive_distance_range'] = drive_stats['distance_range']\n",
    "        neighbourhood['drive_total_distance'] = drive_stats['total_distance']\n",
    "\n",
    "        ## Store statistics along with neighborhood ID or other identifying information\n",
    "        result = {\n",
    "            'neighbourhood_id': neighbourhood['ID'].iloc[0],  # Assuming you have an ID column\n",
    "            'walk_mean_distance': walk_stats['mean_distance'],\n",
    "            'walk_median_distance': walk_stats['median_distance'],\n",
    "            'walk_total_distance': walk_stats['total_distance'],\n",
    "            \n",
    "            'drive_mean_distance': drive_stats['mean_distance'],\n",
    "            'drive_median_distance': drive_stats['median_distance'],\n",
    "            'drive_total_distance': drive_stats['total_distance'],\n",
    "\n",
    "            'total_edge_length': total_length\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    ## Convert the results to a new dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    ## calculate differences\n",
    "    results_df['mean_distance_diff'] = results_df['walk_mean_distance'] - results_df['drive_mean_distance']\n",
    "    results_df['median_distance_diff'] = results_df['walk_median_distance'] - results_df['drive_median_distance']\n",
    "    results_df['total_distance_diff'] = results_df['walk_total_distance'] - results_df['drive_total_distance']\n",
    "\n",
    "    merged_df = pd.merge(neighbourhoods, results_df, left_on=\"ID\", right_on=\"neighbourhood_id\")\n",
    "    access_results_gdf = gpd.GeoDataFrame(merged_df, geometry='geometry')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_barriers(boundary):\n",
    "        \"\"\"\n",
    "        Find modal filters within a given boundary.\n",
    "\n",
    "        Args:\n",
    "            boundary (geopandas.GeoDataFrame): A GeoDataFrame representing the boundary.\n",
    "\n",
    "        Returns:\n",
    "            barriers (geopandas.GeoDataFrame): A GeoDataFrame containing the modal filters.\n",
    "            streets_gdf (geopandas.GeoDataFrame): A GeoDataFrame containing the streets from OSM.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # get the boundary in the correct CRS for OSMnx\n",
    "        boundary_4326 = boundary.to_crs('EPSG:4326')\n",
    "\n",
    "        # get the most \"basic\" filters mapped, the barriers/bollards etc\n",
    "        # Define tags\n",
    "        tags = {\"barrier\": [\"bollard\", \"bus_trap\", \"entrance\", \"planter\", \"sump_buster\", \"wedge\"]}\n",
    "\n",
    "        # Use ox.features_from_polygon to find features matching the specified tags\n",
    "        barriers = ox.features_from_polygon(polygon=boundary_4326.geometry.iloc[0], tags=tags)\n",
    "\n",
    "        # process any linestrings into point geometries\n",
    "        # Filter the GeoDataFrame to select only rows with \"linestring\" geometry\n",
    "        barriers_linestrings = barriers[barriers['geometry'].geom_type == 'LineString']\n",
    "\n",
    "        # Create an empty GeoDataFrame to store the individual points\n",
    "        points_gdf = gpd.GeoDataFrame(columns=list(barriers_linestrings.columns), crs=barriers_linestrings.crs)\n",
    "\n",
    "        # Iterate through each row in the GeoDataFrame with linestrings\n",
    "        for idx, row in barriers_linestrings.iterrows():\n",
    "            if isinstance(row['geometry'], LineString):\n",
    "                # Extract the individual points from the linestring\n",
    "                points = [Point(coord) for coord in list(row['geometry'].coords)]\n",
    "\n",
    "                # Create a GeoDataFrame from the individual points and copy the attributes\n",
    "                points_df = gpd.GeoDataFrame(geometry=points, crs=barriers_linestrings.crs)\n",
    "                for col in barriers_linestrings.columns:\n",
    "                    if col != 'geometry':\n",
    "                        points_df[col] = row[col]\n",
    "\n",
    "                # Rename the \"geometry\" column to \"merged_geometry\"\n",
    "                points_df = points_df.rename(columns={'geometry': 'merged_geometry'})\n",
    "\n",
    "                # Append the points to the points_gdf\n",
    "                points_gdf = pd.concat([points_gdf, points_df], ignore_index=True)\n",
    "\n",
    "        # Now, points_gdf contains all the individual points from the linestrings with inherited attributes\n",
    "\n",
    "        # Remove the \"geometry\" column from the points GeoDataFrame\n",
    "        points_gdf = points_gdf.drop(columns=['geometry'])\n",
    "\n",
    "        # Remove the linestring rows from the original GeoDataFrame\n",
    "        barriers = barriers[barriers['geometry'].geom_type != 'LineString']\n",
    "\n",
    "        # Rename the \"merged_geometry\" column to \"geometry\" in the points GeoDataFrame\n",
    "        points_gdf = points_gdf.rename(columns={'merged_geometry': 'geometry'})\n",
    "\n",
    "        # Concatenate the individual points GeoDataFrame to the original GeoDataFrame\n",
    "        barriers = pd.concat([barriers, points_gdf], ignore_index=True)\n",
    "\n",
    "        # Reset the index to ensure it is continuous\n",
    "        barriers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Create a new column \"previously_linestring\" and set it to False initially\n",
    "        barriers['previously_linestring'] = False\n",
    "\n",
    "        # Iterate through each row in the GeoDataFrame with linestrings\n",
    "        for idx, row in barriers_linestrings.iterrows():\n",
    "            if isinstance(row['geometry'], LineString):\n",
    "                # Extract the individual points from the linestring\n",
    "                points = [Point(coord) for coord in list(row['geometry'].coords)]\n",
    "\n",
    "                # Iterate through the points in the linestring\n",
    "                for point in points:\n",
    "                    # Check if the point's geometry intersects with any of the original linestrings\n",
    "                    mask = barriers['geometry'].intersects(point)\n",
    "                    if mask.any():\n",
    "                        # If the point intersects with any linestring, set \"previously_linestring\" to True\n",
    "                        barriers.loc[mask, 'previously_linestring'] = True\n",
    "\n",
    "        # add a unique ID\n",
    "        barriers['barrier_id'] = range(1, len(barriers) + 1)\n",
    "\n",
    "        # Convert the OSMnx graph to a GeoDataFrame of streets\n",
    "        streets_gdf = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "\n",
    "        # join the barriers to the streets\n",
    "        streets_gdf = gpd.sjoin(streets_gdf, barriers, how=\"left\", op=\"intersects\")\n",
    "\n",
    "        # clean geodataframe and drop streets without a barrier\n",
    "        streets_gdf.columns = streets_gdf.columns.str.replace(\"_right\", \"_barrier\").str.replace(\"_left\", \"_street\")\n",
    "        # we need to double check the name of \"barrier\"\n",
    "        streets_gdf['barrier_barrier'] = streets_gdf['barrier'] if 'barrier' in streets_gdf.columns else streets_gdf[\n",
    "            'barrier_barrier']\n",
    "\n",
    "        if 'name_street' in streets_gdf.columns:\n",
    "            streets_gdf = streets_gdf.rename(columns={'name_street': 'name'})\n",
    "        barrier_streets = streets_gdf.dropna(subset=['barrier_barrier'])\n",
    "\n",
    "        # add barrier tag\n",
    "        barrier_streets['filter_type'] = 'barrier or bollard'\n",
    "\n",
    "        ## extract points which are on/within 1m of streets only\n",
    "        streets_gdf['has_barrier'] = 'yes'\n",
    "\n",
    "        # reset crs before spatial join\n",
    "        barriers, streets_gdf = barriers.to_crs(3857), streets_gdf.to_crs(3857)\n",
    "\n",
    "        barriers = gpd.sjoin_nearest(barriers, streets_gdf, how=\"left\", max_distance=1)\n",
    "        barriers = barriers.dropna(subset=['has_barrier'])\n",
    "        barriers = barriers.reset_index(drop=True)  # Reset the index\n",
    "\n",
    "        # Dissolve based on the 'geometry' column\n",
    "        barriers = barriers.dissolve(by='barrier_id_right')\n",
    "\n",
    "        # add barrier tag\n",
    "        barriers['filter_type'] = 'barrier or bollard'\n",
    "\n",
    "        # Reset the index to remove multi-index\n",
    "        barriers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return barriers, streets_gdf\n",
    "\n",
    "\n",
    "        \n",
    "    def get_bus_gates(streets_gdf):\n",
    "        \"\"\"\n",
    "        Finds all the bus gates within the given streets GeoDataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        streets_gdf (GeoDataFrame): A GeoDataFrame containing street data.\n",
    "\n",
    "        Returns:\n",
    "        busgates (GeoDataFrame): A GeoDataFrame containing the bus gates found in the streets data.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if required columns are present, if not add them with default values\n",
    "        for col in ['access', 'bicycle', 'bus', 'motor_vehicle']:\n",
    "            if col not in streets_gdf.columns:\n",
    "                streets_gdf[col] = None\n",
    "\n",
    "        streets_gdf['access_street'] = streets_gdf['access'] if 'access' in streets_gdf.columns else streets_gdf['access_street']\n",
    "        streets_gdf['bicycle_street'] = streets_gdf['bicycle'] if 'bicycle' in streets_gdf.columns else streets_gdf['bicycle_street']\n",
    "        streets_gdf['bus'] = streets_gdf['bus_street'] if 'bus_street' in streets_gdf.columns else streets_gdf['bus']\n",
    "\n",
    "        if 'bus' in streets_gdf.columns:\n",
    "            busgates = streets_gdf[((streets_gdf[\"bus\"] == \"yes\") & (streets_gdf[\"access_street\"] == \"no\") & (streets_gdf[\"bicycle_street\"] == \"yes\")) |\n",
    "                                (streets_gdf[\"bus\"] == \"yes\") & (streets_gdf[\"motor_vehicle_street\"] == \"no\") & (streets_gdf[\"bicycle_street\"] == \"yes\")]\n",
    "            # add bus gate tag\n",
    "            busgates['filter_type'] = 'bus gate'\n",
    "        else:\n",
    "            print(\"Warning: 'bus' column not found in streets_gdf.\")\n",
    "            busgates = gpd.GeoDataFrame(columns=streets_gdf.columns, crs=streets_gdf.crs)\n",
    "\n",
    "        return busgates, streets_gdf\n",
    "\n",
    "        # add bus gate tag\n",
    "        busgates['filter_type'] = 'bus gate'\n",
    "\n",
    "        return busgates, streets_gdf\n",
    "\n",
    "    def get_contraflows(streets_gdf):\n",
    "        \"\"\"\n",
    "        Finds the unrestricted one-way streets for cycling but restricted for cars.\n",
    "\n",
    "        Parameters:\n",
    "        streets_gdf (GeoDataFrame): A GeoDataFrame containing street data.\n",
    "\n",
    "        Returns:\n",
    "        GeoDataFrame: A GeoDataFrame containing the unrestricted one-way streets for cycling.\n",
    "        \"\"\"\n",
    "        ## one-way streets also can act as modal filters. lets find where cycling is unrestricted but cars are\n",
    "        if 'oneway:bicycle' in streets_gdf.columns:\n",
    "            oneways = streets_gdf[(streets_gdf[\"oneway\"] == True) & (streets_gdf[\"oneway:bicycle\"] == \"no\")]\n",
    "\n",
    "            # Convert values in the \"name\" column to strings if they are not already\n",
    "            oneways['name'] = oneways['name'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "            # Perform dissolve \n",
    "            oneways = oneways.dissolve(by='name')\n",
    "\n",
    "            # Reset the index \n",
    "            oneways = oneways.reset_index()\n",
    "\n",
    "            # add one way tag\n",
    "            oneways['filter_type'] = 'one-way bike'\n",
    "        else:\n",
    "            print(\"Warning: 'oneway:bicycle' column not found in streets_gdf.\")\n",
    "            oneways = gpd.GeoDataFrame(columns=streets_gdf.columns, crs=streets_gdf.crs)\n",
    "\n",
    "        return oneways\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def filter_streets_continuations(input_gdf):\n",
    "        ## clean dataframe\n",
    "        # Check if 'highway_street' column exists and rename it to 'highway'\n",
    "        if 'highway_street' in input_gdf.columns:\n",
    "            input_gdf.rename(columns={'highway_street': 'highway'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # filter dataframe \n",
    "        ## remove indoor roads, these are likey pedestrian only however often don't have any \"cycling\" related tag\n",
    "        if 'covered' in input_gdf.columns:\n",
    "            input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'covered' in str(x))]\n",
    "            input_gdf = input_gdf[input_gdf['covered'] != 'yes']\n",
    "        ## also remove footways and steps, as these are almost pedestrain only, never cyclable\n",
    "        input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'footway' in str(x))]\n",
    "        input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'steps' in str(x))]\n",
    "\n",
    "\n",
    "\n",
    "        ## clean dataframe\n",
    "        input_gdf['name'] = input_gdf['name'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "        input_gdf['highway'] = input_gdf['highway'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## perform street continunation filtering\n",
    "        # Grouping by 'name' and checking for groups with 'pedestrian' and another highway type\n",
    "        grouped = input_gdf.groupby('name').filter(lambda x: any('pedestrian' in val for val in x['highway']) and len(x['highway'].unique()) > 1)\n",
    "        street_continuations_gdf = grouped[grouped['highway'].str.contains('pedestrian', case=False, na=False)] # Extracting the rows containing 'pedestrian' in the highway column\n",
    "\n",
    "        ## deal with nan names\n",
    "\n",
    "\n",
    "        ## dissolve lines that are very very close to each other\n",
    "        if not street_continuations_gdf.empty:\n",
    "            street_continuations_gdf = street_continuations_gdf.to_crs('27700')\n",
    "            street_continuations_gdf['buffer'] = street_continuations_gdf.geometry.buffer(1)\n",
    "            dissolved = street_continuations_gdf.dissolve(by='name')\n",
    "            \n",
    "            # If a MultiPolygon is formed, convert it to individual polygons\n",
    "            if isinstance(dissolved.geometry.iloc[0], MultiPolygon):\n",
    "                dissolved = dissolved.explode()\n",
    "            \n",
    "            # Remove the buffer column\n",
    "            dissolved = dissolved.drop(columns='buffer')\n",
    "            street_continuations_gdf = dissolved.to_crs('4326')\n",
    "\n",
    "        return street_continuations_gdf\n",
    "\n",
    "\n",
    "\n",
    "    barriers, streets_gdf = get_barriers(boundary)\n",
    "    busgates, streets_gdf = get_bus_gates(streets_gdf)\n",
    "    oneways = get_contraflows(streets_gdf)\n",
    "    streets_continuations_gdf = filter_streets_continuations(streets_gdf)\n",
    "\n",
    "    # add street conitinuation tag\n",
    "    streets_continuations_gdf['filter_type'] = 'street continuation'\n",
    "\n",
    "\n",
    "    ## ensure correct crs\n",
    "    barriers, busgates, oneways, streets_continuations_gdf = barriers.to_crs('4326'), busgates.to_crs('4326'), oneways.to_crs('4326'), streets_continuations_gdf.to_crs('4326')\n",
    "\n",
    "    filters = gpd.GeoDataFrame(pd.concat([barriers, busgates, oneways, streets_continuations_gdf], ignore_index=True))\n",
    "\n",
    "\n",
    "\n",
    "    ## alter neighbourhoods before joining\n",
    "    # Reset neighbourhood CRS\n",
    "    filters_results_gdf  = neighbourhoods.to_crs('EPSG:27700')\n",
    "\n",
    "    # Buffer to ensure all filters are captured\n",
    "    filters_results_gdf['geometry'] = filters_results_gdf['geometry'].buffer(5)\n",
    "\n",
    "    # Reset neighbourhood CRS\n",
    "    filters_results_gdf  = filters_results_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "    ## Spatial join\n",
    "    # Perform a spatial join between neighbourhoods and filters\n",
    "    joined_data = gpd.sjoin(filters_results_gdf, filters, how=\"left\", predicate=\"intersects\", lsuffix='_neigh', rsuffix='_filt')\n",
    "\n",
    "    # Count the number of each filter within each neighbourhood\n",
    "    filter_type_counts = joined_data.groupby(['ID', 'filter_type']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Reset the index to make it more readable\n",
    "    filter_type_counts = filter_type_counts.reset_index()\n",
    "\n",
    "    # Merge the filter_type_counts DataFrame with the neighbourhoods GeoDataFrame on the ID column\n",
    "    filters_results_gdf = filters_results_gdf.merge(filter_type_counts, on='ID', how='left')\n",
    "\n",
    "    # Define the columns to sum\n",
    "    columns_to_sum = ['barrier or bollard', 'one-way bike', 'bus gate', 'street continuation']\n",
    "\n",
    "    # Filter out columns that exist in the DataFrame\n",
    "    existing_columns = [col for col in columns_to_sum if col in filters_results_gdf.columns]\n",
    "\n",
    "    # Sum the values in the existing columns per row\n",
    "    filters_results_gdf['total_filter_types'] = filters_results_gdf[existing_columns].sum(axis=1)\n",
    "\n",
    "    # Fill NaN values with 0 if necessary\n",
    "    filters_results_gdf = filters_results_gdf.fillna(0)\n",
    "\n",
    "    # Find locations where filters are found dense\n",
    "    # Convert road density to numeric if not already\n",
    "    filters_results_gdf['road_density'] = pd.to_numeric(filters_results_gdf['road_density'], errors='coerce')\n",
    "\n",
    "    # Create new column to hold filters * density value\n",
    "    filters_results_gdf['filter_road_density'] = filters_results_gdf['total_filter_types'] * filters_results_gdf['road_density']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### rat runs\n",
    "\n",
    "\n",
    "    drive_g = ox.graph_from_place(place, network_type='drive', simplify=True)\n",
    "\n",
    "    ## Clean graph and calculate travel times along edges\n",
    "\n",
    "    # Function to clean 'maxspeed' values\n",
    "    def clean_maxspeed(maxspeed):\n",
    "        if maxspeed is None:\n",
    "            return 30  # Replace None with a default value of 30\n",
    "        elif isinstance(maxspeed, str) and ' mph' in maxspeed:\n",
    "            return float(maxspeed.replace(' mph', ''))\n",
    "        elif isinstance(maxspeed, list):  # Handle cases where 'maxspeed' is a list\n",
    "            return [float(speed.replace(' mph', '')) for speed in maxspeed]\n",
    "        else:\n",
    "            return maxspeed\n",
    "\n",
    "    # Apply the function to 'maxspeed' in each edge attribute\n",
    "    for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "        if 'maxspeed' in data:\n",
    "            data['maxspeed'] = clean_maxspeed(data['maxspeed'])\n",
    "        else:\n",
    "            data['maxspeed'] = 30  # Assign default value of 30 if 'maxspeed' is missing\n",
    "\n",
    "    # Function to convert 'maxspeed' to a numeric value\n",
    "    def convert_maxspeed(maxspeed):\n",
    "        if isinstance(maxspeed, list) and maxspeed:  # Check if 'maxspeed' is a non-empty list\n",
    "            # If 'maxspeed' is a list, convert the first value to a numeric value\n",
    "            return convert_single_maxspeed(maxspeed[0])\n",
    "        else:\n",
    "            # If 'maxspeed' is not a list or an empty list, convert the single value to a numeric value\n",
    "            return convert_single_maxspeed(maxspeed)\n",
    "\n",
    "    # Helper function to convert a single maxspeed value to a numeric value\n",
    "    def convert_single_maxspeed(maxspeed):\n",
    "        if maxspeed is None:\n",
    "            return 30  # Replace None with a default value of 30\n",
    "\n",
    "        if isinstance(maxspeed, str):\n",
    "            # Extract numeric part of the string using regular expression\n",
    "            numeric_part = ''.join(c for c in maxspeed if c.isdigit() or c == '.')\n",
    "            return float(numeric_part) if numeric_part else 30  # Default value if no numeric part found\n",
    "        elif isinstance(maxspeed, (int, float)):\n",
    "            return maxspeed\n",
    "        else:\n",
    "            return 30  # Default value if the type is unknown\n",
    "\n",
    "    # Function to calculate travel time\n",
    "    def calculate_travel_time(length, maxspeed):\n",
    "        # Convert 'maxspeed' to a numeric value\n",
    "        maxspeed_value = convert_maxspeed(maxspeed)\n",
    "\n",
    "        # Convert maxspeed to meters per second\n",
    "        speed_mps = maxspeed_value * 0.44704  # 1 mph = 0.44704 m/s\n",
    "\n",
    "        # Calculate travel time in seconds using the formula: time = distance/speed\n",
    "        if length is not None and speed_mps > 0:\n",
    "            travel_time = length / speed_mps\n",
    "            return travel_time\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply the function to 'length' and 'maxspeed' in each edge attribute\n",
    "    for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "        if 'length' in data:\n",
    "            data['travel_time'] = calculate_travel_time(data.get('length'), data.get('maxspeed'))\n",
    "\n",
    "\n",
    "\n",
    "    def get_sparse_graph(drive_g):\n",
    "        \"\"\"\n",
    "        Create a sparse graph from bounding roads.\n",
    "\n",
    "        Args:\n",
    "            drive_g (networkx.Graph): The original graph.\n",
    "\n",
    "        Returns:\n",
    "            networkx.Graph: The sparse graph.\n",
    "        \"\"\"\n",
    "        # Create a copy of the original graph\n",
    "        sparse_drive_g = drive_g.copy()\n",
    "\n",
    "        # Define the conditions for keeping edges\n",
    "        conditions = [\n",
    "            (\n",
    "                data.get('highway') in ['trunk', 'trunk_link', 'motorway', 'motorway_link', 'primary', 'primary_link',\n",
    "                                        'secondary', 'secondary_link', 'tertiary', 'tertiary_link']\n",
    "            ) or (\n",
    "                data.get('maxspeed') in ['60', '70', '40', ('20', '50'), ('30', '60'), ('30', '50'), ('70', '50'),\n",
    "                                        ('40', '60'), ('70', '60'), ('60', '40'), ('50', '40'), ('30', '40'),\n",
    "                                        ('20', '60'), ('70 ', '40 '), ('30 ', '70')]\n",
    "            )\n",
    "            for u, v, k, data in sparse_drive_g.edges(keys=True, data=True)\n",
    "        ]\n",
    "\n",
    "        # Keep only the edges that satisfy the conditions\n",
    "        edges_to_remove = [\n",
    "            (u, v, k) for (u, v, k), condition in zip(sparse_drive_g.edges(keys=True), conditions) if not condition\n",
    "        ]\n",
    "        sparse_drive_g.remove_edges_from(edges_to_remove)\n",
    "\n",
    "        # Clean nodes by removing isolated nodes from the graph\n",
    "        isolated_nodes = list(nx.isolates(sparse_drive_g))\n",
    "        sparse_drive_g.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "        return sparse_drive_g\n",
    "\n",
    "\n",
    "\n",
    "    sparse_drive_g = get_sparse_graph(drive_g)\n",
    "\n",
    "\n",
    "    #print(\"Number of edges in the sparse graph:\", sparse_drive_g.number_of_edges())\n",
    "\n",
    "\n",
    "\n",
    "    ## create a partitioned network (using the full graph and the sparse graph)\n",
    "\n",
    "    # Make a copy of the original graph\n",
    "    drive_g_copy = drive_g.copy()\n",
    "\n",
    "    ## Remove edges \n",
    "    drive_g_copy.remove_edges_from(sparse_drive_g.edges)\n",
    "\n",
    "    ## Remove nodes\n",
    "    # Convert nodes to strings\n",
    "    sparse_drive_nodes_str = [str(node) for node in sparse_drive_g.nodes]\n",
    "    drive_g_copy.remove_nodes_from(sparse_drive_nodes_str)\n",
    "\n",
    "    # clean nodes by removing isolated nodes from the graph\n",
    "    isolated_nodes = list(nx.isolates(drive_g_copy))\n",
    "    drive_g_copy.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "    len(drive_g_copy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## partition the full graph, by removing the sparse graph from it.\n",
    "\n",
    "    # first nodes shared between sparse_drive_g and drive_g (these nodes are the connection between neighbourhoods and boundary roads)\n",
    "    shared_nodes = set(sparse_drive_g.nodes).intersection(drive_g_copy.nodes)\n",
    "\n",
    "\n",
    "    # we then need to remove nodes where junctions between two neighbourhood nodes and sparse graphs are present. \n",
    "    # we do this by adding new nodes the end of edges which intersect with the sparse graph, to split these junctions up\n",
    "    # Initialize a counter to generate unique indices for new nodes\n",
    "    node_counter = Counter()\n",
    "    # Iterate through shared nodes\n",
    "    for shared_node in shared_nodes:\n",
    "        # Find edges in drive_g connected to the shared node\n",
    "        drive_g_edges = list(drive_g_copy.edges(shared_node, data=True, keys=True))\n",
    "\n",
    "        # Find edges in sparse_drive_g connected to the shared node\n",
    "        sparse_drive_g_edges = list(sparse_drive_g.edges(shared_node, data=True, keys=True))\n",
    "\n",
    "        # Iterate through edges in drive_g connected to the shared node\n",
    "        for u, v, key, data in drive_g_edges:\n",
    "            # Check if the corresponding edge is not in sparse_drive_g\n",
    "            if (u, v, key) not in sparse_drive_g_edges:\n",
    "                # Create new end nodes for the edge in drive_g\n",
    "                new_u = f\"new_{u}\" if u == shared_node else u\n",
    "                new_v = f\"new_{v}\" if v == shared_node else v\n",
    "\n",
    "                # Generate a unique index for each new node ID\n",
    "                new_u_id = f\"{new_u}_{key}_{node_counter[new_u]}\" if new_u != u else new_u\n",
    "                new_v_id = f\"{new_v}_{key}_{node_counter[new_v]}\" if new_v != v else new_v\n",
    "\n",
    "                # Increment the counter for each new node\n",
    "                node_counter[new_u] += 1\n",
    "                node_counter[new_v] += 1\n",
    "\n",
    "                # Add new nodes and update the edge\n",
    "                drive_g_copy.add_node(new_u_id, **drive_g_copy.nodes[u])\n",
    "                drive_g_copy.add_node(new_v_id, **drive_g_copy.nodes[v])\n",
    "\n",
    "                drive_g_copy.add_edge(new_u_id, new_v_id, key=key, **data)\n",
    "\n",
    "                # Check if the reverse edge already exists in drive_g_copy\n",
    "                if not drive_g_copy.has_edge(new_v_id, new_u_id, key):\n",
    "                    # Create the reverse edge with new nodes\n",
    "                    drive_g_copy.add_edge(new_v_id, new_u_id, key=key, **data)\n",
    "\n",
    "                # Disconnect the shared node from the new edge\n",
    "                drive_g_copy.remove_edge(u, v, key)\n",
    "\n",
    "        # Remove the shared node\n",
    "        drive_g_copy.remove_node(shared_node)\n",
    "\n",
    "\n",
    "\n",
    "    # Find strongly connected components in the modified drive_g graph\n",
    "    drive_g_scc = list(nx.strongly_connected_components(drive_g_copy))\n",
    "\n",
    "    # Create a color mapping for edges in each strongly connected component using random colors\n",
    "    edge_colors = {}\n",
    "    for i, component in enumerate(drive_g_scc):\n",
    "        color = (random.random(), random.random(), random.random())  # RGB tuple with random values\n",
    "        for edge in drive_g_copy.edges:\n",
    "            if edge[0] in component and edge[1] in component:\n",
    "                edge_colors[edge] = color\n",
    "\n",
    "    # Plot the graph with edge colors and without nodes\n",
    "    #fig, ax = ox.plot_graph(drive_g_copy, edge_color=[edge_colors.get(edge, (0, 0, 0)) for edge in drive_g_copy.edges], node_size=0, show=False, close=False, figsize=(20, 20))\n",
    "    #ox.plot_graph(sparse_drive_g, ax=ax, edge_color='red', edge_linewidth=2, node_size=0, show=True)\n",
    "    #fig.show()\n",
    "\n",
    "\n",
    "    ## add ssc index to each neighbourhood\n",
    "\n",
    "    # Create a mapping from nodes to their SCC index\n",
    "    node_scc_mapping = {node: i for i, scc in enumerate(drive_g_scc) for node in scc}\n",
    "\n",
    "    # Add SCC attribute to edges\n",
    "    for u, v, key, data in drive_g_copy.edges(keys=True, data=True):\n",
    "        scc_index_u = node_scc_mapping.get(u, None)\n",
    "        scc_index_v = node_scc_mapping.get(v, None)\n",
    "        \n",
    "        # Add the SCC index as an attribute to the edge\n",
    "        drive_g_copy[u][v][key]['scc_index'] = scc_index_u if scc_index_u is not None else scc_index_v\n",
    "\n",
    "\n",
    "    ## join neighbourhood mapping to orignial driving graph\n",
    "\n",
    "    # Add SCC index attribute to drive_g\n",
    "    for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "        scc_index_u = node_scc_mapping.get(u, None)\n",
    "        scc_index_v = node_scc_mapping.get(v, None)\n",
    "        \n",
    "        # Add the SCC index as an attribute to the edge\n",
    "        drive_g[u][v][key]['scc_index'] = scc_index_u if scc_index_u is not None else scc_index_v\n",
    "\n",
    "\n",
    "\n",
    "    ## get random nodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Function to get random nodes present in both graphs for each node\n",
    "    def get_random_nodes_for_each(graph1, graph2):\n",
    "        random_nodes_for_each = {}\n",
    "        common_nodes = set(graph1.nodes()) & set(graph2.nodes())\n",
    "        total_common_nodes = len(common_nodes)\n",
    "        num_nodes = min(iterations, max(1, int(total_common_nodes * 0.9)))  # 10% less than the total number of common nodes, capped at the input max iterations\n",
    "\n",
    "        for node in common_nodes:\n",
    "            neighbors = list(set(graph1.neighbors(node)) & set(graph2.neighbors(node)))\n",
    "            if len(neighbors) >= num_nodes:\n",
    "                random_neighbors = random.sample(neighbors, num_nodes)\n",
    "            else:\n",
    "                random_neighbors = neighbors + random.sample(list(common_nodes - set(neighbors)), num_nodes - len(neighbors))\n",
    "            random_nodes_for_each[node] = random_neighbors\n",
    "        return random_nodes_for_each\n",
    "\n",
    "\n",
    "\n",
    "    # Get random nodes for each common node\n",
    "    random_nodes_for_each = get_random_nodes_for_each(drive_g, sparse_drive_g)\n",
    "\n",
    "\n",
    "\n",
    "    # Print random nodes for each common node\n",
    "    #for node, random_neighbors in random_nodes_for_each.items():\n",
    "        #print(f\"Random nodes for node {node}: {random_neighbors}\")\n",
    "\n",
    "\n",
    "    ## Find shortest paths \n",
    "\n",
    "    # Convert the dictionary of nodes into a list of tuples\n",
    "    nodes_list = [(key, value) for key, values in random_nodes_for_each.items() for value in values]\n",
    "\n",
    "    # Find shortest paths and store them in a dictionary\n",
    "    shortest_paths = {}\n",
    "    for start_node, end_node in nodes_list:\n",
    "        try:\n",
    "            shortest_path = nx.shortest_path(drive_g, start_node, end_node, weight='travel_time')\n",
    "            shortest_paths[(start_node, end_node)] = shortest_path\n",
    "        except nx.NetworkXNoPath:\n",
    "            print(f\"No path found between {start_node} and {end_node}. Skipping...\")\n",
    "\n",
    "    # Print the shortest paths\n",
    "    #for key, value in shortest_paths.items():\n",
    "    #print(f\"Shortest path from {key[0]} to {key[1]}: {value}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## find edges passed through\n",
    "\n",
    "    edges_passed_through = set()\n",
    "\n",
    "    for path in shortest_paths.values():\n",
    "        # Pair consecutive nodes to create edges\n",
    "        path_edges = [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "        \n",
    "        # Check if each edge exists in the graph\n",
    "        for edge in path_edges:\n",
    "            if edge in drive_g.edges:\n",
    "                edges_passed_through.add(edge)\n",
    "\n",
    "    # Convert the set of edges to a list if needed\n",
    "    edges_passed_through = list(edges_passed_through)\n",
    "\n",
    "    for u, v, data in drive_g.edges(data=True):\n",
    "        if (u, v) in edges_passed_through or (v, u) in edges_passed_through:\n",
    "            data['rat_run'] = True\n",
    "        else:\n",
    "            data['rat_run'] = False\n",
    "\n",
    "\n",
    "    # Convert the NetworkX graph to a GeoDataFrame\n",
    "    drive_gdf_nodes, drive_gdf_edges = ox.graph_to_gdfs(drive_g)\n",
    "\n",
    "    drive_gdf_edges = drive_gdf_edges.to_crs(27700)\n",
    "    drive_gdf_nodes = drive_gdf_nodes.to_crs(27700)\n",
    "\n",
    "\n",
    "    # Filter drive_gdf_edges to only include edges with 'rat_run' = True\n",
    "    rat_run_edges = drive_gdf_edges[drive_gdf_edges['rat_run'] == True]\n",
    "\n",
    "    # reset crs\n",
    "    neighbourhoods = neighbourhoods.to_crs(27700)\n",
    "\n",
    "    # Perform spatial join between neighbourhoods and rat_run_edges\n",
    "    join_result = gpd.sjoin(neighbourhoods, rat_run_edges, how='left', op='intersects')\n",
    "\n",
    "    # Group by neighbourhood index and count the number of rat_run edges in each\n",
    "    rat_run_edge_count = join_result.groupby(join_result.index)['ID'].count().reset_index(name='rat_run_edge_count')\n",
    "\n",
    "    # Group by neighbourhood index and count the number of rat_run edges in each\n",
    "    rat_run_edge_count = join_result.groupby(join_result.index)['ID'].count().reset_index(name='rat_run_edge_count')\n",
    "\n",
    "    # reset crs\n",
    "    neighbourhoods = neighbourhoods.to_crs(27700)\n",
    "\n",
    "    # Join rat_run_edge_count with neighbourhoods based on index\n",
    "    neighbourhoods = neighbourhoods.join(rat_run_edge_count.set_index('index'))\n",
    "\n",
    "    ## now we should have filters_results_gdf and access_results_gdf, and neighbourhoods with rat run counts joined\n",
    "\n",
    "    filters_results_gdf, access_results_gdf, neighbourhoods\n",
    "\n",
    "    ## join all together\n",
    "\n",
    "    results_gdf = gpd.GeoDataFrame(filters_results_gdf.merge(access_results_gdf, on=\"ID\", suffixes=('_filters', \"_access\")))\n",
    "    results_gdf = results_gdf.set_geometry('geometry_access')\n",
    "    final_results_gdf = results_gdf.merge(neighbourhoods[['ID', 'rat_run_edge_count']], on='ID', how='left')\n",
    "    final_results_gdf['geometry'] = final_results_gdf['geometry_filters']\n",
    "    final_results_gdf = final_results_gdf.set_geometry('geometry')\n",
    "    final_results_gdf.drop(columns=['geometry_filters', 'geometry_access'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Define the scoring function for \"rat_run_edge_count\"\n",
    "    def score_rat_run_edge_count(value):\n",
    "        if value <= 1:\n",
    "            return 100\n",
    "        else:\n",
    "            return 100 / (2 ** value) # Exponetial scoring\n",
    "\n",
    "    # Apply the scoring function to the \"rat_run_edge_count\" column\n",
    "    final_results_gdf[\"rat_run_score\"] = final_results_gdf[\"rat_run_edge_count\"].apply(score_rat_run_edge_count)\n",
    "\n",
    "    import math\n",
    "\n",
    "    def score_mean_distance_diff(value):\n",
    "        if value >= 0:\n",
    "            return 0\n",
    "        elif value <= -750: # set a 750m cut off\n",
    "            return 100\n",
    "        else:\n",
    "            normalized_value = abs(value) / 750  # Normalize the value between 0 and 1\n",
    "            score = 100 * (1 - math.exp(-5 * normalized_value))  # Exponential increase\n",
    "            return score\n",
    "\n",
    "    # Apply the modified scoring function to the \"mean_distance_diff\" column\n",
    "    final_results_gdf[\"mean_distance_diff_score\"] = final_results_gdf[\"mean_distance_diff\"].apply(score_mean_distance_diff)\n",
    "\n",
    "    def score_road_density_filters(value):\n",
    "        if value <= 0:\n",
    "            return 0\n",
    "        elif value >= 40:\n",
    "            return 100\n",
    "        else:\n",
    "            return (value / 40) * 100\n",
    "\n",
    "    # Apply the scoring function to the \"road_density_filters\" column\n",
    "    final_results_gdf[\"filter_road_density_score\"] = final_results_gdf[\"filter_road_density\"].apply(score_road_density_filters)\n",
    "\n",
    "    # Create the \"scored_neighbourhoods\" geodataframe with the necessary columns\n",
    "    scored_neighbourhoods = final_results_gdf[[\"geometry\", \"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]]\n",
    "\n",
    "    # Calculate overall score\n",
    "    scored_neighbourhoods[\"overall_score\"] = (scored_neighbourhoods[\"rat_run_score\"] + scored_neighbourhoods[\"mean_distance_diff_score\"] + scored_neighbourhoods[\"filter_road_density_score\"]) / 3\n",
    "\n",
    "    # Define weights for each score\n",
    "    weight_rat_run_score = 1\n",
    "    weight_mean_distance_diff_score = 1\n",
    "    weight_road_density_filters_score = 1\n",
    "\n",
    "    weight_rat_run_score = through_route_weighting\n",
    "    weight_mean_distance_diff_score = permiablity_weighting\n",
    "    weight_road_density_filters_score = modal_filter_weighting\n",
    "\n",
    "    # Calculate overall score with weights\n",
    "    scored_neighbourhoods[\"overall_score\"] = (\n",
    "        (weight_rat_run_score * scored_neighbourhoods[\"rat_run_score\"]) +\n",
    "        (weight_mean_distance_diff_score * scored_neighbourhoods[\"mean_distance_diff_score\"]) +\n",
    "        (weight_road_density_filters_score * scored_neighbourhoods[\"filter_road_density_score\"])\n",
    "    ) / (weight_rat_run_score + weight_mean_distance_diff_score + weight_road_density_filters_score)\n",
    "\n",
    "\n",
    "\n",
    "    ## find elbow point for k-means clustering\n",
    "\n",
    "    # Selecting the features for clustering\n",
    "    X = scored_neighbourhoods[[\"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]]\n",
    "\n",
    "    # Initialize a list to store the within-cluster sum of squares (WCSS) for different values of K\n",
    "    wcss = []\n",
    "\n",
    "    # Define the range of K values to try\n",
    "    k_values = range(1, 11)\n",
    "\n",
    "    # Calculate WCSS for each value of K\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    # Plotting the elbow curve\n",
    "    plt.plot(k_values, wcss, marker='o')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.xticks(k_values)\n",
    "    plt.show()\n",
    "\n",
    "    ## Run k-means clustering\n",
    "    # Define the number of clusters\n",
    "    k = 2\n",
    "\n",
    "    # Select the features for clustering\n",
    "    features = [\"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]\n",
    "\n",
    "    # Extract the features from the dataframe\n",
    "    X = scored_neighbourhoods[features]\n",
    "\n",
    "    # Initialize the KMeans model\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Get the cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Add the cluster labels to the dataframe\n",
    "    scored_neighbourhoods[\"cluster_label\"] = cluster_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## maps\n",
    "    ## adjust geodataframe contents for plotting purposes\n",
    "\n",
    "    replacement_map = {\n",
    "        'barrier or bollard': 'Barrier or Bollard',\n",
    "        'bus gate': 'Bus Gate',\n",
    "        'one-way bike': 'Cycle Contraflow',\n",
    "        'street continuation': 'Street Continuation'\n",
    "    }\n",
    "\n",
    "    # Replace filter types in the DataFrame\n",
    "    filters['filter_type'] = filters['filter_type'].map(replacement_map).fillna(filters['filter_type'])\n",
    "\n",
    "\n",
    "    import folium\n",
    "    import branca.colormap as cm\n",
    "    from folium.plugins import MarkerCluster\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the centroid of the scored_neighbourhoods GeoDataFrame\n",
    "    centroid = scored_neighbourhoods.geometry.centroid.iloc[0]\n",
    "    center_latitude, center_longitude = centroid.y, centroid.x\n",
    "\n",
    "    # Create a Folium map centered around the centroid of scored_neighbourhoods\n",
    "    m = folium.Map(location=[center_latitude, center_longitude], zoom_start=12)\n",
    "\n",
    "    # Define the colormap using cm.linear.viridis\n",
    "    cmap = cm.linear.viridis.scale(scored_neighbourhoods['overall_score'].min(), scored_neighbourhoods['overall_score'].max())\n",
    "\n",
    "    # Plot scored_neighbourhoods using the Viridis colormap\n",
    "    folium.GeoJson(scored_neighbourhoods,\n",
    "                name= \"Scored Neighbourhoods\",\n",
    "                style_function=lambda x: {'fillColor': cmap(x['properties']['overall_score']),\n",
    "                                            'color': cmap(x['properties']['overall_score']),\n",
    "                                            'weight': 1, 'fillOpacity': 0.7},\n",
    "                tooltip=folium.features.GeoJsonTooltip(\n",
    "                    fields=['rat_run_score', 'mean_distance_diff_score', 'filter_road_density_score', 'overall_score', 'cluster_label'],\n",
    "                    aliases=['Rat Run Score', 'Mean Distance Diff Score', 'Filter Road Density Score', 'Overall Score', 'Cluster Label'])\n",
    "                ).add_to(m)\n",
    "\n",
    "    # Plot streets_gdf on the map with default blue color and slightly transparent\n",
    "    streets_layer = folium.GeoJson(drive_gdf_edges,\n",
    "                                name=\"Streets\",\n",
    "                                style_function=lambda x: {'color': 'lightgreen', 'weight': 1, 'fillOpacity': 0.7}\n",
    "                                ).add_to(m)\n",
    "\n",
    "    # Plot rat_run_edges on the map with red color\n",
    "    rat_runs_layer = folium.GeoJson(rat_run_edges,\n",
    "                                    name=\"Rat Runs\",\n",
    "                                    style_function=lambda x: {'color': 'red', 'weight': 1.5, 'fillOpacity': 0.7}\n",
    "                                ).add_to(m)\n",
    "\n",
    "    # Plot boundary_roads on the map with orange color and thicker weight\n",
    "    boundary_roads_layer = folium.GeoJson(boundary_roads,\n",
    "                                        name=\"Busy Roads\",\n",
    "                                        style_function=lambda x: {'color': 'orange', 'weight': 3, 'fillOpacity': 0.7}\n",
    "                                        ).add_to(m)\n",
    "\n",
    "\n",
    "\n",
    "    # Create a feature group for each type of layer\n",
    "    point_group = folium.FeatureGroup(name='Modal Filtering Points', show=True)\n",
    "    line_group = folium.FeatureGroup(name='Modal Filtering Streets', show=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot purple point markers for filters with tooltips\n",
    "    for _, row in filters.iterrows():\n",
    "        if row.geometry.type == 'Point':\n",
    "            tooltip_text = f\"Filter type: {row['filter_type']}\"  # Concatenating \"Filter type:\" with the 'filter_type' value\n",
    "            folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=2, color='purple', fill=True, fill_color='purple', tooltip=tooltip_text).add_to(point_group)\n",
    "        elif row.geometry.type == 'MultiLineString' or row.geometry.type == 'LineString':\n",
    "            tooltip_text = f\"Filter type: {row['filter_type']}\"  # Concatenating \"Filter type:\" with the 'filter_type' value\n",
    "            folium.GeoJson(row.geometry, style_function=lambda x: {'color': 'purple', 'weight': 1.5, 'fillOpacity': 0.7}, tooltip=tooltip_text).add_to(line_group)\n",
    "\n",
    "\n",
    "    # Add layer groups to the map\n",
    "    point_group.add_to(m)\n",
    "    line_group.add_to(m)\n",
    "\n",
    "    # Add layer control\n",
    "    folium.LayerControl(autoZIndex=True).add_to(m)\n",
    "\n",
    "    cmap.caption = 'LTN Plausiblity Scores (Possible range: 0-100)'\n",
    "    cmap.add_to(m)\n",
    "\n",
    "\n",
    "    # add text\n",
    "    from folium import IFrame\n",
    "\n",
    "    # Define the HTML content for the text\n",
    "    html_text = \"\"\"\n",
    "    <div style=\"position: fixed; \n",
    "                bottom: 50px; left: 50px; width: 300px; height: 200px; \n",
    "                background-color: rgba(255, 255, 255, 0.6);\n",
    "                border:2px solid grey; z-index:9999;\n",
    "                font-size:14px;\n",
    "                \">\n",
    "        <p style=\"padding: 10px;\">Scored neighbourhoods show a LTN 'Plausibility' score which incorporates metrics based on the presence of rat-runs, modal filters and measures of neighbourhood permeability. Map results are experimental, and should be treated as such. Get in touch via c.larkin@newcastle.ac.uk or <a href=\"https://github.com/Froguin99/LTN-Detection\" target=\"_blank\">https://github.com/Froguin99/LTN-Detection</a>.</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    # Add the HTML content to the map\n",
    "    folium.MacroElement().add_to(m)\n",
    "    m.get_root().html.add_child(folium.Element(html_text))\n",
    "\n",
    "\n",
    "    # save to geopackage\n",
    "\n",
    "    # Extract place name without \", United Kingdom\"\n",
    "    place_name = place.replace(\", United Kingdom\", \"\").strip()\n",
    "\n",
    "    # Create the file paths\n",
    "    map_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\Examples\\maps', f'{place_name}_example.html')\n",
    "    geopackage_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\scored_neighbourhoods', f'scored_neighbourhoods_{place_name}.gpkg')\n",
    "\n",
    "    # Export map\n",
    "    m.save(map_file_path)\n",
    "\n",
    "    # Send to geopackage \n",
    "    geometry_column = scored_neighbourhoods.geometry.name\n",
    "\n",
    "    # Iterate through the columns and convert them to strings\n",
    "    for column in scored_neighbourhoods.columns:\n",
    "        if column != geometry_column:\n",
    "            scored_neighbourhoods[column] = scored_neighbourhoods[column].astype(str)\n",
    "\n",
    "    scored_neighbourhoods.to_file(geopackage_file_path, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "    ## export rat runs \n",
    "    geopackage_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\rat_runs', f'rat_runs_{place_name}.gpkg')\n",
    "\n",
    "\n",
    "    # Send to geopackage \n",
    "    geometry_column = rat_run_edges.geometry.name\n",
    "\n",
    "    # Iterate through the columns and convert them to strings\n",
    "    for column in rat_run_edges.columns:\n",
    "        if column != geometry_column:\n",
    "            rat_run_edges[column] = rat_run_edges[column].astype(str)\n",
    "\n",
    "    rat_run_edges.to_file(geopackage_file_path, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "\n",
    "    ## export modal filters\n",
    "    geopackage_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\filters', f'filters_{place_name}.gpkg')\n",
    "\n",
    "\n",
    "    # Send to geopackage \n",
    "    geometry_column = filters.geometry.name\n",
    "\n",
    "    # Iterate through the columns and convert them to strings\n",
    "    for column in filters.columns:\n",
    "        if column != geometry_column:\n",
    "            filters[column] = filters[column].astype(str)\n",
    "\n",
    "    filters.to_file(geopackage_file_path, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "\n",
    "    # Display the map\n",
    "    #m\n",
    "\n",
    "    print(\"Finished\", place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buffer_and_clip_streets(boundary_gdf, buffer_distance=500, utm_crs=\"EPSG:32633\"):\n",
    "    \"\"\"\n",
    "    Buffers a boundary, retrieves street data within the buffered boundary, and clips it to the original boundary. We need this because some areas\n",
    "    such as Borough of Wyre, have really strange behvaiour with boundaries, and we need to buffer the boundary to get the correct data.\n",
    "    \n",
    "    Parameters:\n",
    "    boundary_gdf (GeoDataFrame): Boundary geometry in EPSG:4326.\n",
    "    buffer_distance (float): Buffer distance in meters. Default is 500 meters.\n",
    "    utm_crs (str): UTM CRS for buffering. Default is EPSG:32633.\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: Clipped streets within the original boundary.\n",
    "    \"\"\"\n",
    "    # Create and buffer the bounding box\n",
    "    bbox_4326 = gpd.GeoDataFrame(geometry=[box(*boundary_gdf.total_bounds)], crs=boundary_gdf.crs)\n",
    "    bbox_utm = bbox_4326.to_crs(utm_crs).buffer(buffer_distance).to_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Retrieve street network data within the buffered bounding box\n",
    "    streets = ox.graph_to_gdfs(\n",
    "        ox.graph_from_polygon(bbox_utm.unary_union, network_type='all'),\n",
    "        nodes=False\n",
    "    )\n",
    "    \n",
    "    # Clip streets to the original boundary\n",
    "    return streets.clip(boundary_gdf)\n",
    "\n",
    "# Example usage:\n",
    "gdf_streets_clipped = buffer_and_clip_streets(boundary_buffered_4326, buffer_distance=1000)\n",
    "\n",
    "# Display the clipped streets\n",
    "gdf_streets_clipped.explore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### shamelessly copied from chatbot\n",
    "\n",
    "# Get the total bounds of the GeoDataFrame (xmin, ymin, xmax, ymax)\n",
    "minx, miny, maxx, maxy = boundary_buffered_4326.total_bounds\n",
    "\n",
    "# Create a bounding box from the total bounds\n",
    "bounding_box = box(minx, miny, maxx, maxy)\n",
    "\n",
    "# Convert the bounding box to a GeoDataFrame with the original CRS (EPSG:4326)\n",
    "bounding_box_gdf = gpd.GeoDataFrame({\"geometry\": [bounding_box]}, crs=boundary_buffered_4326.crs)\n",
    "\n",
    "# Reproject the bounding box to a CRS that uses meters (e.g., UTM)\n",
    "# Assuming you are working in a region near the equator, you can use UTM Zone 33N as an example\n",
    "bounding_box_gdf_utm = bounding_box_gdf.to_crs(\"EPSG:32633\")\n",
    "\n",
    "# Buffer the bounding box by 500 meters in the UTM projection\n",
    "buffered_bounding_box_gdf_utm = bounding_box_gdf_utm.buffer(1000)\n",
    "\n",
    "# Convert the buffered bounding box back to EPSG:4326 (WGS 84)\n",
    "buffered_bounding_box_gdf_4326 = gpd.GeoDataFrame(geometry=buffered_bounding_box_gdf_utm, crs=\"EPSG:32633\").to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Display the buffered bounding box GeoDataFrame\n",
    "buffered_bounding_box_gdf_4326.explore()\n",
    "\n",
    "gdf_streets = ox.graph_to_gdfs(ox.graph_from_polygon(buffered_bounding_box_gdf_4326.iloc[0].geometry, network_type='all'), nodes=False) # bounding box\n",
    "\n",
    "# Assuming you have already created the gdf_streets GeoDataFrame\n",
    "gdf_streets = ox.graph_to_gdfs(\n",
    "    ox.graph_from_polygon(buffered_bounding_box_gdf_4326.iloc[0].geometry, network_type='all'), \n",
    "    nodes=False\n",
    ")\n",
    "\n",
    "# Clip gdf_streets to the boundary of boundary_buffered_4326\n",
    "gdf_streets_clipped = gdf_streets.clip(boundary_buffered_4326)\n",
    "\n",
    "# Display the clipped GeoDataFrame\n",
    "gdf_streets_clipped.explore()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Reset boundary_buffered crs for passing to OSM\n",
    "boundary_buffered_4326 = boundary_buffered.to_crs('4326')\n",
    "boundary_buffered_4326.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhoods = erased_boundary_gdf\n",
    "neighbourhoods = drop_large_or_small_areas(neighbourhoods)\n",
    "neighbourhoods = filter_neighbourhoods_by_roads(neighbourhoods, os_open_roads_clip, 'geometry')\n",
    "\n",
    "## create unique IDs\n",
    "# simple number based ID\n",
    "neighbourhoods['ID'] = range(1, len(neighbourhoods) + 1)\n",
    "\n",
    "neighbourhoods['geometry'] = neighbourhoods['geometry'].apply(remove_holes)\n",
    "\n",
    "\n",
    "## filter neighbourhoods to only locations with more than 1 intersection (1 or fewer intersections indicates that all travel modes will be the same)\n",
    "# reset neighbourhoods crs\n",
    "neighbourhoods = neighbourhoods.to_crs('4326')\n",
    "\n",
    "\n",
    "\n",
    "neighbourhoods.explore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_nodes_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhoods = erased_boundary_gdf\n",
    "neighbourhoods = drop_large_or_small_areas(neighbourhoods)\n",
    "\n",
    "neighbourhoods = filter_neighbourhoods_by_roads(neighbourhoods, os_open_roads_clip, 'geometry')\n",
    "\n",
    "## create unique IDs\n",
    "# simple number based ID\n",
    "neighbourhoods['ID'] = range(1, len(neighbourhoods) + 1)\n",
    "\n",
    "neighbourhoods['geometry'] = neighbourhoods['geometry'].apply(remove_holes)\n",
    "\n",
    "\n",
    "## filter neighbourhoods to only locations with more than 1 intersection (1 or fewer intersections indicates that all travel modes will be the same)\n",
    "# reset neighbourhoods crs\n",
    "neighbourhoods = neighbourhoods.to_crs('4326')\n",
    "\n",
    "# Spatial join to count points within each neighborhood\n",
    "spatial_join = gpd.sjoin(neighbourhoods, common_nodes_gdf, how='left', op='contains')\n",
    "\n",
    "# Group by 'ID' and count the points within each neighborhood\n",
    "point_counts = spatial_join.groupby('ID').size().reset_index(name='point_count')\n",
    "\n",
    "# Filter out neighborhoods with 1 or 0 points\n",
    "filtered_neighbourhood_ids = point_counts[point_counts['point_count'] > 1]['ID']\n",
    "\n",
    "neighbourhoods= neighbourhoods[neighbourhoods['ID'].isin(filtered_neighbourhood_ids)]\n",
    "\n",
    "\n",
    "\n",
    "## we also need to join the length of the streets within the neighbourhood for further analysis\n",
    "# Reset index of neighbourhoods\n",
    "neighbourhoods = neighbourhoods.reset_index(drop=True)\n",
    "\n",
    "# reset neighbourhoods crs\n",
    "neighbourhoods = neighbourhoods.to_crs('27700')\n",
    "\n",
    "# Perform a spatial join\n",
    "joined_data = gpd.sjoin(os_open_roads_clip, neighbourhoods, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "# Group by neighborhood and calculate total road length\n",
    "road_lengths = joined_data.groupby('index_right')['length'].sum().reset_index()\n",
    "\n",
    "# Merge road_lengths with neighbourhoods and drop 'index_right' column\n",
    "neighbourhoods = neighbourhoods.merge(road_lengths, left_index=True, right_on='index_right', how='left').drop(columns=['index_right'])\n",
    "\n",
    "# Rename the column\n",
    "neighbourhoods.rename(columns={'length': 'road_lengths'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhoods.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### find accessiablity\n",
    "    ## all to all\n",
    "    def calculate_distance_stats_from_points(points_gdf, network):\n",
    "        all_pairs_shortest_paths = {}\n",
    "        points_osmids = points_gdf.index.tolist()  # Assuming the 'osmid' is the index in the GeoDataFrame\n",
    "\n",
    "        for start_node in points_osmids:\n",
    "            shortest_paths = {}\n",
    "            try:\n",
    "                for end_node in points_osmids:\n",
    "                    if start_node != end_node:\n",
    "                        distance = nx.shortest_path_length(network, start_node, end_node, weight='length')\n",
    "                        shortest_paths[end_node] = distance\n",
    "                all_pairs_shortest_paths[start_node] = shortest_paths\n",
    "            except nx.NetworkXNoPath:\n",
    "                # If no path is found, skip adding to all_pairs_shortest_paths\n",
    "                continue\n",
    "\n",
    "        distances = [length for paths in all_pairs_shortest_paths.values() for length in paths.values()]\n",
    "\n",
    "        if not distances:\n",
    "            return {\n",
    "                \"mean_distance\": 0,\n",
    "                \"median_distance\": 0,\n",
    "                \"min_distance\": 0,\n",
    "                \"max_distance\": 0,\n",
    "                \"distance_range\": 0,\n",
    "                \"total_distance\": 0\n",
    "            }\n",
    "\n",
    "        mean_distance = statistics.mean(distances)\n",
    "        median_distance = statistics.median(distances)\n",
    "        min_distance = min(distances)\n",
    "        max_distance = max(distances)\n",
    "        distance_range = max_distance - min_distance\n",
    "        total_distance = sum(distances)\n",
    "\n",
    "        return {\n",
    "            \"mean_distance\": mean_distance,\n",
    "            \"median_distance\": median_distance,\n",
    "            \"min_distance\": min_distance,\n",
    "            \"max_distance\": max_distance,\n",
    "            \"distance_range\": distance_range,\n",
    "            \"total_distance\": total_distance\n",
    "        }\n",
    "    print(\"starting all to all\")\n",
    "    ## processing for all to all \n",
    "    results = []\n",
    "\n",
    "    for index, row in neighbourhoods.iterrows():\n",
    "        print(f\"Processing neighbourhood {index}...\")\n",
    "        print(f\"Neighbourhood Data: {neighbourhood}\")\n",
    "        neighbourhood = neighbourhoods.loc[[index]]\n",
    "\n",
    "        ## get neighbourhood boundary and neighbourhood boundary buffer\n",
    "        # set crs\n",
    "        neighbourhood = neighbourhood.to_crs('3395')\n",
    "        # create a buffer neighbourhood\n",
    "        neighbourhood_buffer = neighbourhood['geometry'].buffer(15)\n",
    "        # convert back to a geodataframe (for later on)\n",
    "        neighbourhood_buffer = gpd.GeoDataFrame(geometry=neighbourhood_buffer)\n",
    "        # reset crs\n",
    "        neighbourhood, neighbourhood_buffer = neighbourhood.to_crs('4326'), neighbourhood_buffer.to_crs('4326')\n",
    "\n",
    "        ## get nodes which can be driven to and walked to within area\n",
    "        neighbourhood_nodes = gpd.clip(common_nodes_gdf, neighbourhood_buffer)\n",
    "        print(len(neighbourhood_nodes))\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "        if neighbourhood_nodes.empty:\n",
    "            print(f\"No nodes found for neighbourhood {index}. Using default values.\")\n",
    "            walk_stats = {\n",
    "                \"mean_distance\": 0,\n",
    "                \"median_distance\": 0,\n",
    "                \"min_distance\": 0,\n",
    "                \"max_distance\": 0,\n",
    "                \"distance_range\": 0,\n",
    "                \"total_distance\": 0\n",
    "            }\n",
    "            drive_stats = walk_stats\n",
    "\n",
    "\n",
    "            \n",
    "        else:\n",
    "            ## calculate neighbourhood distance stats for walking and driving\n",
    "            walk_stats = calculate_distance_stats_from_points(neighbourhood_nodes, walk_streets)\n",
    "            drive_stats = calculate_distance_stats_from_points(neighbourhood_nodes, drive_streets)\n",
    "\n",
    "        ## get length of total edges within the neighbourhood\n",
    "        edges_within_neighbourhood = gpd.sjoin(all_edges, neighbourhood, how=\"inner\", op=\"intersects\")\n",
    "        total_length = edges_within_neighbourhood['length'].sum()\n",
    "\n",
    "        ## Add the statistics to the GeoDataFrame\n",
    "        neighbourhood['walk_mean_distance'] = walk_stats['mean_distance']\n",
    "        neighbourhood['walk_median_distance'] = walk_stats['median_distance']\n",
    "        neighbourhood['walk_min_distance'] = walk_stats['min_distance']\n",
    "        neighbourhood['walk_max_distance'] = walk_stats['max_distance']\n",
    "        neighbourhood['walk_distance_range'] = walk_stats['distance_range']\n",
    "        neighbourhood['walk_total_distance'] = walk_stats['total_distance']\n",
    "\n",
    "        neighbourhood['drive_mean_distance'] = drive_stats['mean_distance']\n",
    "        neighbourhood['drive_median_distance'] = drive_stats['median_distance']\n",
    "        neighbourhood['drive_min_distance'] = drive_stats['min_distance']\n",
    "        neighbourhood['drive_max_distance'] = drive_stats['max_distance']\n",
    "        neighbourhood['drive_distance_range'] = drive_stats['distance_range']\n",
    "        neighbourhood['drive_total_distance'] = drive_stats['total_distance']\n",
    "\n",
    "        ## Store statistics along with neighborhood ID or other identifying information\n",
    "        result = {\n",
    "            'neighbourhood_id': neighbourhood['ID'].iloc[0],  # Assuming you have an ID column\n",
    "            'walk_mean_distance': walk_stats['mean_distance'],\n",
    "            'walk_median_distance': walk_stats['median_distance'],\n",
    "            'walk_total_distance': walk_stats['total_distance'],\n",
    "            \n",
    "            'drive_mean_distance': drive_stats['mean_distance'],\n",
    "            'drive_median_distance': drive_stats['median_distance'],\n",
    "            'drive_total_distance': drive_stats['total_distance'],\n",
    "\n",
    "            'total_edge_length': total_length\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    ## Convert the results to a new dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    ## calculate differences\n",
    "    results_df['mean_distance_diff'] = results_df['walk_mean_distance'] - results_df['drive_mean_distance']\n",
    "    results_df['median_distance_diff'] = results_df['walk_median_distance'] - results_df['drive_median_distance']\n",
    "    results_df['total_distance_diff'] = results_df['walk_total_distance'] - results_df['drive_total_distance']\n",
    "\n",
    "    merged_df = pd.merge(neighbourhoods, results_df, left_on=\"ID\", right_on=\"neighbourhood_id\")\n",
    "    access_results_gdf = gpd.GeoDataFrame(merged_df, geometry='geometry')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
