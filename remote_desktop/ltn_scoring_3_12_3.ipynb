{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "This notebook will allow for the detection of modal filters, rat runs, and analysis of neighbourhood accessiablity within a single notebook. The output of this code is a set of neighbourhoods scored on their plausiablity to be a \"Low Traffic Neighbourhood\", which is written to a geopackage. To run this code you will need the OS Open Roads dataset available on the OS website: https://osdatahub.os.uk/downloads/open/OpenRoads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General set up\n",
    "Import libraries, set location etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = \"Northumberland, United Kingdom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up python\n",
    "## Library imports\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import momepy\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import pandas as pd\n",
    "import overpy\n",
    "from shapely.geometry import LineString\n",
    "from shapely.geometry import Point\n",
    "import requests\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import Polygon\n",
    "import statistics\n",
    "from shapely.ops import unary_union\n",
    "import random\n",
    "import overpy\n",
    "import math\n",
    "from itertools import count\n",
    "import requests\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Update settings\n",
    "# update osmnx settings\n",
    "useful_tags_ways = ox.settings.useful_tags_way + ['cycleway'] + ['bicycle'] + ['motor_vehicle'] + ['railway'] + ['tunnel'] + ['barrier'] + ['bus'] + ['access'] + ['oneway'] + ['oneway:bicycle'] + ['covered'] + ['waterway']\n",
    "ox.config(use_cache=True, \n",
    "          log_console=True,\n",
    "          useful_tags_way=useful_tags_ways\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up location boundary\n",
    "# set location and get boundary\n",
    "boundary = ox.geocode_to_gdf(place)\n",
    "boundary = boundary.to_crs('EPSG:27700')\n",
    "\n",
    "# buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "boundary_buffered = boundary.buffer(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get our streets from OpenStreetMap and OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get street nodes and edges for walking and driving from OpenStreetMap\n",
    "\n",
    "## reset boundary_buffered crs for passing to OSM\n",
    "boundary_buffered_4326 = boundary_buffered.to_crs('4326')\n",
    "\n",
    "## get street networks\n",
    "all_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='all', simplify=False)\n",
    "walk_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='walk', simplify=True)\n",
    "drive_streets = ox.graph_from_polygon(boundary_buffered_4326.geometry.iloc[0], network_type='drive', simplify=False)\n",
    "\n",
    "all_edges = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "all_nodes = ox.graph_to_gdfs(all_streets, nodes=True, edges=False)\n",
    "\n",
    "walk_edges = ox.graph_to_gdfs(walk_streets, nodes=False, edges=True)\n",
    "walk_nodes = ox.graph_to_gdfs(walk_streets, nodes=True, edges=False)\n",
    "\n",
    "drive_edges = ox.graph_to_gdfs(drive_streets, nodes=False, edges=True)\n",
    "drive_nodes = ox.graph_to_gdfs(drive_streets, nodes=True, edges=False)\n",
    "\n",
    "\n",
    "\n",
    "## find the common nodes between networks\n",
    "# this ensures that shortest paths between points should always be able to be calculated\n",
    "common_nodes = drive_nodes.merge(walk_nodes, on='osmid', suffixes=('_drive', '_walk'))\n",
    "common_nodes_gdf = gpd.GeoDataFrame(common_nodes, geometry='geometry_drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in roads\n",
    "os_open_roads = gpd.read_file(r\"C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\oproad_gpkg_gb\\Data\\oproad_roads_only.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create neighbourhood boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up neighbourhoods\n",
    "# set location and get boundary\n",
    "boundary = ox.geocode_to_gdf(place)\n",
    "boundary = boundary.to_crs('EPSG:27700')\n",
    "\n",
    "# buffer boundary to ensure clips include riverlines which may act as borders between geographies\n",
    "boundary_buffered = boundary.buffer(50)\n",
    "\n",
    "## get railways\n",
    "\n",
    "# for unknown reasons, using rail = ox.graph_from_place(place, custom_filter='[\"railway\"]')\n",
    "# doesn't ALWAYS retrive the full rail network, hence why multiple lines are used to achive the same result\n",
    "\n",
    "# Define railway types to retrieve\n",
    "railway_types = [\"\", \"rail\", \"light_rail\", \"narrow_gauge\", \"subway\", \"tram\"]\n",
    "\n",
    "# Initialize an empty graph\n",
    "combined_railways = nx.MultiDiGraph()\n",
    "\n",
    "for railway_type in railway_types:\n",
    "    try:\n",
    "        # Fetch the railway network for the specified type\n",
    "        network = ox.graph_from_place(place, simplify=False, custom_filter=f'[\"railway\"~\"{railway_type}\"]')\n",
    "        \n",
    "        # Ensure the fetched network is a MultiDiGraph\n",
    "        if not isinstance(network, nx.MultiDiGraph):\n",
    "            network = nx.MultiDiGraph(network)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"No railway data found for '{railway_type}'.\")\n",
    "        network = nx.MultiDiGraph()\n",
    "\n",
    "    # Compose the networks\n",
    "    combined_railways = nx.compose(combined_railways, network)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "railways = ox.graph_to_gdfs(combined_railways, nodes=False, edges=True)\n",
    "\n",
    "# Drop any other railway types that aren't needed\n",
    "railways = railways.loc[(~railways[\"railway\"].isin([\"tunnel\", \"abandoned\", \"razed\", \"disused\", \"funicular\", \"monorail\", \"miniature\"]))]\n",
    "\n",
    "# Drop rows where any of the specified columns have values \"True\" or \"yes\"\n",
    "columns_to_check = ['tunnel', 'abandoned', 'razed', 'disused', 'funicular', 'monorail', 'miniature']\n",
    "railways = railways.loc[~railways[railways.columns.intersection(columns_to_check)].isin(['True', 'yes']).any(axis=1)]\n",
    "\n",
    "# Set railways CRS\n",
    "railways = railways.to_crs('EPSG:27700')\n",
    "\n",
    "## function for getting unusal tags\n",
    "def retrieve_osm_features(polygon, tags):\n",
    "    try:\n",
    "        features = ox.features_from_polygon(polygon=polygon, tags=tags)\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        if \"There are no data elements in the server response\" in error_message:\n",
    "            print(\"No data elements found for the specified location/tags.\")\n",
    "            features = gpd.GeoDataFrame()  # Create an empty GeoDataFrame\n",
    "        else:\n",
    "            # Handle other exceptions here if needed\n",
    "            print(\"An error occurred:\", error_message)\n",
    "            features = None\n",
    "    return features\n",
    "\n",
    "\n",
    "## get rivers\n",
    "# reset boundary crs to allow for features to be found\n",
    "boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "\n",
    "tags = {\"waterway\": [\"river\", \"rapids\"]}\n",
    "\n",
    "rivers = ox.features_from_polygon(polygon = boundary_buffered.iloc[0], tags = tags)\n",
    "\n",
    "# Dropping rows where 'tunnel' is equal to 'culvert'\n",
    "if 'tunnel' in rivers.columns:\n",
    "    # Dropping rows where 'tunnel' is equal to 'culvert'\n",
    "    rivers = rivers[rivers['tunnel'] != 'culvert']\n",
    "\n",
    "# set/reset crs\n",
    "rivers = rivers.to_crs('27700')\n",
    "boundary_buffered = boundary_buffered.to_crs('27700')\n",
    "\n",
    "\n",
    "## get unsuitable landcover types\n",
    "# reset boundary crs to allow for features to be found\n",
    "boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "# Define tags\n",
    "tags = {\"landuse\": [\"industrial\", \"railway\", \"brownfield\", \"commercial\", \"farmland\", \"meadow\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "landuse = ox.features_from_polygon(polygon = boundary_buffered.iloc[0], tags = tags)\n",
    "\n",
    "# set/reset crs\n",
    "landuse = landuse.to_crs('27700')\n",
    "\n",
    "## get unsuitable \"nature\" types\n",
    "\n",
    "# Define tags\n",
    "tags = {\"natural\": [\"wood\", \"water\", \"scrub\", \"coastline\", \"beach\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "nature = retrieve_osm_features(polygon=boundary_buffered.iloc[0], tags=tags)\n",
    "\n",
    "\n",
    "# set/reset crs\n",
    "nature = nature.to_crs('27700')\n",
    "\n",
    "## get unsuitable \"lesiure\" types. This is mainly for golfcourses\n",
    "\n",
    "# Define tags\n",
    "tags = {\"leisure\": [\"golf_course\", \"track\", \"park\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "leisure = ox.features_from_polygon(polygon = boundary_buffered.iloc[0], tags = tags)\n",
    "\n",
    "# set/reset crs\n",
    "leisure = leisure.to_crs('27700')\n",
    "\n",
    "\n",
    "def retrieve_osm_features(polygon, tags):\n",
    "    try:\n",
    "        features = ox.features_from_polygon(polygon=polygon, tags=tags)\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        if \"There are no data elements in the server response\" in error_message:\n",
    "            print(\"No data elements found for the specified location/tags.\")\n",
    "            features = gpd.GeoDataFrame()  # Create an empty GeoDataFrame\n",
    "        else:\n",
    "            # Handle other exceptions here if needed\n",
    "            print(\"An error occurred:\", error_message)\n",
    "            features = None\n",
    "    return features\n",
    "\n",
    "# Define the tags for aeroway\n",
    "aeroway_tags = {\"aeroway\": [\"aerodrome\"]}\n",
    "\n",
    "# Use the function to retrieve aeroway features\n",
    "aeroway = retrieve_osm_features(polygon=boundary_buffered.iloc[0], tags=aeroway_tags)\n",
    "\n",
    "# Check if any features were retrieved\n",
    "if aeroway is not None:\n",
    "    if not aeroway.empty:\n",
    "        # set/reset crs\n",
    "        aeroway = aeroway.to_crs('27700')\n",
    "\n",
    "# concat\n",
    "landuse = pd.concat([landuse, nature, leisure, aeroway])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## get bus routes from OSM/NAPTAN\n",
    "\n",
    "# reset boundary crs to allow for features to be found\n",
    "boundary_buffered = boundary_buffered.to_crs('EPSG:4326')\n",
    "\n",
    "# Calculate the bounding box for XML query\n",
    "bounding_box = boundary_buffered.bounds\n",
    "\n",
    "# Extract the minimum and maximum coordinates\n",
    "minx = bounding_box['minx'].min()\n",
    "miny = bounding_box['miny'].min()\n",
    "maxx = bounding_box['maxx'].max()\n",
    "maxy = bounding_box['maxy'].max()\n",
    "\n",
    "# Create a list of four elements representing the bounding box\n",
    "bbox = [minx, miny, maxx, maxy]\n",
    "\n",
    "# reset boundary_buffer crs\n",
    "boundary_buffered = boundary_buffered.to_crs('27700')\n",
    "\n",
    "# Define the Overpass API endpoint\n",
    "overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "# Define the XML query\n",
    "xml_query = f\"\"\"\n",
    "<osm-script output=\"json\" output-config=\"\" timeout=\"160\">\n",
    "  <union into=\"_\">\n",
    "    <query into=\"_\" type=\"node\">\n",
    "      <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "      <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "    </query>\n",
    "    <query into=\"_\" type=\"way\">\n",
    "      <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "      <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "    </query>\n",
    "    <query into=\"_\" type=\"relation\">\n",
    "      <has-kv k=\"route\" modv=\"\" v=\"bus\"/>\n",
    "      <bbox-query s=\"{bbox[1]}\" w=\"{bbox[0]}\" n=\"{bbox[3]}\" e=\"{bbox[2]}\" />\n",
    "    </query>\n",
    "  </union>\n",
    "  <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"body\" n=\"\" order=\"id\" s=\"\" w=\"\"/>\n",
    "  <recurse from=\"_\" into=\"_\" type=\"down\"/>\n",
    "  <print e=\"\" from=\"_\" geometry=\"full\" ids=\"yes\" limit=\"\" mode=\"skeleton\" n=\"\" order=\"quadtile\" s=\"\" w=\"\"/>\n",
    "</osm-script>\n",
    "\n",
    "\"\"\"\n",
    "# Initialize lists to store data\n",
    "geometries = []\n",
    "element_data = []\n",
    "\n",
    "# Make the Overpass API request\n",
    "response = requests.post(overpass_url, data=xml_query)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Access the data from the response\n",
    "    for element in data.get(\"elements\", []):\n",
    "        if element.get('type') == 'way' and 'geometry' in element:\n",
    "            # Extract geometry coordinates from 'geometry' field\n",
    "            coordinates = [(node['lon'], node['lat']) for node in element['geometry']]\n",
    "            # Create a LineString geometry\n",
    "            line = LineString(coordinates)\n",
    "            geometries.append(line)\n",
    "            element_data.append(element)\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    bus_routes = gpd.GeoDataFrame(element_data, geometry=geometries)\n",
    "\n",
    "    # Set CRS\n",
    "    bus_routes = bus_routes.set_crs('4326')\n",
    "    bus_routes = bus_routes.to_crs('27700')\n",
    "\n",
    "else:\n",
    "    print(f\"Error fetching data: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "\n",
    "## clip roads, rivers and railways to boundary\n",
    "\n",
    "# clip\n",
    "os_open_roads_clip = gpd.clip(os_open_roads, boundary_buffered)\n",
    "rivers_clip = gpd.clip(rivers, boundary_buffered)\n",
    "railways_clip = gpd.clip(railways, boundary_buffered)\n",
    "landuse_clip = gpd.clip(landuse, boundary_buffered)\n",
    "bus_routes_clip = gpd.clip(bus_routes, boundary_buffered)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## count bus routes per road and remove roads with greater than 1 bus route on them\n",
    "\n",
    "# set a buffer distance to convert roads to polygons\n",
    "buffer_distance = 0.2  # Adjust this value as needed. Set in meters\n",
    "\n",
    "# Create a new GeoDataFrame with the buffered geometries\n",
    "bus_routes_buffered = bus_routes_clip.copy()  # Copy the original GeoDataFrame\n",
    "bus_routes_buffered['geometry'] = bus_routes_buffered['geometry'].buffer(buffer_distance)\n",
    "\n",
    "# count the number of overlapping bus routes\n",
    "def count_overlapping_features(gdf):\n",
    "    # Create an empty column to store the count of overlapping features\n",
    "    gdf['Bus_routes_count'] = 0\n",
    "\n",
    "    # Iterate through each row in the GeoDataFrame\n",
    "    for idx, row in gdf.iterrows():\n",
    "        # Get the geometry of the current row\n",
    "        geometry = row['geometry']\n",
    "        \n",
    "        # Use a spatial filter to find overlapping features\n",
    "        overlaps = gdf[gdf['geometry'].intersects(geometry)]\n",
    "        \n",
    "        # Update the Bus_routes_count column with the count of overlapping features\n",
    "        gdf.at[idx, 'Bus_routes_count'] = len(overlaps)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# call function\n",
    "bus_routes_buffered_with_count = count_overlapping_features(bus_routes_buffered)\n",
    "\n",
    "# drop any roads which have less than two bus routes on them\n",
    "\n",
    "bus_routes_clip = bus_routes_buffered_with_count[bus_routes_buffered_with_count['Bus_routes_count'] >= 2]\n",
    "\n",
    "\n",
    "\n",
    "# Find \"boundary\" roads\n",
    "boundary_roads = os_open_roads_clip.loc[((os_open_roads_clip['primary_route'] == 'True') |\n",
    "                        (os_open_roads_clip['trunk_road'] == 'True') |\n",
    "                        (os_open_roads_clip['fictitious'] == 'True') |\n",
    "                        (os_open_roads_clip['road_classification'] == 'A Road') | \n",
    "                        (os_open_roads_clip['road_classification'] == 'B Road') | \n",
    "                        #(os_open_roads_clip['road_function'] == 'Restricted Local Access Road') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Minor Road') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Motorway') |\n",
    "                        (os_open_roads_clip['road_function'] == 'Minor Road')  \n",
    "                        )]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## buffering and dissolving functions\n",
    " \n",
    "def buffer_and_dissolve(input_gdf):\n",
    "    # Buffer around boundaries\n",
    "    buffered_gdf = input_gdf.copy()  # Create a copy to avoid modifying the original\n",
    "    buffered_gdf['geometry'] = buffered_gdf['geometry'].buffer(5) # set a 5 meter buffer\n",
    "\n",
    "    # Dissolve the geometries\n",
    "    dissolved_geo = buffered_gdf.unary_union\n",
    "\n",
    "    # Create a new GeoDataFrame with the dissolved geometry\n",
    "    dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "\n",
    "    # Set the CRS (Coordinate Reference System)\n",
    "    dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "    return dissolved_gdf\n",
    "\n",
    "def dissolve_gdf(input_gdf):\n",
    "    # dissolve geometries\n",
    "    dissolved_geo = input_gdf.unary_union\n",
    "    dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_geo])\n",
    "    dissolved_gdf.crs = input_gdf.crs\n",
    "\n",
    "    return dissolved_gdf\n",
    "\n",
    "\n",
    "## buffer and dissolve \n",
    "boundary_roads_bd = buffer_and_dissolve(boundary_roads)\n",
    "boundary_rivers_bd = buffer_and_dissolve(rivers_clip)\n",
    "boundary_rail_bd = buffer_and_dissolve(railways_clip)\n",
    "boundary_landuse_bd = buffer_and_dissolve(landuse_clip)\n",
    "boundary_bus_routes_bd = buffer_and_dissolve(bus_routes_clip)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## join all boundary features\n",
    "boundaries = pd.concat([boundary_rivers_bd \n",
    "                        ,boundary_roads_bd \n",
    "                        ,boundary_rail_bd \n",
    "                        ,boundary_landuse_bd\n",
    "                        ,boundary_bus_routes_bd\n",
    "                        ], ignore_index=True)\n",
    "boundary_features = dissolve_gdf(boundaries)\n",
    "# Use the `difference` method to perform the \"Erase\" operation\n",
    "erased_boundary = boundary.difference(boundary_features.unary_union)\n",
    "\n",
    "# Convert the GeoSeries to a single geometry using unary_union\n",
    "erased_boundary = erased_boundary.unary_union\n",
    "\n",
    "# Create a new GeoDataFrame with the result of \"Erase\" operation\n",
    "erased_boundary_gdf = gpd.GeoDataFrame(geometry=[erased_boundary], crs=boundary.crs)\n",
    "\n",
    "# explode multipolygon to polygons\n",
    "erased_boundary_gdf = erased_boundary_gdf.explode()\n",
    "\n",
    "neighbourhoods = erased_boundary_gdf\n",
    "\n",
    "\n",
    "## drop very small areas (and very large!) (such as the centre of roundabouts etc)\n",
    "# calculate area\n",
    "neighbourhoods[\"area\"] = neighbourhoods.geometry.area\n",
    "# Drop rows where area is less than 5000. This value is arbitary\n",
    "neighbourhoods = neighbourhoods.loc[neighbourhoods[\"area\"] >= 10000]\n",
    "neighbourhoods = neighbourhoods.loc[neighbourhoods[\"area\"] <= 7000000]\n",
    "\n",
    "\n",
    "\n",
    "## drop areas with no roads\n",
    "def count_roads_within_polygons(polygons_gdf, roads_gdf, polygon_column_name):\n",
    "    \"\"\"\n",
    "    Count the number of roads within each polygon in a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        polygons_gdf (GeoDataFrame): GeoDataFrame containing polygons.\n",
    "        roads_gdf (GeoDataFrame): GeoDataFrame containing roads.\n",
    "        polygon_column_name (str): Name of the column in polygons_gdf to use for grouping.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Original polygons GeoDataFrame with a \"road_count\" column added.\n",
    "    \"\"\"\n",
    "    \n",
    "    # spatial join\n",
    "    joined = gpd.sjoin(polygons_gdf, roads_gdf, how='left', op='intersects')\n",
    "    \n",
    "    # Group by the polygon column and count the number of roads in each\n",
    "    road_counts = joined.groupby(polygon_column_name).size().reset_index(name='road_count')\n",
    "    \n",
    "    # Merge the road counts back into the polygons GeoDataFrame\n",
    "    polygons_gdf = polygons_gdf.merge(road_counts, on=polygon_column_name, how='left')\n",
    "\n",
    "     # Calculate road density (area divided by road_count). It is mulitiplied by 10000 for ease of understanding the numbers involved with this\n",
    "    polygons_gdf['road_density'] = (polygons_gdf['road_count'] / polygons_gdf['area'] ) * 10000\n",
    "    \n",
    "    return polygons_gdf\n",
    "\n",
    "neighbourhoods = count_roads_within_polygons(neighbourhoods, os_open_roads_clip, 'geometry')\n",
    "\n",
    "# Drop rows with road_density below 0.2 or less than 4 roads\n",
    "neighbourhoods = neighbourhoods[(neighbourhoods['road_count'] > 2)]\n",
    "neighbourhoods = neighbourhoods[(neighbourhoods['road_density'] > 0.2)]\n",
    "## create unique IDs\n",
    "\n",
    "# simple number based ID\n",
    "neighbourhoods['ID'] = range(1, len(neighbourhoods) + 1)\n",
    "\n",
    "## remove holes from neighbourhoods (for visual reasons mostly)\n",
    "# Function to remove holes from neighbourhoods\n",
    "def remove_holes(polygon):\n",
    "    if polygon.geom_type == 'Polygon':\n",
    "        return Polygon(polygon.exterior)\n",
    "    else:\n",
    "        return polygon\n",
    "\n",
    "# Apply the function to the 'geometry' column of the GeoDataFrame\n",
    "neighbourhoods['geometry'] = neighbourhoods['geometry'].apply(remove_holes)\n",
    "\n",
    "## filter neighbourhoods to only locations with more than 1 intersection (1 or fewer intersections indicates that all travel modes will be the same)\n",
    "# reset neighbourhoods crs\n",
    "neighbourhoods = neighbourhoods.to_crs('4326')\n",
    "\n",
    "# Spatial join to count points within each neighborhood\n",
    "spatial_join = gpd.sjoin(neighbourhoods, common_nodes_gdf, how='left', op='contains')\n",
    "\n",
    "# Group by 'ID' and count the points within each neighborhood\n",
    "point_counts = spatial_join.groupby('ID').size().reset_index(name='point_count')\n",
    "\n",
    "# Filter out neighborhoods with 1 or 0 points\n",
    "filtered_neighbourhood_ids = point_counts[point_counts['point_count'] > 1]['ID']\n",
    "\n",
    "neighbourhoods= neighbourhoods[neighbourhoods['ID'].isin(filtered_neighbourhood_ids)]\n",
    "\n",
    "\n",
    "\n",
    "## we also need to join the length of the streets within the neighbourhood for further analysis\n",
    "\n",
    "# Reset index of neighbourhoods\n",
    "neighbourhoods = neighbourhoods.reset_index(drop=True)\n",
    "\n",
    "# reset neighbourhoods crs\n",
    "neighbourhoods = neighbourhoods.to_crs('27700')\n",
    "\n",
    "# Perform a spatial join\n",
    "joined_data = gpd.sjoin(os_open_roads_clip, neighbourhoods, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "# Group by neighborhood and calculate total road length\n",
    "road_lengths = joined_data.groupby('index_right')['length'].sum().reset_index()\n",
    "\n",
    "# Merge road_lengths with neighbourhoods and drop 'index_right' column\n",
    "neighbourhoods = neighbourhoods.merge(road_lengths, left_index=True, right_on='index_right', how='left').drop(columns=['index_right'])\n",
    "\n",
    "# Rename the column\n",
    "neighbourhoods.rename(columns={'length': 'road_lengths'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check neighbourhoods look good\n",
    "neighbourhoods.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate access metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find accessiablity\n",
    "\n",
    "## all to all\n",
    "def calculate_distance_stats_from_points(points_gdf, network):\n",
    "    all_pairs_shortest_paths = {}\n",
    "    points_osmids = points_gdf.index.tolist()  # Assuming the 'osmid' is the index in the GeoDataFrame\n",
    "\n",
    "    for start_node in points_osmids:\n",
    "        shortest_paths = {}\n",
    "        try:\n",
    "            for end_node in points_osmids:\n",
    "                if start_node != end_node:\n",
    "                    distance = nx.shortest_path_length(network, start_node, end_node, weight='length')\n",
    "                    shortest_paths[end_node] = distance\n",
    "            all_pairs_shortest_paths[start_node] = shortest_paths\n",
    "        except nx.NetworkXNoPath:\n",
    "            # If no path is found, skip adding to all_pairs_shortest_paths\n",
    "            continue\n",
    "\n",
    "    distances = [length for paths in all_pairs_shortest_paths.values() for length in paths.values()]\n",
    "\n",
    "    mean_distance = statistics.mean(distances)\n",
    "    median_distance = statistics.median(distances)\n",
    "    min_distance = min(distances)\n",
    "    max_distance = max(distances)\n",
    "    distance_range = max_distance - min_distance\n",
    "    total_distance = sum(distances)\n",
    "\n",
    "    return {\n",
    "        \"mean_distance\": mean_distance,\n",
    "        \"median_distance\": median_distance,\n",
    "        \"min_distance\": min_distance,\n",
    "        \"max_distance\": max_distance,\n",
    "        \"distance_range\": distance_range,\n",
    "        \"total_distance\": total_distance\n",
    "    }\n",
    "\n",
    "## processing for all to all \n",
    "results = []\n",
    "\n",
    "for index, row in neighbourhoods.iterrows():\n",
    "    neighbourhood = neighbourhoods.loc[[index]]\n",
    "    print(\"Starting index number\", index)\n",
    "\n",
    "    ## get neighbourhood boundary and neighbourhood boundary buffer\n",
    "    # set crs\n",
    "    neighbourhood = neighbourhood.to_crs('27700')\n",
    "    # create a buffer neighbourhood\n",
    "    neighbourhood_buffer = neighbourhood['geometry'].buffer(15)\n",
    "    # convert back to a geodataframe (for later on)\n",
    "    neighbourhood_buffer = gpd.GeoDataFrame(geometry=neighbourhood_buffer)\n",
    "    # reset crs\n",
    "    neighbourhood, neighbourhood_buffer = neighbourhood.to_crs('4326'), neighbourhood_buffer.to_crs('4326')\n",
    "\n",
    "\n",
    "    ## get nodes which can be driven to and walked to within area\n",
    "    neighbourhood_nodes = gpd.clip(common_nodes_gdf, neighbourhood_buffer)\n",
    "\n",
    "    ## get length of total edges within the neighbourhood\n",
    "    edges_within_neighbourhood = gpd.sjoin(all_edges, neighbourhood, how=\"inner\", op=\"intersects\")\n",
    "    total_length = edges_within_neighbourhood['length'].sum()\n",
    "\n",
    "\n",
    "    ## calculate neighbourhood distance stats for walking and driving\n",
    "    walk_stats = calculate_distance_stats_from_points(neighbourhood_nodes, walk_streets)\n",
    "    drive_stats = calculate_distance_stats_from_points(neighbourhood_nodes, drive_streets)\n",
    "\n",
    "\n",
    "    ## Add the statistics to the GeoDataFrame\n",
    "    neighbourhood['walk_mean_distance'] = walk_stats['mean_distance']\n",
    "    neighbourhood['walk_median_distance'] = walk_stats['median_distance']\n",
    "    neighbourhood['walk_min_distance'] = walk_stats['min_distance']\n",
    "    neighbourhood['walk_max_distance'] = walk_stats['max_distance']\n",
    "    neighbourhood['walk_distance_range'] = walk_stats['distance_range']\n",
    "    neighbourhood['walk_total_distance'] = walk_stats['total_distance']\n",
    "\n",
    "    neighbourhood['drive_mean_distance'] = drive_stats['mean_distance']\n",
    "    neighbourhood['drive_median_distance'] = drive_stats['median_distance']\n",
    "    neighbourhood['drive_min_distance'] = drive_stats['min_distance']\n",
    "    neighbourhood['drive_max_distance'] = drive_stats['max_distance']\n",
    "    neighbourhood['drive_distance_range'] = drive_stats['distance_range']\n",
    "    neighbourhood['drive_total_distance'] = drive_stats['total_distance']\n",
    "\n",
    "    ## Store statistics along with neighborhood ID or other identifying information\n",
    "    result = {\n",
    "        'neighbourhood_id': neighbourhood['ID'].iloc[0],  # Assuming you have an ID column\n",
    "        'walk_mean_distance': walk_stats['mean_distance'],\n",
    "        'walk_median_distance': walk_stats['median_distance'],\n",
    "        'walk_total_distance': walk_stats['total_distance'],\n",
    "        \n",
    "\n",
    "        'drive_mean_distance': drive_stats['mean_distance'],\n",
    "        'drive_median_distance': drive_stats['median_distance'],\n",
    "        'drive_total_distance': drive_stats['total_distance'],\n",
    "\n",
    "        'total_edge_length': total_length\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "## Convert the results to a new dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "## calculate differances\n",
    "\n",
    "results_df['mean_distance_diff'] = results_df['walk_mean_distance'] - results_df['drive_mean_distance']\n",
    "results_df['median_distance_diff'] = results_df['walk_median_distance'] - results_df['drive_median_distance']\n",
    "results_df['total_distance_diff'] = results_df['walk_total_distance'] - results_df['drive_total_distance']\n",
    "\n",
    "merged_df = pd.merge(neighbourhoods, results_df, left_on = \"ID\", right_on = \"neighbourhood_id\")\n",
    "access_results_gdf = gpd.GeoDataFrame(merged_df, geometry='geometry')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find modal filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find modal filters\n",
    "\n",
    "\n",
    "## get barrier filters\n",
    "# get the boundary in the correct CRS for OSMnx\n",
    "boundary_4326 = boundary.to_crs('EPSG:4326')\n",
    "\n",
    "# get the most \"basic\" filters mapped, the barriers/bollards etc\n",
    "# get barrier filters\n",
    "# Define tags\n",
    "tags = {\"barrier\": [\"bollard\", \"bus_trap\", \"entrance\", \"planter\", \"sump_buster\", \"wedge\"]}\n",
    "\n",
    "# Use ox.features_from_polygon to find features matching the specified tags\n",
    "barriers = ox.features_from_polygon(polygon = boundary_4326.geometry.iloc[0], tags = tags)\n",
    "\n",
    "\n",
    "## process any linestrings into point geometries\n",
    "\n",
    "# Filter the GeoDataFrame to select only rows with \"linestring\" geometry\n",
    "barriers_linestrings = barriers[barriers['geometry'].geom_type == 'LineString']\n",
    "\n",
    "# Create an empty GeoDataFrame to store the individual points\n",
    "points_gdf = gpd.GeoDataFrame(columns=list(barriers_linestrings.columns), crs=barriers_linestrings.crs)\n",
    "\n",
    "# Iterate through each row in the GeoDataFrame with linestrings\n",
    "for idx, row in barriers_linestrings.iterrows():\n",
    "    if isinstance(row['geometry'], LineString):\n",
    "        # Extract the individual points from the linestring\n",
    "        points = [Point(coord) for coord in list(row['geometry'].coords)]\n",
    "        \n",
    "        # Create a GeoDataFrame from the individual points and copy the attributes\n",
    "        points_df = gpd.GeoDataFrame(geometry=points, crs=barriers_linestrings.crs)\n",
    "        for col in barriers_linestrings.columns:\n",
    "            if col != 'geometry':\n",
    "                points_df[col] = row[col]\n",
    "        \n",
    "        # Rename the \"geometry\" column to \"merged_geometry\"\n",
    "        points_df = points_df.rename(columns={'geometry': 'merged_geometry'})\n",
    "        \n",
    "        # Append the points to the points_gdf\n",
    "        points_gdf = pd.concat([points_gdf, points_df], ignore_index=True)\n",
    "\n",
    "# Now, points_gdf contains all the individual points from the linestrings with inherited attributes\n",
    "\n",
    "# Remove the \"geometry\" column from the points GeoDataFrame\n",
    "points_gdf = points_gdf.drop(columns=['geometry'])\n",
    "\n",
    "# Remove the linestring rows from the original GeoDataFrame\n",
    "barriers = barriers[barriers['geometry'].geom_type != 'LineString']\n",
    "\n",
    "# Rename the \"merged_geometry\" column to \"geometry\" in the points GeoDataFrame\n",
    "points_gdf = points_gdf.rename(columns={'merged_geometry': 'geometry'})\n",
    "\n",
    "# Concatenate the individual points GeoDataFrame to the original GeoDataFrame\n",
    "barriers = pd.concat([barriers, points_gdf], ignore_index=True)\n",
    "\n",
    "# Reset the index to ensure it is continuous\n",
    "barriers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create a new column \"previously_linestring\" and set it to False initially\n",
    "barriers['previously_linestring'] = False\n",
    "\n",
    "# Iterate through each row in the GeoDataFrame with linestrings\n",
    "for idx, row in barriers_linestrings.iterrows():\n",
    "    if isinstance(row['geometry'], LineString):\n",
    "        # Extract the individual points from the linestring\n",
    "        points = [Point(coord) for coord in list(row['geometry'].coords)]\n",
    "        \n",
    "        # Iterate through the points in the linestring\n",
    "        for point in points:\n",
    "            # Check if the point's geometry intersects with any of the original linestrings\n",
    "            mask = barriers['geometry'].intersects(point)\n",
    "            if mask.any():\n",
    "                # If the point intersects with any linestring, set \"previously_linestring\" to True\n",
    "                barriers.loc[mask, 'previously_linestring'] = True\n",
    "\n",
    "# add a unique ID\n",
    "barriers['barrier_id'] = range(1, len(barriers) + 1)\n",
    "\n",
    "# Convert the OSMnx graph to a GeoDataFrame of streets\n",
    "streets_gdf = ox.graph_to_gdfs(all_streets, nodes=False, edges=True)\n",
    "\n",
    "# join the barriers to the streets \n",
    "streets_gdf = gpd.sjoin(streets_gdf, barriers, how = \"left\", op=\"intersects\")\n",
    "\n",
    "# clean geodataframe and drop streets without a barrier\n",
    "streets_gdf.columns = streets_gdf.columns.str.replace(\"_right\", \"_barrier\").str.replace(\"_left\",\"_street\")\n",
    "# we need to double check the name of \"barrier\"\n",
    "streets_gdf['barrier_barrier'] = streets_gdf['barrier'] if 'barrier' in streets_gdf.columns else streets_gdf['barrier_barrier']\n",
    "\n",
    "if 'name_street' in streets_gdf.columns:\n",
    "    streets_gdf = streets_gdf.rename(columns={'name_street': 'name'})\n",
    "barrier_streets = streets_gdf.dropna(subset=['barrier_barrier'])\n",
    "\n",
    "# add barrier tag\n",
    "barrier_streets['filter_type'] = 'barrier or bollard'\n",
    "\n",
    "\n",
    "## extract points which are on/within 1m of streets only\n",
    "streets_gdf['has_barrier'] = 'yes'\n",
    "\n",
    "# reset crs before spatail join\n",
    "barriers, streets_gdf = barriers.to_crs(3857), streets_gdf.to_crs(3857)\n",
    "\n",
    "barriers = gpd.sjoin_nearest(barriers, streets_gdf, how = \"left\", max_distance = 1)\n",
    "barriers = barriers.dropna(subset=['has_barrier'])\n",
    "barriers = barriers.reset_index(drop=True)  # Reset the index\n",
    "# Dissolve based on the 'geometry' column\n",
    "\n",
    "# re-reset crs \n",
    "barriers, streets_gdf = barriers.to_crs(4326), streets_gdf.to_crs(4326)\n",
    "\n",
    "# we need to double check the name of \"barrier_id\"\n",
    "import numpy as np\n",
    "streets_gdf['barrier_id_right'] = streets_gdf['barrier_id'] if 'barrier_id' in streets_gdf.columns else streets_gdf['barrier_id_right']\n",
    "# dissolve\n",
    "barriers = barriers.dissolve(by='barrier_id_right')\n",
    "# add barrier tag\n",
    "barriers['filter_type'] = 'barrier or bollard'\n",
    "# Reset the index to remove multi-index\n",
    "barriers.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## bus gates also act as modal filters. lets now find all the bus gates within our streets\n",
    "\n",
    "# we need to double check the name of \"access\"\n",
    "streets_gdf['access_street'] = streets_gdf['access'] if 'access' in streets_gdf.columns else streets_gdf['access_street']\n",
    "streets_gdf['bicycle_street'] = streets_gdf['bicycle'] if 'bicycle' in streets_gdf.columns else streets_gdf['bicycle_street']\n",
    "streets_gdf['bus'] = streets_gdf['bus_street'] if 'bus_street' in streets_gdf.columns else streets_gdf['bus']\n",
    "\n",
    "\n",
    "busgates = streets_gdf[((streets_gdf[\"bus\"] == \"yes\") & (streets_gdf[\"access_street\"] == \"no\") & (streets_gdf[\"bicycle_street\"] == \"yes\")) |\n",
    "                    (streets_gdf[\"bus\"] == \"yes\") & (streets_gdf[\"motor_vehicle_street\"] == \"no\") & (streets_gdf[\"bicycle_street\"] == \"yes\")\n",
    "                    ]\n",
    "\n",
    "# add bus gate tag\n",
    "busgates['filter_type'] = 'bus gate'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## one-way streets also can act as modal filters. lets find where cycling is unrestricted but cars are\n",
    "oneways = streets_gdf[(streets_gdf[\"oneway\"] == True) & (streets_gdf[\"oneway:bicycle\"] == \"no\")]\n",
    "\n",
    "# we dissolve the roads with the same name as to not miscount the total number of oneways\n",
    "# Convert values in the \"name\" column to strings if they are not already\n",
    "oneways['name'] = oneways['name'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "# Perform dissolve \n",
    "oneways = oneways.dissolve(by='name')\n",
    "\n",
    "# Reset the index \n",
    "oneways = oneways.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# add one way tag\n",
    "oneways['filter_type'] = 'one-way bike'\n",
    "\n",
    "\n",
    "\n",
    "def filter_streets_continuations(input_gdf):\n",
    "    ## clean dataframe\n",
    "    # Check if 'highway_street' column exists and rename it to 'highway'\n",
    "    if 'highway_street' in input_gdf.columns:\n",
    "        input_gdf.rename(columns={'highway_street': 'highway'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # filter dataframe \n",
    "    ## remove indoor roads, these are likey pedestrian only however often don't have any \"cycling\" related tag\n",
    "    if 'covered' in input_gdf.columns:\n",
    "        input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'covered' in str(x))]\n",
    "        input_gdf = input_gdf[input_gdf['covered'] != 'yes']\n",
    "    ## also remove footways and steps, as these are almost pedestrain only, never cyclable\n",
    "    input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'footway' in str(x))]\n",
    "    input_gdf = input_gdf[~input_gdf['highway'].apply(lambda x: 'steps' in str(x))]\n",
    "\n",
    "\n",
    "\n",
    "    ## clean dataframe\n",
    "    input_gdf['name'] = input_gdf['name'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "    input_gdf['highway'] = input_gdf['highway'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## perform street continunation filtering\n",
    "    # Grouping by 'name' and checking for groups with 'pedestrian' and another highway type\n",
    "    grouped = input_gdf.groupby('name').filter(lambda x: any('pedestrian' in val for val in x['highway']) and len(x['highway'].unique()) > 1)\n",
    "    street_continuations_gdf = grouped[grouped['highway'].str.contains('pedestrian', case=False, na=False)] # Extracting the rows containing 'pedestrian' in the highway column\n",
    "\n",
    "    ## deal with nan names\n",
    "\n",
    "\n",
    "    ## dissolve lines that are very very close to each other\n",
    "    if not street_continuations_gdf.empty:\n",
    "        street_continuations_gdf = street_continuations_gdf.to_crs('27700')\n",
    "        street_continuations_gdf['buffer'] = street_continuations_gdf.geometry.buffer(1)\n",
    "        dissolved = street_continuations_gdf.dissolve(by='name')\n",
    "        \n",
    "        # If a MultiPolygon is formed, convert it to individual polygons\n",
    "        if isinstance(dissolved.geometry.iloc[0], MultiPolygon):\n",
    "            dissolved = dissolved.explode()\n",
    "        \n",
    "        # Remove the buffer column\n",
    "        dissolved = dissolved.drop(columns='buffer')\n",
    "        street_continuations_gdf = dissolved.to_crs('4326')\n",
    "\n",
    "    return street_continuations_gdf\n",
    "\n",
    "\n",
    "# get street continunations\n",
    "streets_continuations_gdf = filter_streets_continuations(streets_gdf)\n",
    "\n",
    "# add street conitinuation tag\n",
    "streets_continuations_gdf['filter_type'] = 'street continuation'\n",
    "\n",
    "\n",
    "filters = gpd.GeoDataFrame(pd.concat([barriers, busgates, oneways, streets_continuations_gdf], ignore_index=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## alter neighbourhoods before joining\n",
    "# Reset neighbourhood CRS\n",
    "filters_results_gdf  = neighbourhoods.to_crs('EPSG:27700')\n",
    "\n",
    "# Buffer to ensure all filters are captured\n",
    "filters_results_gdf['geometry'] = filters_results_gdf['geometry'].buffer(5)\n",
    "\n",
    "# Reset neighbourhood CRS\n",
    "filters_results_gdf  = filters_results_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "## Spatial join\n",
    "# Perform a spatial join between neighbourhoods and filters\n",
    "joined_data = gpd.sjoin(filters_results_gdf, filters, how=\"left\", predicate=\"intersects\", lsuffix='_neigh', rsuffix='_filt')\n",
    "\n",
    "# Count the number of each filter within each neighbourhood\n",
    "filter_type_counts = joined_data.groupby(['ID', 'filter_type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Reset the index to make it more readable\n",
    "filter_type_counts = filter_type_counts.reset_index()\n",
    "\n",
    "# Merge the filter_type_counts DataFrame with the neighbourhoods GeoDataFrame on the ID column\n",
    "filters_results_gdf = filters_results_gdf.merge(filter_type_counts, on='ID', how='left')\n",
    "\n",
    "# Define the columns to sum\n",
    "columns_to_sum = ['barrier or bollard', 'one-way bike', 'bus gate', 'street continuation']\n",
    "\n",
    "# Filter out columns that exist in the DataFrame\n",
    "existing_columns = [col for col in columns_to_sum if col in filters_results_gdf.columns]\n",
    "\n",
    "# Sum the values in the existing columns per row\n",
    "filters_results_gdf['total_filter_types'] = filters_results_gdf[existing_columns].sum(axis=1)\n",
    "\n",
    "# Fill NaN values with 0 if necessary\n",
    "filters_results_gdf = filters_results_gdf.fillna(0)\n",
    "\n",
    "# Find locations where filters are found dense\n",
    "# Convert road density to numeric if not already\n",
    "filters_results_gdf['road_density'] = pd.to_numeric(filters_results_gdf['road_density'], errors='coerce')\n",
    "\n",
    "# Create new column to hold filters * density value\n",
    "filters_results_gdf['filter_road_density'] = filters_results_gdf['total_filter_types'] * filters_results_gdf['road_density']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find rat runs through neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_g = ox.graph_from_place(place, network_type='drive', simplify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean graph and calculate travel times along edges\n",
    "\n",
    "# Function to clean 'maxspeed' values\n",
    "def clean_maxspeed(maxspeed):\n",
    "    if maxspeed is None:\n",
    "        return 30  # Replace None with a default value of 30\n",
    "    elif isinstance(maxspeed, str) and ' mph' in maxspeed:\n",
    "        return float(maxspeed.replace(' mph', ''))\n",
    "    elif isinstance(maxspeed, list):  # Handle cases where 'maxspeed' is a list\n",
    "        return [float(speed.replace(' mph', '')) for speed in maxspeed]\n",
    "    else:\n",
    "        return maxspeed\n",
    "\n",
    "# Apply the function to 'maxspeed' in each edge attribute\n",
    "for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "    if 'maxspeed' in data:\n",
    "        data['maxspeed'] = clean_maxspeed(data['maxspeed'])\n",
    "    else:\n",
    "        data['maxspeed'] = 30  # Assign default value of 30 if 'maxspeed' is missing\n",
    "\n",
    "# Function to convert 'maxspeed' to a numeric value\n",
    "def convert_maxspeed(maxspeed):\n",
    "    if isinstance(maxspeed, list) and maxspeed:  # Check if 'maxspeed' is a non-empty list\n",
    "        # If 'maxspeed' is a list, convert the first value to a numeric value\n",
    "        return convert_single_maxspeed(maxspeed[0])\n",
    "    else:\n",
    "        # If 'maxspeed' is not a list or an empty list, convert the single value to a numeric value\n",
    "        return convert_single_maxspeed(maxspeed)\n",
    "\n",
    "# Helper function to convert a single maxspeed value to a numeric value\n",
    "def convert_single_maxspeed(maxspeed):\n",
    "    if maxspeed is None:\n",
    "        return 30  # Replace None with a default value of 30\n",
    "\n",
    "    if isinstance(maxspeed, str):\n",
    "        # Extract numeric part of the string using regular expression\n",
    "        numeric_part = ''.join(c for c in maxspeed if c.isdigit() or c == '.')\n",
    "        return float(numeric_part) if numeric_part else 30  # Default value if no numeric part found\n",
    "    elif isinstance(maxspeed, (int, float)):\n",
    "        return maxspeed\n",
    "    else:\n",
    "        return 30  # Default value if the type is unknown\n",
    "\n",
    "# Function to calculate travel time\n",
    "def calculate_travel_time(length, maxspeed):\n",
    "    # Convert 'maxspeed' to a numeric value\n",
    "    maxspeed_value = convert_maxspeed(maxspeed)\n",
    "\n",
    "    # Convert maxspeed to meters per second\n",
    "    speed_mps = maxspeed_value * 0.44704  # 1 mph = 0.44704 m/s\n",
    "\n",
    "    # Calculate travel time in seconds using the formula: time = distance/speed\n",
    "    if length is not None and speed_mps > 0:\n",
    "        travel_time = length / speed_mps\n",
    "        return travel_time\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to 'length' and 'maxspeed' in each edge attribute\n",
    "for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "    if 'length' in data:\n",
    "        data['travel_time'] = calculate_travel_time(data.get('length'), data.get('maxspeed'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a sparse graph from bounding roads\n",
    "\n",
    "# Create a copy of the original graph\n",
    "sparse_drive_g = drive_g.copy()\n",
    "\n",
    "# Define the conditions for keeping edges\n",
    "conditions = [\n",
    "    (\n",
    "        data.get('highway') in ['trunk', 'trunk_link', 'motorway', 'motorway_link', 'primary', 'primary_link',\n",
    "                                'secondary', 'secondary_link', 'tertiary', 'tertiary_link']\n",
    "    ) or (\n",
    "        data.get('maxspeed') in ['60', '70', '40', ('20', '50'), ('30', '60'), ('30', '50'), ('70', '50'),\n",
    "                                 ('40', '60'), ('70', '60'), ('60', '40'), ('50', '40'), ('30', '40'),\n",
    "                                 ('20', '60'), ('70 ', '40 '), ('30 ', '70')]\n",
    "    )\n",
    "    for u, v, k, data in sparse_drive_g.edges(keys=True, data=True)\n",
    "]\n",
    "\n",
    "# Keep only the edges that satisfy the conditions\n",
    "edges_to_remove = [\n",
    "    (u, v, k) for (u, v, k), condition in zip(sparse_drive_g.edges(keys=True), conditions) if not condition\n",
    "]\n",
    "sparse_drive_g.remove_edges_from(edges_to_remove)\n",
    "\n",
    "# clean nodes by removing isolated nodes from the graph\n",
    "isolated_nodes = list(nx.isolates(sparse_drive_g))\n",
    "sparse_drive_g.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "\n",
    "# Print the number of edges in the sparse graph\n",
    "print(\"Number of edges in the sparse graph:\", sparse_drive_g.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a partitioned network (using the full graph and the sparse graph)\n",
    "\n",
    "# Make a copy of the original graph\n",
    "drive_g_copy = drive_g.copy()\n",
    "\n",
    "## Remove edges \n",
    "drive_g_copy.remove_edges_from(sparse_drive_g.edges)\n",
    "\n",
    "## Remove nodes\n",
    "# Convert nodes to strings\n",
    "sparse_drive_nodes_str = [str(node) for node in sparse_drive_g.nodes]\n",
    "drive_g_copy.remove_nodes_from(sparse_drive_nodes_str)\n",
    "\n",
    "# clean nodes by removing isolated nodes from the graph\n",
    "isolated_nodes = list(nx.isolates(drive_g_copy))\n",
    "drive_g_copy.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "len(drive_g_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## partition the full graph, by removing the sparse graph from it.\n",
    "\n",
    "# first nodes shared between sparse_drive_g and drive_g (these nodes are the connection between neighbourhoods and boundary roads)\n",
    "shared_nodes = set(sparse_drive_g.nodes).intersection(drive_g_copy.nodes)\n",
    "\n",
    "\n",
    "# we then need to remove nodes where junctions between two neighbourhood nodes and sparse graphs are present. \n",
    "# we do this by adding new nodes the end of edges which intersect with the sparse graph, to split these junctions up\n",
    "# Initialize a counter to generate unique indices for new nodes\n",
    "node_counter = Counter()\n",
    "# Iterate through shared nodes\n",
    "for shared_node in shared_nodes:\n",
    "    # Find edges in drive_g connected to the shared node\n",
    "    drive_g_edges = list(drive_g_copy.edges(shared_node, data=True, keys=True))\n",
    "\n",
    "    # Find edges in sparse_drive_g connected to the shared node\n",
    "    sparse_drive_g_edges = list(sparse_drive_g.edges(shared_node, data=True, keys=True))\n",
    "\n",
    "    # Iterate through edges in drive_g connected to the shared node\n",
    "    for u, v, key, data in drive_g_edges:\n",
    "        # Check if the corresponding edge is not in sparse_drive_g\n",
    "        if (u, v, key) not in sparse_drive_g_edges:\n",
    "            # Create new end nodes for the edge in drive_g\n",
    "            new_u = f\"new_{u}\" if u == shared_node else u\n",
    "            new_v = f\"new_{v}\" if v == shared_node else v\n",
    "\n",
    "            # Generate a unique index for each new node ID\n",
    "            new_u_id = f\"{new_u}_{key}_{node_counter[new_u]}\" if new_u != u else new_u\n",
    "            new_v_id = f\"{new_v}_{key}_{node_counter[new_v]}\" if new_v != v else new_v\n",
    "\n",
    "            # Increment the counter for each new node\n",
    "            node_counter[new_u] += 1\n",
    "            node_counter[new_v] += 1\n",
    "\n",
    "            # Add new nodes and update the edge\n",
    "            drive_g_copy.add_node(new_u_id, **drive_g_copy.nodes[u])\n",
    "            drive_g_copy.add_node(new_v_id, **drive_g_copy.nodes[v])\n",
    "\n",
    "            drive_g_copy.add_edge(new_u_id, new_v_id, key=key, **data)\n",
    "\n",
    "            # Check if the reverse edge already exists in drive_g_copy\n",
    "            if not drive_g_copy.has_edge(new_v_id, new_u_id, key):\n",
    "                # Create the reverse edge with new nodes\n",
    "                drive_g_copy.add_edge(new_v_id, new_u_id, key=key, **data)\n",
    "\n",
    "            # Disconnect the shared node from the new edge\n",
    "            drive_g_copy.remove_edge(u, v, key)\n",
    "\n",
    "    # Remove the shared node\n",
    "    drive_g_copy.remove_node(shared_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find strongly connected components in the modified drive_g graph\n",
    "drive_g_scc = list(nx.strongly_connected_components(drive_g_copy))\n",
    "\n",
    "# Create a color mapping for edges in each strongly connected component using random colors\n",
    "edge_colors = {}\n",
    "for i, component in enumerate(drive_g_scc):\n",
    "    color = (random.random(), random.random(), random.random())  # RGB tuple with random values\n",
    "    for edge in drive_g_copy.edges:\n",
    "        if edge[0] in component and edge[1] in component:\n",
    "            edge_colors[edge] = color\n",
    "\n",
    "# Plot the graph with edge colors and without nodes\n",
    "fig, ax = ox.plot_graph(drive_g_copy, edge_color=[edge_colors.get(edge, (0, 0, 0)) for edge in drive_g_copy.edges], node_size=0, show=False, close=False, figsize=(20, 20))\n",
    "ox.plot_graph(sparse_drive_g, ax=ax, edge_color='red', edge_linewidth=2, node_size=0, show=True)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add ssc index to each neighbourhood\n",
    "\n",
    "# Create a mapping from nodes to their SCC index\n",
    "node_scc_mapping = {node: i for i, scc in enumerate(drive_g_scc) for node in scc}\n",
    "\n",
    "# Add SCC attribute to edges\n",
    "for u, v, key, data in drive_g_copy.edges(keys=True, data=True):\n",
    "    scc_index_u = node_scc_mapping.get(u, None)\n",
    "    scc_index_v = node_scc_mapping.get(v, None)\n",
    "    \n",
    "    # Add the SCC index as an attribute to the edge\n",
    "    drive_g_copy[u][v][key]['scc_index'] = scc_index_u if scc_index_u is not None else scc_index_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join neighbourhood mapping to orignial driving graph\n",
    "\n",
    "# Add SCC index attribute to drive_g\n",
    "for u, v, key, data in drive_g.edges(keys=True, data=True):\n",
    "    scc_index_u = node_scc_mapping.get(u, None)\n",
    "    scc_index_v = node_scc_mapping.get(v, None)\n",
    "    \n",
    "    # Add the SCC index as an attribute to the edge\n",
    "    drive_g[u][v][key]['scc_index'] = scc_index_u if scc_index_u is not None else scc_index_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get random nodes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to get random nodes present in both graphs for each node\n",
    "def get_random_nodes_for_each(graph1, graph2):\n",
    "    random_nodes_for_each = {}\n",
    "    common_nodes = set(graph1.nodes()) & set(graph2.nodes())\n",
    "    total_common_nodes = len(common_nodes)\n",
    "    num_nodes = min(1000, max(1, int(total_common_nodes * 0.9)))  # 10% less than the total number of common nodes, capped at 1000\n",
    "\n",
    "    for node in common_nodes:\n",
    "        neighbors = list(set(graph1.neighbors(node)) & set(graph2.neighbors(node)))\n",
    "        if len(neighbors) >= num_nodes:\n",
    "            random_neighbors = random.sample(neighbors, num_nodes)\n",
    "        else:\n",
    "            random_neighbors = neighbors + random.sample(list(common_nodes - set(neighbors)), num_nodes - len(neighbors))\n",
    "        random_nodes_for_each[node] = random_neighbors\n",
    "    return random_nodes_for_each\n",
    "\n",
    "\n",
    "\n",
    "# Get random nodes for each common node\n",
    "random_nodes_for_each = get_random_nodes_for_each(drive_g, sparse_drive_g)\n",
    "\n",
    "\n",
    "# Print random nodes for each common node\n",
    "#for node, random_neighbors in random_nodes_for_each.items():\n",
    "    #print(f\"Random nodes for node {node}: {random_neighbors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find shortest paths \n",
    "\n",
    "# Convert the dictionary of nodes into a list of tuples\n",
    "nodes_list = [(key, value) for key, values in random_nodes_for_each.items() for value in values]\n",
    "\n",
    "# Find shortest paths and store them in a dictionary\n",
    "shortest_paths = {}\n",
    "for start_node, end_node in nodes_list:\n",
    "    try:\n",
    "        shortest_path = nx.shortest_path(drive_g, start_node, end_node, weight='travel_time')\n",
    "        shortest_paths[(start_node, end_node)] = shortest_path\n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"No path found between {start_node} and {end_node}. Skipping...\")\n",
    "\n",
    "# Print the shortest paths\n",
    "#for key, value in shortest_paths.items():\n",
    "#print(f\"Shortest path from {key[0]} to {key[1]}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find edges passed through\n",
    "\n",
    "edges_passed_through = set()\n",
    "\n",
    "for path in shortest_paths.values():\n",
    "    # Pair consecutive nodes to create edges\n",
    "    path_edges = [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "    \n",
    "    # Check if each edge exists in the graph\n",
    "    for edge in path_edges:\n",
    "        if edge in drive_g.edges:\n",
    "            edges_passed_through.add(edge)\n",
    "\n",
    "# Convert the set of edges to a list if needed\n",
    "edges_passed_through = list(edges_passed_through)\n",
    "\n",
    "for u, v, data in drive_g.edges(data=True):\n",
    "    if (u, v) in edges_passed_through or (v, u) in edges_passed_through:\n",
    "        data['rat_run'] = True\n",
    "    else:\n",
    "        data['rat_run'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NetworkX graph to a GeoDataFrame\n",
    "drive_gdf_nodes, drive_gdf_edges = ox.graph_to_gdfs(drive_g)\n",
    "\n",
    "drive_gdf_edges = drive_gdf_edges.to_crs(27700)\n",
    "drive_gdf_nodes = drive_gdf_nodes.to_crs(27700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter drive_gdf_edges to only include edges with 'rat_run' = True\n",
    "rat_run_edges = drive_gdf_edges[drive_gdf_edges['rat_run'] == True]\n",
    "\n",
    "# reset crs\n",
    "neighbourhoods = neighbourhoods.to_crs(27700)\n",
    "\n",
    "# Perform spatial join between neighbourhoods and rat_run_edges\n",
    "join_result = gpd.sjoin(neighbourhoods, rat_run_edges, how='left', op='intersects')\n",
    "\n",
    "# Group by neighbourhood index and count the number of rat_run edges in each\n",
    "rat_run_edge_count = join_result.groupby(join_result.index)['ID'].count().reset_index(name='rat_run_edge_count')\n",
    "\n",
    "# Group by neighbourhood index and count the number of rat_run edges in each\n",
    "rat_run_edge_count = join_result.groupby(join_result.index)['ID'].count().reset_index(name='rat_run_edge_count')\n",
    "\n",
    "# reset crs\n",
    "neighbourhoods = neighbourhoods.to_crs(27700)\n",
    "\n",
    "# Join rat_run_edge_count with neighbourhoods based on index\n",
    "neighbourhoods = neighbourhoods.join(rat_run_edge_count.set_index('index'))\n",
    "\n",
    "print(neighbourhoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we should have filters_results_gdf and access_results_gdf, and neighbourhoods with rat run counts joined\n",
    "\n",
    "filters_results_gdf, access_results_gdf, neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join all together\n",
    "\n",
    "results_gdf = gpd.GeoDataFrame(filters_results_gdf.merge(access_results_gdf, on=\"ID\", suffixes=('_filters', \"_access\")))\n",
    "results_gdf = results_gdf.set_geometry('geometry_access')\n",
    "final_results_gdf = results_gdf.merge(neighbourhoods[['ID', 'rat_run_edge_count']], on='ID', how='left')\n",
    "final_results_gdf['geometry'] = final_results_gdf['geometry_filters']\n",
    "final_results_gdf = final_results_gdf.set_geometry('geometry')\n",
    "final_results_gdf.drop(columns=['geometry_filters', 'geometry_access'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score ltns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scoring function for \"rat_run_edge_count\"\n",
    "def score_rat_run_edge_count(value):\n",
    "    if value <= 1:\n",
    "        return 100\n",
    "    else:\n",
    "        return 100 / (2 ** value) # Exponetial scoring\n",
    "\n",
    "# Apply the scoring function to the \"rat_run_edge_count\" column\n",
    "final_results_gdf[\"rat_run_score\"] = final_results_gdf[\"rat_run_edge_count\"].apply(score_rat_run_edge_count)\n",
    "\n",
    "import math\n",
    "\n",
    "def score_mean_distance_diff(value):\n",
    "    if value >= 0:\n",
    "        return 0\n",
    "    elif value <= -750: # set a 750m cut off\n",
    "        return 100\n",
    "    else:\n",
    "        normalized_value = abs(value) / 750  # Normalize the value between 0 and 1\n",
    "        score = 100 * (1 - math.exp(-5 * normalized_value))  # Exponential increase\n",
    "        return score\n",
    "\n",
    "# Apply the modified scoring function to the \"mean_distance_diff\" column\n",
    "final_results_gdf[\"mean_distance_diff_score\"] = final_results_gdf[\"mean_distance_diff\"].apply(score_mean_distance_diff)\n",
    "\n",
    "def score_road_density_filters(value):\n",
    "    if value <= 0:\n",
    "        return 0\n",
    "    elif value >= 40:\n",
    "        return 100\n",
    "    else:\n",
    "        return (value / 40) * 100\n",
    "\n",
    "# Apply the scoring function to the \"road_density_filters\" column\n",
    "final_results_gdf[\"filter_road_density_score\"] = final_results_gdf[\"filter_road_density\"].apply(score_road_density_filters)\n",
    "\n",
    "# Create the \"scored_neighbourhoods\" geodataframe with the necessary columns\n",
    "scored_neighbourhoods = final_results_gdf[[\"geometry\", \"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]]\n",
    "\n",
    "# Calculate overall score\n",
    "scored_neighbourhoods[\"overall_score\"] = (scored_neighbourhoods[\"rat_run_score\"] + scored_neighbourhoods[\"mean_distance_diff_score\"] + scored_neighbourhoods[\"filter_road_density_score\"]) / 3\n",
    "\n",
    "# Define weights for each score\n",
    "weight_rat_run_score = 1\n",
    "weight_mean_distance_diff_score = 0.25\n",
    "weight_road_density_filters_score = 0.75\n",
    "\n",
    "# Calculate overall score with weights\n",
    "scored_neighbourhoods[\"overall_score\"] = (\n",
    "    (weight_rat_run_score * scored_neighbourhoods[\"rat_run_score\"]) +\n",
    "    (weight_mean_distance_diff_score * scored_neighbourhoods[\"mean_distance_diff_score\"]) +\n",
    "    (weight_road_density_filters_score * scored_neighbourhoods[\"filter_road_density_score\"])\n",
    ") / (weight_rat_run_score + weight_mean_distance_diff_score + weight_road_density_filters_score)\n",
    "\n",
    "# Print the scored_neighbourhoods dataframe with overall score\n",
    "print(scored_neighbourhoods)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_neighbourhoods.explore(column = \"overall_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram of \"mean_distance_diff\"\n",
    "plt.hist(scored_neighbourhoods[\"overall_score\"], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('LTN Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('LTN Plausiblity Scores (Newcastle)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the variables of interest\n",
    "variables_of_interest = [\"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]\n",
    "\n",
    "# Create a scatter plot matrix\n",
    "sns.pairplot(scored_neighbourhoods[variables_of_interest])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find elbow point for k-means clustering\n",
    "\n",
    "# Selecting the features for clustering\n",
    "X = scored_neighbourhoods[[\"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]]\n",
    "\n",
    "# Initialize a list to store the within-cluster sum of squares (WCSS) for different values of K\n",
    "wcss = []\n",
    "\n",
    "# Define the range of K values to try\n",
    "k_values = range(1, 11)\n",
    "\n",
    "# Calculate WCSS for each value of K\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the elbow curve\n",
    "plt.plot(k_values, wcss, marker='o')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(k_values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run k-means clustering\n",
    "# Define the number of clusters\n",
    "k = 2\n",
    "\n",
    "# Select the features for clustering\n",
    "features = [\"rat_run_score\", \"mean_distance_diff_score\", \"filter_road_density_score\"]\n",
    "\n",
    "# Extract the features from the dataframe\n",
    "X = scored_neighbourhoods[features]\n",
    "\n",
    "# Initialize the KMeans model\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels to the dataframe\n",
    "scored_neighbourhoods[\"cluster_label\"] = cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_neighbourhoods.explore(column='cluster_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adjust geodataframe contents for plotting purposes\n",
    "\n",
    "replacement_map = {\n",
    "    'barrier or bollard': 'Barrier or Bollard',\n",
    "    'bus gate': 'Bus Gate',\n",
    "    'one-way bike': 'Cycle Contraflow',\n",
    "    'street continuation': 'Street Continuation'\n",
    "}\n",
    "\n",
    "# Replace filter types in the DataFrame\n",
    "filters['filter_type'] = filters['filter_type'].map(replacement_map).fillna(filters['filter_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "import branca.colormap as cm\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "\n",
    "# Assuming you have already loaded your GeoDataFrames: scored_neighbourhoods, filters_results_gdf, and streets_gdf\n",
    "\n",
    "# Calculate the centroid of the scored_neighbourhoods GeoDataFrame\n",
    "centroid = scored_neighbourhoods.geometry.centroid.iloc[0]\n",
    "center_latitude, center_longitude = centroid.y, centroid.x\n",
    "\n",
    "# Create a Folium map centered around the centroid of scored_neighbourhoods\n",
    "m = folium.Map(location=[center_latitude, center_longitude], zoom_start=12)\n",
    "\n",
    "# Define the colormap using cm.linear.viridis\n",
    "cmap = cm.linear.viridis.scale(scored_neighbourhoods['overall_score'].min(), scored_neighbourhoods['overall_score'].max())\n",
    "\n",
    "# Plot scored_neighbourhoods using the Viridis colormap\n",
    "folium.GeoJson(scored_neighbourhoods,\n",
    "               name= \"Scored Neighbourhoods\",\n",
    "               style_function=lambda x: {'fillColor': cmap(x['properties']['overall_score']),\n",
    "                                         'color': cmap(x['properties']['overall_score']),\n",
    "                                         'weight': 1, 'fillOpacity': 0.7},\n",
    "               tooltip=folium.features.GeoJsonTooltip(\n",
    "                   fields=['rat_run_score', 'mean_distance_diff_score', 'filter_road_density_score', 'overall_score', 'cluster_label'],\n",
    "                   aliases=['Rat Run Score', 'Mean Distance Diff Score', 'Filter Road Density Score', 'Overall Score', 'Cluster Label'])\n",
    "               ).add_to(m)\n",
    "\n",
    "# Plot streets_gdf on the map with default blue color and slightly transparent\n",
    "streets_layer = folium.GeoJson(drive_gdf_edges,\n",
    "                               name=\"Streets\",\n",
    "                               style_function=lambda x: {'color': 'lightgreen', 'weight': 1, 'fillOpacity': 0.7}\n",
    "                              ).add_to(m)\n",
    "\n",
    "# Plot rat_run_edges on the map with red color\n",
    "rat_runs_layer = folium.GeoJson(rat_run_edges,\n",
    "                                name=\"Rat Runs\",\n",
    "                                style_function=lambda x: {'color': 'red', 'weight': 1.5, 'fillOpacity': 0.7}\n",
    "                               ).add_to(m)\n",
    "\n",
    "# Plot boundary_roads on the map with orange color and thicker weight\n",
    "boundary_roads_layer = folium.GeoJson(boundary_roads,\n",
    "                                      name=\"Busy Roads\",\n",
    "                                      style_function=lambda x: {'color': 'orange', 'weight': 3, 'fillOpacity': 0.7}\n",
    "                                     ).add_to(m)\n",
    "\n",
    "\n",
    "\n",
    "# Create a feature group for each type of layer\n",
    "point_group = folium.FeatureGroup(name='Modal Filtering Points', show=True)\n",
    "line_group = folium.FeatureGroup(name='Modal Filtering Streets', show=True)\n",
    "\n",
    "\n",
    "\n",
    "# Plot purple point markers for filters with tooltips\n",
    "for _, row in filters.iterrows():\n",
    "    if row.geometry.type == 'Point':\n",
    "        tooltip_text = f\"Filter type: {row['filter_type']}\"  # Concatenating \"Filter type:\" with the 'filter_type' value\n",
    "        folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=2, color='purple', fill=True, fill_color='purple', tooltip=tooltip_text).add_to(point_group)\n",
    "    elif row.geometry.type == 'MultiLineString' or row.geometry.type == 'LineString':\n",
    "        tooltip_text = f\"Filter type: {row['filter_type']}\"  # Concatenating \"Filter type:\" with the 'filter_type' value\n",
    "        folium.GeoJson(row.geometry, style_function=lambda x: {'color': 'purple', 'weight': 1.5, 'fillOpacity': 0.7}, tooltip=tooltip_text).add_to(line_group)\n",
    "\n",
    "\n",
    "# Add layer groups to the map\n",
    "point_group.add_to(m)\n",
    "line_group.add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl(autoZIndex=True).add_to(m)\n",
    "\n",
    "cmap.caption = 'LTN Plausiblity Scores (Possible range: 0-100)'\n",
    "cmap.add_to(m)\n",
    "\n",
    "\n",
    "# add text\n",
    "from folium import IFrame\n",
    "\n",
    "# Define the HTML content for the text\n",
    "html_text = \"\"\"\n",
    "<div style=\"position: fixed; \n",
    "             bottom: 50px; left: 50px; width: 300px; height: 200px; \n",
    "             background-color: rgba(255, 255, 255, 0.6);\n",
    "             border:2px solid grey; z-index:9999;\n",
    "             font-size:14px;\n",
    "             \">\n",
    "    <p style=\"padding: 10px;\">Scored neighbourhoods show a LTN 'Plausibility' score which incorporates metrics based on the presence of rat-runs, modal filters and measures of neighbourhood permeability. Map results are experimental, and should be treated as such. Get in touch via c.larkin@newcastle.ac.uk or <a href=\"https://github.com/Froguin99/LTN-Detection\" target=\"_blank\">https://github.com/Froguin99/LTN-Detection</a>.</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Add the HTML content to the map\n",
    "folium.MacroElement().add_to(m)\n",
    "m.get_root().html.add_child(folium.Element(html_text))\n",
    "\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Extract place name without \", United Kingdom\"\n",
    "place_name = place.replace(\", United Kingdom\", \"\").strip()\n",
    "\n",
    "# Create the file paths\n",
    "map_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\Examples\\maps', f'{place_name}_example.html')\n",
    "geopackage_file_path = os.path.join(r'C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\ltnDetection\\LTN-Detection\\data\\scored_neighbourhoods', f'scored_neighbourhoods_{place_name}.gpkg')\n",
    "\n",
    "# Export map\n",
    "m.save(map_file_path)\n",
    "\n",
    "# Send to geopackage \n",
    "geometry_column = scored_neighbourhoods.geometry.name\n",
    "\n",
    "# Iterate through the columns and convert them to strings\n",
    "for column in scored_neighbourhoods.columns:\n",
    "    if column != geometry_column:\n",
    "        scored_neighbourhoods[column] = scored_neighbourhoods[column].astype(str)\n",
    "\n",
    "scored_neighbourhoods.to_file(geopackage_file_path, driver=\"GPKG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
